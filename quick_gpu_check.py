#!/usr/bin/env python3
"""
å¿«é€ŸGPUä½¿ç”¨æ£€æŸ¥
"""

import torch
import time

def check_gpu_usage():
    """æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨")
        return
    
    print("ğŸ” GPUä½¿ç”¨æƒ…å†µæ£€æŸ¥")
    print("=" * 40)
    
    # GPUä¿¡æ¯
    gpu_name = torch.cuda.get_device_name(0)
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print(f"GPU: {gpu_name}")
    print(f"æ€»æ˜¾å­˜: {total_memory:.1f} GB")
    
    # å½“å‰ä½¿ç”¨æƒ…å†µ
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    cached = torch.cuda.memory_reserved(0) / 1024**3
    
    print(f"å·²åˆ†é…æ˜¾å­˜: {allocated:.2f} GB")
    print(f"å·²ç¼“å­˜æ˜¾å­˜: {cached:.2f} GB")
    print(f"ä½¿ç”¨ç‡: {(allocated/total_memory)*100:.1f}%")
    
    # æµ‹è¯•GPUè®¡ç®—
    print("\nğŸ§ª GPUè®¡ç®—æµ‹è¯•...")
    device = torch.device('cuda')
    
    # åˆ›å»ºå¤§å¼ é‡æµ‹è¯•
    start_time = time.time()
    x = torch.randn(1000, 1000, device=device)
    y = torch.randn(1000, 1000, device=device)
    z = torch.mm(x, y)  # çŸ©é˜µä¹˜æ³•
    torch.cuda.synchronize()
    end_time = time.time()
    
    print(f"çŸ©é˜µä¹˜æ³•è€—æ—¶: {(end_time-start_time)*1000:.1f}ms")
    
    # æ£€æŸ¥ä½¿ç”¨åçš„æ˜¾å­˜
    allocated_after = torch.cuda.memory_allocated(0) / 1024**3
    print(f"è®¡ç®—åæ˜¾å­˜: {allocated_after:.2f} GB")
    
    # æ¸…ç†
    del x, y, z
    torch.cuda.empty_cache()
    
    print("âœ… GPUå·¥ä½œæ­£å¸¸")

if __name__ == "__main__":
    check_gpu_usage()