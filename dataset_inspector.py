#!/usr/bin/env python3
"""
æ•°æ®é›†æ£€æŸ¥å·¥å…·
ç”¨äºåˆ†æCropPilot AIå›¾åƒè¯†åˆ«é¡¹ç›®çš„æ•°æ®é›†ç»“æ„å’Œå†…å®¹
"""

import os
import sys
from pathlib import Path
import json
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import time

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("âš ï¸  PILæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥å›¾åƒè¯¦æƒ…")
    print("   å®‰è£…: pip install Pillow")

def check_path_exists(path: str) -> bool:
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨"""
    return os.path.exists(path) and os.path.isdir(path)

def get_directory_size(path: str) -> Tuple[int, str]:
    """è·å–ç›®å½•å¤§å°"""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
    except Exception as e:
        print(f"è®¡ç®—å¤§å°æ—¶å‡ºé”™: {e}")
        return 0, "æœªçŸ¥"
    
    # è½¬æ¢ä¸ºå¯è¯»æ ¼å¼
    for unit in ['B', 'KB', 'MB', 'GB']:
        if total_size < 1024.0:
            return total_size, f"{total_size:.1f} {unit}"
        total_size /= 1024.0
    return total_size, f"{total_size:.1f} TB"

def count_files_by_extension(path: str) -> Dict[str, int]:
    """ç»Ÿè®¡ä¸åŒæ‰©å±•åçš„æ–‡ä»¶æ•°é‡"""
    extensions = defaultdict(int)
    try:
        for root, dirs, files in os.walk(path):
            for file in files:
                ext = os.path.splitext(file)[1].lower()
                extensions[ext] += 1
    except Exception as e:
        print(f"ç»Ÿè®¡æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    return dict(extensions)

def analyze_image_dataset(path: str, max_samples: int = 100) -> Dict[str, Any]:
    """åˆ†æå›¾åƒæ•°æ®é›†"""
    analysis = {
        'total_images': 0,
        'image_formats': Counter(),
        'image_sizes': [],
        'directory_structure': {},
        'sample_files': []
    }
    
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    
    try:
        for root, dirs, files in os.walk(path):
            # è®°å½•ç›®å½•ç»“æ„
            rel_path = os.path.relpath(root, path)
            if rel_path != '.':
                analysis['directory_structure'][rel_path] = len(files)
            
            for file in files:
                ext = os.path.splitext(file)[1].lower()
                if ext in image_extensions:
                    analysis['total_images'] += 1
                    analysis['image_formats'][ext] += 1
                    
                    # é‡‡æ ·åˆ†æå›¾åƒè¯¦æƒ…
                    if len(analysis['sample_files']) < max_samples and PIL_AVAILABLE:
                        try:
                            img_path = os.path.join(root, file)
                            with Image.open(img_path) as img:
                                analysis['image_sizes'].append(img.size)
                                analysis['sample_files'].append({
                                    'path': os.path.relpath(img_path, path),
                                    'size': img.size,
                                    'mode': img.mode,
                                    'format': img.format
                                })
                        except Exception as e:
                            print(f"æ— æ³•è¯»å–å›¾åƒ {file}: {e}")
    
    except Exception as e:
        print(f"åˆ†æå›¾åƒæ•°æ®é›†æ—¶å‡ºé”™: {e}")
    
    return analysis

def inspect_plantvillage_dataset(base_path: str) -> Dict[str, Any]:
    """æ£€æŸ¥PlantVillageæ•°æ®é›†"""
    print("\nğŸ” æ£€æŸ¥PlantVillageæ•°æ®é›†...")
    
    # æ£€æŸ¥ä¸‰ä¸ªPlantVillageå­æ•°æ®é›†
    plantvillage_base = os.path.join(base_path, "1.å›¾åƒæ•°æ®ï¼ˆç—…è™«å®³è¯†åˆ«æ ¸å¿ƒï¼‰", "plantvillage dataset")
    
    datasets = {
        'color': os.path.join(plantvillage_base, "color"),
        'grayscale': os.path.join(plantvillage_base, "grayscale"), 
        'segmented': os.path.join(plantvillage_base, "segmented")
    }
    
    results = {}
    total_images = 0
    all_classes = {}
    
    for dataset_type, dataset_path in datasets.items():
        if not check_path_exists(dataset_path):
            results[dataset_type] = {
                'status': 'not_found',
                'message': f'è·¯å¾„ä¸å­˜åœ¨: {dataset_path}'
            }
            continue
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        size_bytes, size_str = get_directory_size(dataset_path)
        file_types = count_files_by_extension(dataset_path)
        
        # åˆ†æå›¾åƒæ•°æ®
        image_analysis = analyze_image_dataset(dataset_path)
        
        # æŸ¥æ‰¾ç±»åˆ«ä¿¡æ¯
        class_info = {}
        for root, dirs, files in os.walk(dataset_path):
            if dirs:  # å¦‚æœæœ‰å­ç›®å½•ï¼Œå¯èƒ½æ˜¯ç±»åˆ«ç›®å½•
                for dir_name in dirs:
                    dir_path = os.path.join(root, dir_name)
                    img_count = len([f for f in os.listdir(dir_path) 
                                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
                    if img_count > 0:
                        class_info[dir_name] = img_count
                        all_classes[dir_name] = all_classes.get(dir_name, 0) + img_count
            break  # åªæ£€æŸ¥ç¬¬ä¸€å±‚ç›®å½•
        
        total_images += image_analysis['total_images']
        
        results[dataset_type] = {
            'status': 'found',
            'path': dataset_path,
            'size': size_str,
            'file_types': file_types,
            'total_images': image_analysis['total_images'],
            'image_formats': dict(image_analysis['image_formats']),
            'directory_structure': image_analysis['directory_structure'],
            'classes': class_info,
            'sample_images': image_analysis['sample_files'][:3]
        }
    
    return {
        'status': 'found' if any(r.get('status') == 'found' for r in results.values()) else 'not_found',
        'datasets': results,
        'total_images': total_images,
        'total_classes': len(all_classes),
        'class_distribution': all_classes
    }

def inspect_baidu_dataset(base_path: str) -> Dict[str, Any]:
    """æ£€æŸ¥ç™¾åº¦AI Studioæ•°æ®é›†"""
    print("\nğŸ” æ£€æŸ¥ç™¾åº¦AI Studioæ•°æ®é›†...")
    
    baidu_path = os.path.join(base_path, "1.å›¾åƒæ•°æ®ï¼ˆç—…è™«å®³è¯†åˆ«æ ¸å¿ƒï¼‰", "ai_challenger_pdr2018")
    
    if not check_path_exists(baidu_path):
        return {
            'status': 'not_found',
            'message': f'è·¯å¾„ä¸å­˜åœ¨: {baidu_path}'
        }
    
    # è·å–åŸºæœ¬ä¿¡æ¯
    size_bytes, size_str = get_directory_size(baidu_path)
    file_types = count_files_by_extension(baidu_path)
    
    # åˆ†æå›¾åƒæ•°æ®
    image_analysis = analyze_image_dataset(baidu_path)
    
    # æŸ¥æ‰¾æ ‡æ³¨æ–‡ä»¶
    annotation_files = []
    for root, dirs, files in os.walk(baidu_path):
        for file in files:
            if file.endswith(('.json', '.csv', '.txt', '.xml')):
                annotation_files.append(os.path.join(root, file))
    
    return {
        'status': 'found',
        'path': baidu_path,
        'size': size_str,
        'file_types': file_types,
        'total_images': image_analysis['total_images'],
        'image_formats': dict(image_analysis['image_formats']),
        'directory_structure': image_analysis['directory_structure'],
        'annotation_files': annotation_files,
        'sample_images': image_analysis['sample_files'][:5]
    }

def inspect_phenology_dataset(base_path: str) -> Dict[str, Any]:
    """æ£€æŸ¥ç‰©å€™æ•°æ®é›†"""
    print("\nğŸ” æ£€æŸ¥ChinaCropPhen1kmç‰©å€™æ•°æ®é›†...")
    
    phenology_path = os.path.join(base_path, "2.ç”Ÿé•¿æ•°æ®ï¼ˆæ—¶é—´åºåˆ—ï¼‰", "8313530")
    
    if not check_path_exists(phenology_path):
        return {
            'status': 'not_found',
            'message': f'è·¯å¾„ä¸å­˜åœ¨: {phenology_path}'
        }
    
    # è·å–åŸºæœ¬ä¿¡æ¯
    size_bytes, size_str = get_directory_size(phenology_path)
    file_types = count_files_by_extension(phenology_path)
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_files = []
    for root, dirs, files in os.walk(phenology_path):
        for file in files:
            if file.endswith(('.tif', '.tiff', '.nc', '.hdf', '.dat')):
                data_files.append({
                    'name': file,
                    'path': os.path.relpath(os.path.join(root, file), phenology_path),
                    'size': os.path.getsize(os.path.join(root, file))
                })
    
    return {
        'status': 'found',
        'path': phenology_path,
        'size': size_str,
        'file_types': file_types,
        'data_files': data_files[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
        'total_data_files': len(data_files)
    }

def print_dataset_summary(dataset_name: str, info: Dict[str, Any]):
    """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
    print(f"\nğŸ“Š {dataset_name} æ•°æ®é›†åˆ†æç»“æœ")
    print("=" * 60)
    
    if info['status'] == 'not_found':
        print(f"âŒ {info['message']}")
        return
    
    # å¤„ç†PlantVillageçš„ç‰¹æ®Šç»“æ„
    if 'datasets' in info:
        print(f"âœ… PlantVillageæ•°æ®é›†åŒ…å« {len(info['datasets'])} ä¸ªå­æ•°æ®é›†")
        print(f"ğŸ–¼ï¸  æ€»å›¾åƒæ•°: {info['total_images']}")
        print(f"ğŸ·ï¸  æ€»ç±»åˆ«æ•°: {info['total_classes']}")
        
        for dataset_type, dataset_info in info['datasets'].items():
            if dataset_info['status'] == 'found':
                print(f"\nğŸ“‚ {dataset_type.upper()} æ•°æ®é›†:")
                print(f"   ğŸ“¦ å¤§å°: {dataset_info['size']}")
                print(f"   ğŸ–¼ï¸  å›¾åƒæ•°: {dataset_info['total_images']}")
                print(f"   ğŸ·ï¸  ç±»åˆ«æ•°: {len(dataset_info['classes'])}")
                
                if dataset_info['image_formats']:
                    print("   ğŸ“‹ å›¾åƒæ ¼å¼:")
                    for fmt, count in dataset_info['image_formats'].items():
                        print(f"      {fmt}: {count} å¼ ")
            else:
                print(f"\nâŒ {dataset_type.upper()}: {dataset_info['message']}")
        
        if info['class_distribution']:
            print(f"\nğŸ·ï¸  ç±»åˆ«åˆ†å¸ƒ (å‰10ä¸ª):")
            sorted_classes = sorted(info['class_distribution'].items(), key=lambda x: x[1], reverse=True)
            for class_name, count in sorted_classes[:10]:
                print(f"   {class_name}: {count} å¼ ")
            if len(sorted_classes) > 10:
                print(f"   ... è¿˜æœ‰ {len(sorted_classes) - 10} ä¸ªç±»åˆ«")
        return
    
    print(f"âœ… è·¯å¾„: {info['path']}")
    print(f"ğŸ“¦ å¤§å°: {info['size']}")
    
    if 'total_images' in info:
        print(f"ğŸ–¼ï¸  å›¾åƒæ€»æ•°: {info['total_images']}")
        if info['image_formats']:
            print("ğŸ“‹ å›¾åƒæ ¼å¼:")
            for fmt, count in info['image_formats'].items():
                print(f"   {fmt}: {count} å¼ ")
    
    if 'classes' in info and info['classes']:
        print(f"ğŸ·ï¸  ç±»åˆ«æ•°é‡: {len(info['classes'])}")
        print("ğŸ“‚ ç±»åˆ«åˆ†å¸ƒ (å‰10ä¸ª):")
        sorted_classes = sorted(info['classes'].items(), key=lambda x: x[1], reverse=True)
        for class_name, count in sorted_classes[:10]:
            print(f"   {class_name}: {count} å¼ ")
        if len(sorted_classes) > 10:
            print(f"   ... è¿˜æœ‰ {len(sorted_classes) - 10} ä¸ªç±»åˆ«")
    
    if 'annotation_files' in info and info['annotation_files']:
        print(f"ğŸ“ æ ‡æ³¨æ–‡ä»¶: {len(info['annotation_files'])} ä¸ª")
        for ann_file in info['annotation_files'][:3]:
            print(f"   {os.path.basename(ann_file)}")
    
    if 'total_data_files' in info:
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {info['total_data_files']} ä¸ª")
    
    if 'file_types' in info:
        print("ğŸ“ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
        for ext, count in sorted(info['file_types'].items(), key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"   {ext or 'æ— æ‰©å±•å'}: {count} ä¸ª")

def generate_recommendations(plantvillage_info: Dict, baidu_info: Dict, phenology_info: Dict) -> List[str]:
    """ç”Ÿæˆæ•°æ®é›†ä½¿ç”¨å»ºè®®"""
    recommendations = []
    
    # æ£€æŸ¥æ•°æ®é›†å¯ç”¨æ€§
    available_datasets = []
    if plantvillage_info['status'] == 'found':
        available_datasets.append('PlantVillage')
    if baidu_info['status'] == 'found':
        available_datasets.append('ç™¾åº¦AI Studio')
    if phenology_info['status'] == 'found':
        available_datasets.append('ç‰©å€™æ•°æ®')
    
    if len(available_datasets) == 3:
        recommendations.append("âœ… æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†éƒ½å¯ç”¨ï¼Œå¯ä»¥å®ç°å®Œæ•´çš„AIè¯†åˆ«ç³»ç»Ÿ")
    elif len(available_datasets) >= 1:
        recommendations.append(f"âš ï¸  åªæœ‰ {len(available_datasets)} ä¸ªæ•°æ®é›†å¯ç”¨: {', '.join(available_datasets)}")
    else:
        recommendations.append("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥è·¯å¾„é…ç½®")
        return recommendations
    
    # PlantVillageæ•°æ®é›†å»ºè®®
    if plantvillage_info['status'] == 'found':
        total_images = plantvillage_info.get('total_images', 0)
        if total_images > 40000:
            recommendations.append("ğŸ¯ PlantVillageæ•°æ®é›†å›¾åƒå……è¶³ï¼Œé€‚åˆä½œä¸ºä¸»è¦è®­ç»ƒæ•°æ®")
            
            # æ£€æŸ¥å“ªä¸ªå­æ•°æ®é›†æœ€é€‚åˆ
            if 'datasets' in plantvillage_info:
                color_images = plantvillage_info['datasets'].get('color', {}).get('total_images', 0)
                if color_images > 30000:
                    recommendations.append("ğŸŒˆ å»ºè®®ä¼˜å…ˆä½¿ç”¨coloræ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ˆå½©è‰²å›¾åƒæ•ˆæœæ›´å¥½ï¼‰")
                
                segmented_images = plantvillage_info['datasets'].get('segmented', {}).get('total_images', 0)
                if segmented_images > 20000:
                    recommendations.append("âœ‚ï¸  segmentedæ•°æ®é›†å¯ç”¨äºæé«˜è¯†åˆ«ç²¾åº¦ï¼ˆèƒŒæ™¯å·²åˆ†ç¦»ï¼‰")
                    
        elif total_images > 10000:
            recommendations.append("âš ï¸  PlantVillageæ•°æ®é›†å›¾åƒè¾ƒå°‘ï¼Œå»ºè®®ç»“åˆç™¾åº¦æ•°æ®é›†")
        else:
            recommendations.append("âŒ PlantVillageæ•°æ®é›†å›¾åƒè¿‡å°‘ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
    
    # ç™¾åº¦æ•°æ®é›†å»ºè®®
    if baidu_info['status'] == 'found':
        if baidu_info.get('annotation_files'):
            recommendations.append("ğŸ“ ç™¾åº¦æ•°æ®é›†åŒ…å«æ ‡æ³¨æ–‡ä»¶ï¼Œå¯ç”¨äºéªŒè¯å’Œå¾®è°ƒ")
        else:
            recommendations.append("âš ï¸  ç™¾åº¦æ•°æ®é›†ç¼ºå°‘æ ‡æ³¨æ–‡ä»¶ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
    
    # ç‰©å€™æ•°æ®å»ºè®®
    if phenology_info['status'] == 'found':
        recommendations.append("ğŸŒ± ç‰©å€™æ•°æ®å¯ç”¨äºæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºè¯†åˆ«å‡†ç¡®æ€§")
    
    # å®æ–½å»ºè®®
    if len(available_datasets) >= 2:
        recommendations.append("ğŸš€ å»ºè®®æŒ‰è®¡åˆ’å®æ–½ï¼šå…ˆç”¨PlantVillageè®­ç»ƒï¼Œå†ç”¨ç™¾åº¦æ•°æ®éªŒè¯")
        recommendations.append("ğŸ’¡ å¯ä»¥å®ç°é¢„æœŸçš„85%+è¯†åˆ«å‡†ç¡®ç‡")
    
    return recommendations

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ¾ CropPilot AIå›¾åƒè¯†åˆ«æ•°æ®é›†æ£€æŸ¥å·¥å…·")
    print("=" * 60)
    
    # æ•°æ®é›†åŸºç¡€è·¯å¾„
    base_path = r"C:\Users\hp\Desktop\ä½œç‰©ç”Ÿé•¿çŠ¶æ€ç®¡ç†ä¸å†³ç­–æ”¯æŒç³»ç»Ÿ\æ•°æ®"
    
    print(f"ğŸ“ æ•°æ®é›†åŸºç¡€è·¯å¾„: {base_path}")
    
    if not check_path_exists(base_path):
        print(f"âŒ åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}")
        print("\nğŸ’¡ è¯·ç¡®è®¤æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    print("âœ… åŸºç¡€è·¯å¾„å­˜åœ¨ï¼Œå¼€å§‹æ£€æŸ¥å„ä¸ªæ•°æ®é›†...")
    
    # æ£€æŸ¥å„ä¸ªæ•°æ®é›†
    start_time = time.time()
    
    plantvillage_info = inspect_plantvillage_dataset(base_path)
    baidu_info = inspect_baidu_dataset(base_path)
    phenology_info = inspect_phenology_dataset(base_path)
    
    # æ‰“å°ç»“æœ
    print_dataset_summary("PlantVillage", plantvillage_info)
    print_dataset_summary("ç™¾åº¦AI Studio", baidu_info)
    print_dataset_summary("ç‰©å€™æ•°æ®", phenology_info)
    
    # ç”Ÿæˆå»ºè®®
    recommendations = generate_recommendations(plantvillage_info, baidu_info, phenology_info)
    
    print(f"\nğŸ’¡ æ•°æ®é›†ä½¿ç”¨å»ºè®®")
    print("=" * 60)
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec}")
    
    # æ€»ç»“
    elapsed_time = time.time() - start_time
    print(f"\nâ±ï¸  æ£€æŸ¥å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f} ç§’")
    
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    print("1. å¦‚æœæ•°æ®é›†éƒ½å¯ç”¨ï¼Œå¯ä»¥å¼€å§‹æ‰§è¡Œä»»åŠ¡è®¡åˆ’")
    print("2. å¦‚æœæœ‰é—®é¢˜ï¼Œè¯·å…ˆè§£å†³æ•°æ®é›†è·¯å¾„æˆ–æ ¼å¼é—®é¢˜")
    print("3. å»ºè®®å…ˆè¿è¡Œä¸€ä¸ªå°è§„æ¨¡æµ‹è¯•æ¥éªŒè¯æ•°æ®åŠ è½½")

if __name__ == "__main__":
    main()