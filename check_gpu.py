#!/usr/bin/env python3
"""
GPUæ£€æµ‹å’Œä¼˜åŒ–è„šæœ¬
æ£€æŸ¥CUDAå¯ç”¨æ€§å¹¶ä¼˜åŒ–GPUè®­ç»ƒè®¾ç½®
"""

import os
import sys

try:
    import torch
    import torch.cuda
    TORCH_AVAILABLE = True
except ImportError:
    print("âŒ PyTorchæœªå®‰è£…")
    TORCH_AVAILABLE = False

def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    print("ğŸ” GPUå¯ç”¨æ€§æ£€æŸ¥")
    print("=" * 50)
    
    if not TORCH_AVAILABLE:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPU")
        return False
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
    
    if not cuda_available:
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. æ²¡æœ‰å®‰è£…CUDAå…¼å®¹çš„PyTorchç‰ˆæœ¬")
        print("2. CUDAé©±åŠ¨ç¨‹åºæœªæ­£ç¡®å®‰è£…")
        print("3. GPUä¸æ”¯æŒCUDA")
        print("\nå»ºè®®:")
        print("è®¿é—® https://pytorch.org/ å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
        return False
    
    # è·å–GPUä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"GPUæ•°é‡: {gpu_count}")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    # æµ‹è¯•GPUå†…å­˜
    if gpu_count > 0:
        try:
            device = torch.device('cuda:0')
            test_tensor = torch.randn(1000, 1000, device=device)
            print(f"âœ… GPUå†…å­˜æµ‹è¯•é€šè¿‡")
            
            # æ˜¾ç¤ºå½“å‰GPUå†…å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            cached = torch.cuda.memory_reserved(0) / 1024**3
            print(f"GPUå†…å­˜ä½¿ç”¨: {allocated:.2f} GB (å·²åˆ†é…) / {cached:.2f} GB (å·²ç¼“å­˜)")
            
            del test_tensor
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ GPUå†…å­˜æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    return True

def optimize_gpu_settings():
    """ä¼˜åŒ–GPUè®­ç»ƒè®¾ç½®"""
    print("\nâš™ï¸ GPUè®­ç»ƒä¼˜åŒ–å»ºè®®")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œæ— æ³•æä¾›ä¼˜åŒ–å»ºè®®")
        return {}
    
    gpu_props = torch.cuda.get_device_properties(0)
    gpu_memory_gb = gpu_props.total_memory / 1024**3
    
    print(f"GPU: {gpu_props.name}")
    print(f"æ˜¾å­˜: {gpu_memory_gb:.1f} GB")
    print(f"è®¡ç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
    
    # æ ¹æ®æ˜¾å­˜å¤§å°æ¨èæ‰¹å¤§å°
    if gpu_memory_gb >= 24:
        recommended_batch_size = 64
        mixed_precision = True
        print("ğŸš€ é«˜ç«¯GPUé…ç½®")
    elif gpu_memory_gb >= 12:
        recommended_batch_size = 32
        mixed_precision = True
        print("ğŸ’ª ä¸­é«˜ç«¯GPUé…ç½®")
    elif gpu_memory_gb >= 8:
        recommended_batch_size = 16
        mixed_precision = True
        print("ğŸ‘ ä¸­ç«¯GPUé…ç½®")
    elif gpu_memory_gb >= 4:
        recommended_batch_size = 8
        mixed_precision = True
        print("âš ï¸ å…¥é—¨çº§GPUé…ç½®")
    else:
        recommended_batch_size = 4
        mixed_precision = False
        print("ğŸ”¥ ä½æ˜¾å­˜GPUé…ç½®")
    
    recommendations = {
        'batch_size': recommended_batch_size,
        'mixed_precision': mixed_precision,
        'num_workers': min(8, os.cpu_count()),
        'pin_memory': True,
        'device': 'cuda'
    }
    
    print(f"\næ¨èè®¾ç½®:")
    print(f"  æ‰¹å¤§å°: {recommended_batch_size}")
    print(f"  æ··åˆç²¾åº¦: {'å¯ç”¨' if mixed_precision else 'ç¦ç”¨'}")
    print(f"  æ•°æ®åŠ è½½è¿›ç¨‹: {recommendations['num_workers']}")
    print(f"  å†…å­˜å›ºå®š: {'å¯ç”¨' if recommendations['pin_memory'] else 'ç¦ç”¨'}")
    
    # é¢å¤–ä¼˜åŒ–å»ºè®®
    print(f"\né¢å¤–ä¼˜åŒ–å»ºè®®:")
    if gpu_memory_gb < 8:
        print("  - è€ƒè™‘ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹å¤§å°")
        print("  - ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å¦‚efficientnet-b2æˆ–b3")
    
    if mixed_precision:
        print("  - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥èŠ‚çœçº¦50%æ˜¾å­˜")
        print("  - å¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼ˆé€šå¸¸å¢åŠ 1.5-2å€ï¼‰")
    
    print("  - ä½¿ç”¨torch.backends.cudnn.benchmark = TrueåŠ é€Ÿè®­ç»ƒ")
    print("  - å®šæœŸæ¸…ç†GPUç¼“å­˜: torch.cuda.empty_cache()")
    
    return recommendations

def create_gpu_optimized_config():
    """åˆ›å»ºGPUä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    if not torch.cuda.is_available():
        print("\nâŒ GPUä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºGPUä¼˜åŒ–é…ç½®")
        return
    
    recommendations = optimize_gpu_settings()
    
    config = {
        "num_epochs": 50,
        "batch_size": recommendations['batch_size'],
        "learning_rate": 0.001,
        "weight_decay": 1e-4,
        "model_type": "efficientnet",
        "model_name": "efficientnet-b4",
        "num_classes": 38,
        "pretrained": True,
        "optimizer_type": "adamw",
        "scheduler_type": "cosine",
        "early_stopping": True,
        "patience": 10,
        "min_delta": 0.001,
        "save_dir": "checkpoints/plant_disease_gpu",
        "device": "cuda",
        "mixed_precision": recommendations['mixed_precision'],
        "gradient_clip_norm": 1.0,
        "label_smoothing": 0.1
    }
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    import json
    config_path = "gpu_training_config.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nâœ… GPUä¼˜åŒ–é…ç½®å·²ä¿å­˜: {config_path}")
    print("ä½¿ç”¨æ–¹æ³•: python train_model.py --config gpu_training_config.json")

def benchmark_gpu_performance():
    """GPUæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    if not torch.cuda.is_available():
        print("\nâŒ GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½æµ‹è¯•")
        return
    
    print("\nğŸƒ GPUæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    device = torch.device('cuda')
    
    # æµ‹è¯•ä¸åŒæ‰¹å¤§å°çš„æ€§èƒ½
    batch_sizes = [8, 16, 32, 64]
    input_size = (3, 224, 224)
    
    print("æµ‹è¯•EfficientNet-B4å‰å‘ä¼ æ’­æ€§èƒ½:")
    
    try:
        from src.model_architecture import create_plant_disease_model
        model = create_plant_disease_model('efficientnet', pretrained=False).to(device)
        model.eval()
        
        import time
        
        for batch_size in batch_sizes:
            try:
                # é¢„çƒ­
                with torch.no_grad():
                    dummy_input = torch.randn(batch_size, *input_size, device=device)
                    for _ in range(5):
                        _ = model(dummy_input)
                
                # æ€§èƒ½æµ‹è¯•
                torch.cuda.synchronize()
                start_time = time.time()
                
                with torch.no_grad():
                    for _ in range(20):
                        _ = model(dummy_input)
                
                torch.cuda.synchronize()
                end_time = time.time()
                
                avg_time = (end_time - start_time) / 20
                throughput = batch_size / avg_time
                
                print(f"  æ‰¹å¤§å° {batch_size:2d}: {avg_time*1000:.1f}ms/batch, {throughput:.1f} images/sec")
                
                del dummy_input
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"  æ‰¹å¤§å° {batch_size:2d}: âŒ æ˜¾å­˜ä¸è¶³")
                    torch.cuda.empty_cache()
                else:
                    raise e
        
        del model
        torch.cuda.empty_cache()
        
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥æ¨¡å‹ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ CropPilot GPUæ£€æµ‹å’Œä¼˜åŒ–å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpu_available = check_gpu_availability()
    
    if gpu_available:
        # ä¼˜åŒ–è®¾ç½®å»ºè®®
        optimize_gpu_settings()
        
        # åˆ›å»ºGPUä¼˜åŒ–é…ç½®
        create_gpu_optimized_config()
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark_gpu_performance()
        
        print("\nğŸ‰ GPUæ£€æµ‹å’Œä¼˜åŒ–å®Œæˆ!")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨GPUè¿›è¡Œé«˜æ•ˆè®­ç»ƒäº†")
    else:
        print("\nğŸ’¡ å»ºè®®:")
        print("1. å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch:")
        print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
        print("2. ç¡®ä¿NVIDIAé©±åŠ¨ç¨‹åºå·²æ­£ç¡®å®‰è£…")
        print("3. é‡å¯ç³»ç»Ÿåé‡æ–°æµ‹è¯•")

if __name__ == "__main__":
    main()