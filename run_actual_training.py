#!/usr/bin/env python3
"""
å®é™…æ‰§è¡Œè®­ç»ƒè„šæœ¬
ä½¿ç”¨GPUä¼˜åŒ–é…ç½®æ‰§è¡ŒçœŸå®çš„æ¤ç‰©ç—…å®³è¯†åˆ«æ¨¡å‹è®­ç»ƒ
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset, random_split
    import numpy as np
    from tqdm import tqdm
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.model_architecture import create_plant_disease_model, ModelFactory
from src.model_trainer import ModelTrainer, create_default_config
from src.model_evaluator import create_evaluator
from src.training_strategies import ClassBalanceStrategy, FocalLoss
from src.model_optimization import InferenceOptimizer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('actual_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def create_realistic_dataset(num_samples: int = 5000, num_classes: int = 38):
    """åˆ›å»ºæ›´çœŸå®çš„æ•°æ®é›†ç”¨äºè®­ç»ƒ"""
    logger.info(f"åˆ›å»ºçœŸå®è®­ç»ƒæ•°æ®é›†: {num_samples} æ ·æœ¬, {num_classes} ç±»åˆ«")
    
    # åˆ›å»ºæ›´çœŸå®çš„å›¾åƒæ•°æ®ï¼ˆæ¨¡æ‹Ÿæ¤ç‰©ç—…å®³å›¾åƒç‰¹å¾ï¼‰
    images = []
    labels = []
    
    # æ¨¡æ‹Ÿä¸å¹³è¡¡çš„ç±»åˆ«åˆ†å¸ƒï¼ˆçœŸå®æ•°æ®é›†é€šå¸¸æ˜¯ä¸å¹³è¡¡çš„ï¼‰
    class_weights = np.random.exponential(1.0, num_classes)
    class_weights = class_weights / class_weights.sum()
    
    for i in range(num_samples):
        # æ ¹æ®æƒé‡é€‰æ‹©ç±»åˆ«
        class_id = np.random.choice(num_classes, p=class_weights)
        
        # åˆ›å»ºå…·æœ‰ä¸€å®šæ¨¡å¼çš„å›¾åƒï¼ˆæ¨¡æ‹Ÿæ¤ç‰©ç‰¹å¾ï¼‰
        base_image = torch.randn(3, 224, 224)
        
        # æ·»åŠ ç±»åˆ«ç‰¹å®šçš„æ¨¡å¼
        if class_id < 10:  # å¥åº·æ¤ç‰©
            base_image = base_image * 0.5 + 0.3  # è¾ƒäº®çš„ç»¿è‰²è°ƒ
        elif class_id < 20:  # å¶æ–‘ç—…
            base_image[0] *= 1.2  # å¢å¼ºçº¢è‰²é€šé“
        elif class_id < 30:  # èè”«ç—…
            base_image = base_image * 0.7  # è¾ƒæš—
        else:  # å…¶ä»–ç—…å®³
            base_image[1] *= 0.8  # å‡å°‘ç»¿è‰²é€šé“
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(base_image) * 0.1
        final_image = torch.clamp(base_image + noise, -2, 2)
        
        images.append(final_image)
        labels.append(class_id)
    
    # è½¬æ¢ä¸ºå¼ é‡
    images_tensor = torch.stack(images)
    labels_tensor = torch.tensor(labels, dtype=torch.long)
    
    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    from collections import Counter
    class_distribution = Counter(labels)
    logger.info(f"ç±»åˆ«åˆ†å¸ƒ (å‰10ç±»): {dict(list(class_distribution.most_common(10)))}")
    
    return TensorDataset(images_tensor, labels_tensor), class_distribution

def run_gpu_training():
    """æ‰§è¡ŒGPUè®­ç»ƒ"""
    logger.info("ğŸš€ å¼€å§‹GPUåŠ é€Ÿæ¤ç‰©ç—…å®³è¯†åˆ«æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 80)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        logger.error("âŒ GPUä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥CUDAå®‰è£…")
        return False
    
    device = torch.device('cuda')
    logger.info(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    start_time = time.time()
    
    try:
        # 1. åˆ›å»ºæ•°æ®é›†
        logger.info("\nğŸ“Š æ­¥éª¤1: åˆ›å»ºè®­ç»ƒæ•°æ®é›†")
        full_dataset, class_distribution = create_realistic_dataset(
            num_samples=3000,  # é€‚ä¸­çš„æ•°æ®é›†å¤§å°
            num_classes=38
        )
        
        # åˆ†å‰²æ•°æ®é›†
        train_size = int(0.7 * len(full_dataset))
        val_size = int(0.15 * len(full_dataset))
        test_size = len(full_dataset) - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        logger.info(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        logger.info(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        logger.info("\nğŸ”„ æ­¥éª¤2: åˆ›å»ºæ•°æ®åŠ è½½å™¨")
        train_loader = DataLoader(
            train_dataset, 
            batch_size=8,  # é€‚åˆ6GBæ˜¾å­˜
            shuffle=True,
            num_workers=0,  # Windowså…¼å®¹æ€§
            pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=8, 
            shuffle=False,
            num_workers=0,
            pin_memory=True
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=8, 
            shuffle=False,
            num_workers=0,
            pin_memory=True
        )
        
        # 3. åˆ›å»ºè®­ç»ƒé…ç½®
        logger.info("\nâš™ï¸ æ­¥éª¤3: é…ç½®è®­ç»ƒå‚æ•°")
        config = create_default_config(
            num_epochs=15,  # é€‚ä¸­çš„è®­ç»ƒè½®æ•°
            batch_size=8,
            learning_rate=0.001,
            model_name='efficientnet-b4',
            pretrained=True,  # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            device='cuda',
            mixed_precision=True,
            early_stopping=True,
            patience=5,
            save_dir='checkpoints/actual_training_run',
            num_classes=38
        )
        
        logger.info(f"   è®­ç»ƒè½®æ•°: {config.num_epochs}")
        logger.info(f"   æ‰¹å¤§å°: {config.batch_size}")
        logger.info(f"   å­¦ä¹ ç‡: {config.learning_rate}")
        logger.info(f"   æ··åˆç²¾åº¦: {config.mixed_precision}")
        
        # 4. åˆ›å»ºè®­ç»ƒå™¨å’Œæ¨¡å‹
        logger.info("\nğŸ—ï¸ æ­¥éª¤4: åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒå™¨")
        trainer = ModelTrainer(config)
        model = trainer.setup_model()
        
        # ä½¿ç”¨Focal Losså¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        focal_loss = FocalLoss(alpha=1.0, gamma=2.0)
        trainer.criterion = focal_loss.to(device)
        
        model_info = ModelFactory.get_model_info(model)
        logger.info(f"   æ¨¡å‹: {model_info['model_type']}")
        logger.info(f"   å‚æ•°æ•°é‡: {model_info['total_parameters']:,}")
        logger.info(f"   æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.1f} MB")
        logger.info(f"   æŸå¤±å‡½æ•°: Focal Loss")
        
        # 5. æ‰§è¡Œè®­ç»ƒ
        logger.info("\nğŸš€ æ­¥éª¤5: å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        logger.info("-" * 50)
        
        training_start = time.time()
        training_results = trainer.train(train_loader, val_loader)
        training_time = time.time() - training_start
        
        logger.info("-" * 50)
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ!")
        logger.info(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        logger.info(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_results['best_val_acc']:.2f}%")
        logger.info(f"   æœ€ä½³è½®æ¬¡: {training_results['best_epoch']}")
        
        # 6. æ¨¡å‹è¯„ä¼°
        logger.info("\nğŸ“Š æ­¥éª¤6: æ¨¡å‹è¯„ä¼°")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = Path(config.save_dir) / 'best_model.pth'
        if best_model_path.exists():
            trainer.load_checkpoint('best_model.pth')
            logger.info("   å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        class_names = [f'Disease_{i:02d}' for i in range(38)]
        evaluator = create_evaluator(class_names=class_names, device='cuda')
        
        # è¯„ä¼°æ¨¡å‹
        metrics, _ = evaluator.evaluate_model(
            trainer.model, test_loader, return_predictions=False
        )
        
        logger.info(f"   æµ‹è¯•å‡†ç¡®ç‡: {metrics.accuracy:.4f}")
        logger.info(f"   F1åˆ†æ•°(å®): {metrics.f1_macro:.4f}")
        logger.info(f"   F1åˆ†æ•°(åŠ æƒ): {metrics.f1_weighted:.4f}")
        logger.info(f"   Top-3å‡†ç¡®ç‡: {metrics.top_k_accuracy.get(3, 0):.4f}")
        
        # 7. æ¨ç†ä¼˜åŒ–
        logger.info("\nâš¡ æ­¥éª¤7: æ¨ç†ä¼˜åŒ–")
        
        # è·å–ç¤ºä¾‹è¾“å…¥
        example_input = next(iter(test_loader))[0][:1].to(device)
        
        # æ¨ç†ä¼˜åŒ–
        inference_opt = InferenceOptimizer()
        
        # åŸå§‹æ¨¡å‹æ€§èƒ½
        original_benchmark = inference_opt.benchmark_inference(
            trainer.model, example_input, num_runs=50
        )
        
        # JITä¼˜åŒ–
        try:
            optimized_model = inference_opt.optimize_for_inference(
                trainer.model, example_input, 'basic'
            )
            optimized_benchmark = inference_opt.benchmark_inference(
                optimized_model, example_input, num_runs=50
            )
            
            speedup = original_benchmark['avg_inference_time_ms'] / optimized_benchmark['avg_inference_time_ms']
            
            logger.info(f"   åŸå§‹æ¨ç†é€Ÿåº¦: {original_benchmark['avg_inference_time_ms']:.1f}ms")
            logger.info(f"   ä¼˜åŒ–æ¨ç†é€Ÿåº¦: {optimized_benchmark['avg_inference_time_ms']:.1f}ms")
            logger.info(f"   æ¨ç†åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
        except Exception as e:
            logger.warning(f"   æ¨ç†ä¼˜åŒ–è·³è¿‡: {e}")
            speedup = 1.0
        
        # 8. ä¿å­˜è®­ç»ƒæ€»ç»“
        logger.info("\nğŸ’¾ æ­¥éª¤8: ä¿å­˜è®­ç»ƒæ€»ç»“")
        
        summary = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'training_config': {
                'num_epochs': config.num_epochs,
                'batch_size': config.batch_size,
                'learning_rate': config.learning_rate,
                'model_name': config.model_name,
                'device': str(device)
            },
            'dataset_info': {
                'total_samples': len(full_dataset),
                'train_samples': len(train_dataset),
                'val_samples': len(val_dataset),
                'test_samples': len(test_dataset),
                'num_classes': 38
            },
            'training_results': {
                'best_val_acc': training_results['best_val_acc'],
                'best_epoch': training_results['best_epoch'],
                'training_time_sec': training_time
            },
            'evaluation_results': {
                'test_accuracy': metrics.accuracy,
                'f1_macro': metrics.f1_macro,
                'f1_weighted': metrics.f1_weighted,
                'top_k_accuracy': metrics.top_k_accuracy
            },
            'optimization_results': {
                'inference_speedup': speedup,
                'original_inference_ms': original_benchmark['avg_inference_time_ms'],
                'optimized_inference_ms': optimized_benchmark['avg_inference_time_ms'] if 'optimized_benchmark' in locals() else None
            }
        }
        
        summary_path = Path(config.sav