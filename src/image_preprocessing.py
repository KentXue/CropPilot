#!/usr/bin/env python3
"""
é«˜çº§å›¾åƒé¢„å¤„ç†ç®¡é“
ä½¿ç”¨Albumentationså®ç°æ¤ç‰©ç—…å®³å›¾åƒçš„ä¸“ä¸šé¢„å¤„ç†å’Œæ•°æ®å¢å¼º
"""

import os
import sys
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import logging
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    import torch
    from PIL import Image, ImageEnhance, ImageFilter
    import numpy as np
    ALBUMENTATIONS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    ALBUMENTATIONS_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PreprocessingMode(Enum):
    """é¢„å¤„ç†æ¨¡å¼æšä¸¾"""
    TRAINING = "training"
    VALIDATION = "validation"
    INFERENCE = "inference"
    QUALITY_CHECK = "quality_check"

class ImageQualityAssessment:
    """å›¾åƒè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è´¨é‡è¯„ä¼°å™¨"""
        self.quality_thresholds = {
            'min_resolution': (64, 64),
            'max_resolution': (4096, 4096),
            'min_brightness': 20,
            'max_brightness': 235,
            'min_contrast': 0.1,
            'max_blur_variance': 100,  # Laplacianæ–¹å·®é˜ˆå€¼
            'min_saturation': 0.05,
            'max_noise_level': 0.3
        }
    
    def assess_image_quality(self, image: np.ndarray) -> Dict[str, Any]:
        """
        è¯„ä¼°å›¾åƒè´¨é‡
        
        Args:
            image: è¾“å…¥å›¾åƒ (H, W, C) æˆ– (H, W)
            
        Returns:
            è´¨é‡è¯„ä¼°ç»“æœå­—å…¸
        """
        if len(image.shape) == 3:
            height, width, channels = image.shape
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if channels == 3 else image[:,:,0]
        else:
            height, width = image.shape
            gray = image
            channels = 1
        
        # åŸºæœ¬å°ºå¯¸æ£€æŸ¥
        resolution_ok = (
            width >= self.quality_thresholds['min_resolution'][0] and
            height >= self.quality_thresholds['min_resolution'][1] and
            width <= self.quality_thresholds['max_resolution'][0] and
            height <= self.quality_thresholds['max_resolution'][1]
        )
        
        # äº®åº¦æ£€æŸ¥
        mean_brightness = np.mean(gray)
        brightness_ok = (
            self.quality_thresholds['min_brightness'] <= mean_brightness <= 
            self.quality_thresholds['max_brightness']
        )
        
        # å¯¹æ¯”åº¦æ£€æŸ¥
        contrast = np.std(gray) / 255.0
        contrast_ok = contrast >= self.quality_thresholds['min_contrast']
        
        # æ¨¡ç³Šæ£€æŸ¥ï¼ˆä½¿ç”¨Laplacianæ–¹å·®ï¼‰
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        blur_ok = laplacian_var >= self.quality_thresholds['max_blur_variance']
        
        # é¥±å’Œåº¦æ£€æŸ¥ï¼ˆä»…å¯¹å½©è‰²å›¾åƒï¼‰
        saturation_ok = True
        if len(image.shape) == 3 and channels == 3:
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            saturation = np.mean(hsv[:,:,1]) / 255.0
            saturation_ok = saturation >= self.quality_thresholds['min_saturation']
        
        # å™ªå£°æ£€æŸ¥ï¼ˆç®€åŒ–å®ç°ï¼‰
        noise_level = np.std(gray) / np.mean(gray) if np.mean(gray) > 0 else 0
        noise_ok = noise_level <= self.quality_thresholds['max_noise_level']
        
        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality_checks = [resolution_ok, brightness_ok, contrast_ok, blur_ok, saturation_ok, noise_ok]
        quality_score = sum(quality_checks) / len(quality_checks)
        
        return {
            'overall_quality': quality_score,
            'is_good_quality': quality_score >= 0.7,
            'resolution': (width, height),
            'resolution_ok': resolution_ok,
            'brightness': mean_brightness,
            'brightness_ok': brightness_ok,
            'contrast': contrast,
            'contrast_ok': contrast_ok,
            'blur_variance': laplacian_var,
            'blur_ok': blur_ok,
            'saturation_ok': saturation_ok,
            'noise_level': noise_level,
            'noise_ok': noise_ok,
            'issues': self._identify_issues(
                resolution_ok, brightness_ok, contrast_ok, 
                blur_ok, saturation_ok, noise_ok
            )
        }
    
    def _identify_issues(self, resolution_ok: bool, brightness_ok: bool, 
                        contrast_ok: bool, blur_ok: bool, 
                        saturation_ok: bool, noise_ok: bool) -> List[str]:
        """è¯†åˆ«å›¾åƒè´¨é‡é—®é¢˜"""
        issues = []
        if not resolution_ok:
            issues.append("åˆ†è¾¨ç‡å¼‚å¸¸")
        if not brightness_ok:
            issues.append("äº®åº¦å¼‚å¸¸")
        if not contrast_ok:
            issues.append("å¯¹æ¯”åº¦è¿‡ä½")
        if not blur_ok:
            issues.append("å›¾åƒæ¨¡ç³Š")
        if not saturation_ok:
            issues.append("é¥±å’Œåº¦è¿‡ä½")
        if not noise_ok:
            issues.append("å™ªå£°è¿‡å¤š")
        return issues

class PlantDiseasePreprocessor:
    """æ¤ç‰©ç—…å®³å›¾åƒé¢„å¤„ç†å™¨"""
    
    def __init__(self, 
                 input_size: Tuple[int, int] = (224, 224),
                 mode: PreprocessingMode = PreprocessingMode.TRAINING):
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            input_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            mode: é¢„å¤„ç†æ¨¡å¼
        """
        if not ALBUMENTATIONS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…albumentationsåº“: pip install albumentations")
        
        self.input_size = input_size
        self.mode = mode
        self.quality_assessor = ImageQualityAssessment()
        
        # åˆ›å»ºé¢„å¤„ç†ç®¡é“
        self.transforms = self._create_transforms()
        
        logger.info(f"æ¤ç‰©ç—…å®³é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å¼: {mode.value}, å°ºå¯¸: {input_size}")
    
    def _create_transforms(self) -> A.Compose:
        """åˆ›å»ºAlbumentationså˜æ¢ç®¡é“"""
        
        if self.mode == PreprocessingMode.TRAINING:
            return self._create_training_transforms()
        elif self.mode == PreprocessingMode.VALIDATION:
            return self._create_validation_transforms()
        elif self.mode == PreprocessingMode.INFERENCE:
            return self._create_inference_transforms()
        else:  # QUALITY_CHECK
            return self._create_quality_check_transforms()
    
    def _create_training_transforms(self) -> A.Compose:
        """åˆ›å»ºè®­ç»ƒæ—¶çš„æ•°æ®å¢å¼ºç®¡é“"""
        return A.Compose([
            # åŸºç¡€å‡ ä½•å˜æ¢
            A.Resize(height=self.input_size[0] + 32, width=self.input_size[1] + 32),
            A.RandomCrop(height=self.input_size[0], width=self.input_size[1]),
            
            # æ¤ç‰©å›¾åƒä¸“ç”¨çš„å‡ ä½•å¢å¼º
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.2),  # æ¤ç‰©å¯èƒ½å€’ç½®ç”Ÿé•¿
            A.RandomRotate90(p=0.3),
            A.Rotate(limit=15, p=0.4),  # å°è§’åº¦æ—‹è½¬
            A.Affine(
                translate_percent={'x': (-0.1, 0.1), 'y': (-0.1, 0.1)},
                scale=(0.8, 1.2),
                rotate=(-10, 10),
                p=0.3
            ),
            
            # å…‰ç…§å’Œé¢œè‰²å¢å¼ºï¼ˆæ¨¡æ‹Ÿä¸åŒæ‹æ‘„æ¡ä»¶ï¼‰
            A.RandomBrightnessContrast(
                brightness_limit=0.2,
                contrast_limit=0.2,
                p=0.5
            ),
            A.HueSaturationValue(
                hue_shift_limit=10,
                sat_shift_limit=20,
                val_shift_limit=15,
                p=0.4
            ),
            A.ColorJitter(
                brightness=0.1,
                contrast=0.1,
                saturation=0.1,
                hue=0.05,
                p=0.3
            ),
            
            # æ¨¡æ‹Ÿä¸åŒå…‰ç…§æ¡ä»¶
            A.RandomShadow(p=0.2),
            
            # æ¨¡æ‹Ÿç›¸æœºæ•ˆæœ
            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
            A.Blur(blur_limit=3, p=0.1),
            A.MotionBlur(blur_limit=3, p=0.1),
            
            # è£å‰ªå’Œé®æŒ¡ï¼ˆæ¨¡æ‹Ÿéƒ¨åˆ†é®æŒ¡çš„å¶ç‰‡ï¼‰
            A.CoarseDropout(
                max_holes=3,
                max_height=32,
                max_width=32,
                fill_value=0,
                p=0.2
            ),
            
            # æœ€ç»ˆæ ‡å‡†åŒ–
            A.Normalize(
                mean=[0.485, 0.456, 0.406],  # ImageNetæ ‡å‡†
                std=[0.229, 0.224, 0.225],
                max_pixel_value=255.0
            ),
            ToTensorV2()
        ])
    
    def _create_validation_transforms(self) -> A.Compose:
        """åˆ›å»ºéªŒè¯æ—¶çš„é¢„å¤„ç†ç®¡é“"""
        return A.Compose([
            A.Resize(height=self.input_size[0], width=self.input_size[1]),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
                max_pixel_value=255.0
            ),
            ToTensorV2()
        ])
    
    def _create_inference_transforms(self) -> A.Compose:
        """åˆ›å»ºæ¨ç†æ—¶çš„é¢„å¤„ç†ç®¡é“"""
        return A.Compose([
            # å¤šå°ºåº¦æµ‹è¯•å¢å¼º
            A.Resize(height=self.input_size[0], width=self.input_size[1]),
            
            # è½»å¾®çš„è´¨é‡å¢å¼º
            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),
            A.Sharpen(alpha=(0.1, 0.3), lightness=(0.8, 1.2), p=0.2),
            
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
                max_pixel_value=255.0
            ),
            ToTensorV2()
        ])
    
    def _create_quality_check_transforms(self) -> A.Compose:
        """åˆ›å»ºè´¨é‡æ£€æŸ¥çš„é¢„å¤„ç†ç®¡é“"""
        return A.Compose([
            A.Resize(height=self.input_size[0], width=self.input_size[1]),
            # ä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä¿æŒåŸå§‹åƒç´ å€¼ç”¨äºè´¨é‡åˆ†æ
        ])
    
    def preprocess_image(self, 
                        image: Union[np.ndarray, str, Path],
                        return_quality_info: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, Any]]]:
        """
        é¢„å¤„ç†å•å¼ å›¾åƒ
        
        Args:
            image: è¾“å…¥å›¾åƒï¼ˆnumpyæ•°ç»„ã€æ–‡ä»¶è·¯å¾„æˆ–Pathå¯¹è±¡ï¼‰
            return_quality_info: æ˜¯å¦è¿”å›è´¨é‡ä¿¡æ¯
            
        Returns:
            é¢„å¤„ç†åçš„å›¾åƒå¼ é‡ï¼Œå¯é€‰è´¨é‡ä¿¡æ¯
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, (str, Path)):
            image_array = cv2.imread(str(image))
            if image_array is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image}")
            image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
        else:
            image_array = image.copy()
        
        # è´¨é‡è¯„ä¼°
        quality_info = None
        if return_quality_info or self.mode == PreprocessingMode.QUALITY_CHECK:
            quality_info = self.quality_assessor.assess_image_quality(image_array)
            
            # å¦‚æœè´¨é‡å¤ªå·®ï¼Œå¯ä»¥é€‰æ‹©è·³è¿‡æˆ–åº”ç”¨ä¿®å¤
            if not quality_info['is_good_quality']:
                logger.warning(f"å›¾åƒè´¨é‡è¾ƒå·®: {quality_info['issues']}")
                # åº”ç”¨è´¨é‡ä¿®å¤
                image_array = self._apply_quality_fixes(image_array, quality_info)
        
        # åº”ç”¨å˜æ¢
        try:
            if self.mode == PreprocessingMode.QUALITY_CHECK:
                # è´¨é‡æ£€æŸ¥æ¨¡å¼ä¸åº”ç”¨æ ‡å‡†åŒ–
                transformed = self.transforms(image=image_array)
                result = transformed['image']
            else:
                transformed = self.transforms(image=image_array)
                result = transformed['image']
            
            if return_quality_info:
                return result, quality_info
            else:
                return result
                
        except Exception as e:
            logger.error(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _apply_quality_fixes(self, image: np.ndarray, quality_info: Dict[str, Any]) -> np.ndarray:
        """åº”ç”¨å›¾åƒè´¨é‡ä¿®å¤"""
        fixed_image = image.copy()
        
        # äº®åº¦ä¿®å¤
        if not quality_info['brightness_ok']:
            if quality_info['brightness'] < 50:
                # å›¾åƒè¿‡æš—ï¼Œå¢åŠ äº®åº¦
                fixed_image = cv2.convertScaleAbs(fixed_image, alpha=1.2, beta=30)
            elif quality_info['brightness'] > 200:
                # å›¾åƒè¿‡äº®ï¼Œé™ä½äº®åº¦
                fixed_image = cv2.convertScaleAbs(fixed_image, alpha=0.8, beta=-20)
        
        # å¯¹æ¯”åº¦ä¿®å¤
        if not quality_info['contrast_ok']:
            # åº”ç”¨CLAHEå¢å¼ºå¯¹æ¯”åº¦
            if len(fixed_image.shape) == 3:
                lab = cv2.cvtColor(fixed_image, cv2.COLOR_RGB2LAB)
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                lab[:,:,0] = clahe.apply(lab[:,:,0])
                fixed_image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            else:
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                fixed_image = clahe.apply(fixed_image)
        
        # å™ªå£°ä¿®å¤
        if not quality_info['noise_ok']:
            # åº”ç”¨åŒè¾¹æ»¤æ³¢å»å™ª
            fixed_image = cv2.bilateralFilter(fixed_image, 9, 75, 75)
        
        return fixed_image
    
    def preprocess_batch(self, 
                        images: List[Union[np.ndarray, str, Path]],
                        return_quality_info: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, List[Dict[str, Any]]]]:
        """
        æ‰¹é‡é¢„å¤„ç†å›¾åƒ
        
        Args:
            images: å›¾åƒåˆ—è¡¨
            return_quality_info: æ˜¯å¦è¿”å›è´¨é‡ä¿¡æ¯
            
        Returns:
            æ‰¹é‡é¢„å¤„ç†åçš„å›¾åƒå¼ é‡ï¼Œå¯é€‰è´¨é‡ä¿¡æ¯åˆ—è¡¨
        """
        processed_images = []
        quality_infos = []
        
        for image in images:
            if return_quality_info:
                processed_img, quality_info = self.preprocess_image(image, return_quality_info=True)
                processed_images.append(processed_img)
                quality_infos.append(quality_info)
            else:
                processed_img = self.preprocess_image(image, return_quality_info=False)
                processed_images.append(processed_img)
        
        # å †å ä¸ºæ‰¹æ¬¡å¼ é‡
        batch_tensor = torch.stack(processed_images)
        
        if return_quality_info:
            return batch_tensor, quality_infos
        else:
            return batch_tensor
    
    def create_test_time_augmentation(self, image: Union[np.ndarray, str, Path]) -> torch.Tensor:
        """
        åˆ›å»ºæµ‹è¯•æ—¶å¢å¼ºï¼ˆTTAï¼‰
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            å¢å¼ºåçš„å›¾åƒæ‰¹æ¬¡ (N, C, H, W)
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, (str, Path)):
            image_array = cv2.imread(str(image))
            image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
        else:
            image_array = image.copy()
        
        # åˆ›å»ºTTAå˜æ¢
        tta_transforms = [
            A.Compose([A.Resize(*self.input_size), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),  # åŸå›¾
            A.Compose([A.HorizontalFlip(p=1.0), A.Resize(*self.input_size), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),  # æ°´å¹³ç¿»è½¬
            A.Compose([A.Rotate(limit=5, p=1.0), A.Resize(*self.input_size), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),  # è½»å¾®æ—‹è½¬
            A.Compose([A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0), A.Resize(*self.input_size), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),  # äº®åº¦è°ƒæ•´
        ]
        
        augmented_images = []
        for transform in tta_transforms:
            augmented = transform(image=image_array)['image']
            augmented_images.append(augmented)
        
        return torch.stack(augmented_images)

class PreprocessingPipeline:
    """é¢„å¤„ç†ç®¡é“ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç®¡é“ç®¡ç†å™¨"""
        self.preprocessors = {}
        self.statistics = {
            'processed_images': 0,
            'quality_issues': 0,
            'processing_times': []
        }
    
    def get_preprocessor(self, 
                        mode: PreprocessingMode,
                        input_size: Tuple[int, int] = (224, 224)) -> PlantDiseasePreprocessor:
        """è·å–æˆ–åˆ›å»ºé¢„å¤„ç†å™¨"""
        key = f"{mode.value}_{input_size[0]}x{input_size[1]}"
        
        if key not in self.preprocessors:
            self.preprocessors[key] = PlantDiseasePreprocessor(
                input_size=input_size,
                mode=mode
            )
        
        return self.preprocessors[key]
    
    def process_dataset_sample(self, 
                             image_paths: List[str],
                             mode: PreprocessingMode = PreprocessingMode.TRAINING,
                             sample_size: int = 100) -> Dict[str, Any]:
        """å¤„ç†æ•°æ®é›†æ ·æœ¬ä»¥è¯„ä¼°é¢„å¤„ç†æ•ˆæœ"""
        preprocessor = self.get_preprocessor(mode)
        
        # éšæœºé‡‡æ ·
        import random
        sampled_paths = random.sample(image_paths, min(sample_size, len(image_paths)))
        
        results = {
            'total_processed': 0,
            'quality_issues': 0,
            'quality_distribution': {},
            'processing_times': [],
            'sample_results': []
        }
        
        for img_path in sampled_paths:
            try:
                import time
                start_time = time.time()
                
                processed_img, quality_info = preprocessor.preprocess_image(
                    img_path, return_quality_info=True
                )
                
                processing_time = time.time() - start_time
                results['processing_times'].append(processing_time)
                results['total_processed'] += 1
                
                if not quality_info['is_good_quality']:
                    results['quality_issues'] += 1
                
                # ç»Ÿè®¡è´¨é‡åˆ†å¸ƒ
                quality_score = quality_info['overall_quality']
                quality_bin = f"{int(quality_score * 10) * 10}-{int(quality_score * 10) * 10 + 10}%"
                results['quality_distribution'][quality_bin] = results['quality_distribution'].get(quality_bin, 0) + 1
                
                results['sample_results'].append({
                    'path': img_path,
                    'quality_score': quality_score,
                    'issues': quality_info['issues'],
                    'processing_time': processing_time
                })
                
            except Exception as e:
                logger.error(f"å¤„ç†å›¾åƒå¤±è´¥ {img_path}: {e}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if results['processing_times']:
            results['avg_processing_time'] = np.mean(results['processing_times'])
            results['quality_issue_rate'] = results['quality_issues'] / results['total_processed']
        
        return results

# å…¨å±€ç®¡é“å®ä¾‹
preprocessing_pipeline = PreprocessingPipeline()

def get_preprocessing_pipeline() -> PreprocessingPipeline:
    """è·å–å…¨å±€é¢„å¤„ç†ç®¡é“å®ä¾‹"""
    return preprocessing_pipeline

def create_plant_preprocessor(mode: PreprocessingMode = PreprocessingMode.TRAINING,
                            input_size: Tuple[int, int] = (224, 224)) -> PlantDiseasePreprocessor:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºæ¤ç‰©ç—…å®³é¢„å¤„ç†å™¨"""
    return PlantDiseasePreprocessor(input_size=input_size, mode=mode)

if __name__ == "__main__":
    # æµ‹è¯•å›¾åƒé¢„å¤„ç†ç®¡é“
    print("ğŸ§ª å›¾åƒé¢„å¤„ç†ç®¡é“æµ‹è¯•")
    print("=" * 60)
    
    if not ALBUMENTATIONS_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•é¢„å¤„ç†å™¨åˆ›å»º
        print("ğŸ“‹ æµ‹è¯•é¢„å¤„ç†å™¨åˆ›å»º...")
        
        # åˆ›å»ºä¸åŒæ¨¡å¼çš„é¢„å¤„ç†å™¨
        train_preprocessor = create_plant_preprocessor(PreprocessingMode.TRAINING)
        val_preprocessor = create_plant_preprocessor(PreprocessingMode.VALIDATION)
        inference_preprocessor = create_plant_preprocessor(PreprocessingMode.INFERENCE)
        
        print(f"âœ… é¢„å¤„ç†å™¨åˆ›å»ºæˆåŠŸ:")
        print(f"   è®­ç»ƒæ¨¡å¼: {train_preprocessor.mode.value}")
        print(f"   éªŒè¯æ¨¡å¼: {val_preprocessor.mode.value}")
        print(f"   æ¨ç†æ¨¡å¼: {inference_preprocessor.mode.value}")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        print(f"\nğŸ” æµ‹è¯•å›¾åƒå¤„ç†...")
        test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        
        # æµ‹è¯•è®­ç»ƒæ¨¡å¼é¢„å¤„ç†
        processed_train = train_preprocessor.preprocess_image(test_image)
        print(f"âœ… è®­ç»ƒæ¨¡å¼å¤„ç†å®Œæˆ:")
        print(f"   è¾“å‡ºå½¢çŠ¶: {processed_train.shape}")
        print(f"   æ•°æ®ç±»å‹: {processed_train.dtype}")
        print(f"   æ•°å€¼èŒƒå›´: [{processed_train.min():.3f}, {processed_train.max():.3f}]")
        
        # æµ‹è¯•è´¨é‡è¯„ä¼°
        print(f"\nğŸ“Š æµ‹è¯•å›¾åƒè´¨é‡è¯„ä¼°...")
        quality_assessor = ImageQualityAssessment()
        quality_info = quality_assessor.assess_image_quality(test_image)
        
        print(f"âœ… è´¨é‡è¯„ä¼°å®Œæˆ:")
        print(f"   æ•´ä½“è´¨é‡: {quality_info['overall_quality']:.2f}")
        print(f"   è´¨é‡è‰¯å¥½: {quality_info['is_good_quality']}")
        print(f"   åˆ†è¾¨ç‡: {quality_info['resolution']}")
        print(f"   äº®åº¦: {quality_info['brightness']:.1f}")
        print(f"   å¯¹æ¯”åº¦: {quality_info['contrast']:.3f}")
        
        # æµ‹è¯•TTA
        print(f"\nğŸ”„ æµ‹è¯•æµ‹è¯•æ—¶å¢å¼º...")
        tta_batch = inference_preprocessor.create_test_time_augmentation(test_image)
        print(f"âœ… TTAç”Ÿæˆå®Œæˆ:")
        print(f"   æ‰¹æ¬¡å½¢çŠ¶: {tta_batch.shape}")
        print(f"   å¢å¼ºæ•°é‡: {tta_batch.shape[0]}")
        
        # æµ‹è¯•ç®¡é“ç®¡ç†å™¨
        print(f"\nğŸ”§ æµ‹è¯•ç®¡é“ç®¡ç†å™¨...")
        pipeline = get_preprocessing_pipeline()
        
        # è·å–é¢„å¤„ç†å™¨
        train_proc = pipeline.get_preprocessor(PreprocessingMode.TRAINING)
        val_proc = pipeline.get_preprocessor(PreprocessingMode.VALIDATION)
        
        print(f"âœ… ç®¡é“ç®¡ç†å™¨æµ‹è¯•å®Œæˆ:")
        print(f"   ç¼“å­˜çš„é¢„å¤„ç†å™¨æ•°é‡: {len(pipeline.preprocessors)}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… å›¾åƒé¢„å¤„ç†ç®¡é“æµ‹è¯•å®Œæˆ")