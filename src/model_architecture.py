#!/usr/bin/env python3
"""
EfficientNet-B4æ¨¡å‹æ¶æ„
å®ç°æ¤ç‰©ç—…å®³è¯†åˆ«çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„
"""

import os
import sys
import math
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.hub import load_state_dict_from_url
    import torchvision.models as models
    from efficientnet_pytorch import EfficientNet
    import numpy as np
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PlantDiseaseEfficientNet(nn.Module):
    """æ¤ç‰©ç—…å®³è¯†åˆ«EfficientNetæ¨¡å‹"""
    
    def __init__(self, 
                 num_classes: int = 38,
                 model_name: str = 'efficientnet-b4',
                 pretrained: bool = True,
                 dropout_rate: float = 0.3,
                 drop_connect_rate: float = 0.2):
        """
        åˆå§‹åŒ–EfficientNetæ¨¡å‹
        
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            model_name: æ¨¡å‹åç§°
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            dropout_rate: Dropoutç‡
            drop_connect_rate: DropConnectç‡
        """
        super(PlantDiseaseEfficientNet, self).__init__()
        
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…å¿…è¦ä¾èµ–")
        
        self.num_classes = num_classes
        self.model_name = model_name
        self.dropout_rate = dropout_rate
        
        # åŠ è½½EfficientNetéª¨å¹²ç½‘ç»œ
        try:
            if pretrained:
                self.backbone = EfficientNet.from_pretrained(
                    model_name, 
                    num_classes=num_classes,
                    dropout_rate=dropout_rate,
                    drop_connect_rate=drop_connect_rate
                )
                logger.info(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {model_name}")
            else:
                self.backbone = EfficientNet.from_name(
                    model_name,
                    num_classes=num_classes,
                    dropout_rate=dropout_rate,
                    drop_connect_rate=drop_connect_rate
                )
        except Exception as e:
            logger.warning(f"åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
            self.backbone = EfficientNet.from_name(
                model_name,
                num_classes=num_classes,
                dropout_rate=dropout_rate,
                drop_connect_rate=drop_connect_rate
            )
        
        # è·å–ç‰¹å¾ç»´åº¦
        self.feature_dim = self.backbone._fc.in_features
        
        # æ›¿æ¢åˆ†ç±»å¤´
        self.backbone._fc = nn.Identity()
        
        # åˆ›å»ºè‡ªå®šä¹‰åˆ†ç±»å¤´
        self.classifier = self._create_classifier_head()
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        logger.info(f"PlantDiseaseEfficientNetåˆå§‹åŒ–å®Œæˆ: {model_name}, ç±»åˆ«æ•°: {num_classes}")
    
    def _create_classifier_head(self) -> nn.Module:
        """åˆ›å»ºåˆ†ç±»å¤´"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Dropout(self.dropout_rate),
            nn.Linear(self.feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout_rate / 2),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout_rate / 4),
            nn.Linear(256, self.num_classes)
        )
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # æå–ç‰¹å¾
        features = self.backbone.extract_features(x)
        
        # åˆ†ç±»
        output = self.classifier(features)
        
        return output
    
    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """æå–ç‰¹å¾å‘é‡"""
        features = self.backbone.extract_features(x)
        features = F.adaptive_avg_pool2d(features, 1)
        features = features.flatten(1)
        return features
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'feature_dim': self.feature_dim,
            'dropout_rate': self.dropout_rate
        }

class MultiScaleEfficientNet(nn.Module):
    """å¤šå°ºåº¦EfficientNetæ¨¡å‹"""
    
    def __init__(self, 
                 num_classes: int = 38,
                 scales: List[str] = None,
                 fusion_method: str = 'attention'):
        """
        åˆå§‹åŒ–å¤šå°ºåº¦æ¨¡å‹
        
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            scales: å°ºåº¦åˆ—è¡¨
            fusion_method: èåˆæ–¹æ³• ('concat', 'attention', 'weighted')
        """
        super(MultiScaleEfficientNet, self).__init__()
        
        if scales is None:
            scales = ['efficientnet-b2', 'efficientnet-b4', 'efficientnet-b5']
        
        self.scales = scales
        self.fusion_method = fusion_method
        self.num_classes = num_classes
        
        # åˆ›å»ºå¤šä¸ªå°ºåº¦çš„æ¨¡å‹
        self.scale_models = nn.ModuleDict()
        self.feature_dims = []
        
        for scale in scales:
            model = PlantDiseaseEfficientNet(
                num_classes=num_classes,
                model_name=scale,
                pretrained=True
            )
            # ç§»é™¤åˆ†ç±»å¤´ï¼Œåªä¿ç•™ç‰¹å¾æå–
            model.classifier = nn.Identity()
            self.scale_models[scale] = model
            self.feature_dims.append(model.feature_dim)
        
        # åˆ›å»ºèåˆå±‚
        self.fusion_layer = self._create_fusion_layer()
        
        # æœ€ç»ˆåˆ†ç±»å™¨
        self.final_classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(self._get_fusion_dim(), 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)
        )
        
        logger.info(f"MultiScaleEfficientNetåˆå§‹åŒ–å®Œæˆ: {len(scales)} ä¸ªå°ºåº¦")
    
    def _create_fusion_layer(self) -> nn.Module:
        """åˆ›å»ºç‰¹å¾èåˆå±‚"""
        if self.fusion_method == 'concat':
            return nn.Identity()
        elif self.fusion_method == 'attention':
            total_dim = sum(self.feature_dims)
            return nn.Sequential(
                nn.Linear(total_dim, total_dim // 4),
                nn.ReLU(inplace=True),
                nn.Linear(total_dim // 4, len(self.scales)),
                nn.Softmax(dim=1)
            )
        elif self.fusion_method == 'weighted':
            return nn.Parameter(torch.ones(len(self.scales)) / len(self.scales))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆæ–¹æ³•: {self.fusion_method}")
    
    def _get_fusion_dim(self) -> int:
        """è·å–èåˆåçš„ç‰¹å¾ç»´åº¦"""
        if self.fusion_method == 'concat':
            return sum(self.feature_dims)
        else:
            return max(self.feature_dims)  # å‡è®¾ä½¿ç”¨æœ€å¤§ç»´åº¦
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # æå–å¤šå°ºåº¦ç‰¹å¾
        scale_features = []
        for scale, model in self.scale_models.items():
            features = model.extract_features(x)
            scale_features.append(features)
        
        # ç‰¹å¾èåˆ
        if self.fusion_method == 'concat':
            fused_features = torch.cat(scale_features, dim=1)
        elif self.fusion_method == 'attention':
            stacked_features = torch.stack(scale_features, dim=1)  # [B, N, D]
            attention_weights = self.fusion_layer(torch.cat(scale_features, dim=1))
            attention_weights = attention_weights.unsqueeze(-1)  # [B, N, 1]
            fused_features = (stacked_features * attention_weights).sum(dim=1)
        elif self.fusion_method == 'weighted':
            stacked_features = torch.stack(scale_features, dim=1)  # [B, N, D]
            weights = F.softmax(self.fusion_layer, dim=0).unsqueeze(0).unsqueeze(-1)
            fused_features = (stacked_features * weights).sum(dim=1)
        
        # æœ€ç»ˆåˆ†ç±»
        output = self.final_classifier(fused_features)
        
        return output

class EnsembleEfficientNet(nn.Module):
    """é›†æˆEfficientNetæ¨¡å‹"""
    
    def __init__(self, 
                 num_classes: int = 38,
                 model_configs: List[Dict[str, Any]] = None,
                 ensemble_method: str = 'voting'):
        """
        åˆå§‹åŒ–é›†æˆæ¨¡å‹
        
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨
            ensemble_method: é›†æˆæ–¹æ³• ('voting', 'weighted', 'stacking')
        """
        super(EnsembleEfficientNet, self).__init__()
        
        if model_configs is None:
            model_configs = [
                {'model_name': 'efficientnet-b3', 'dropout_rate': 0.3},
                {'model_name': 'efficientnet-b4', 'dropout_rate': 0.3},
                {'model_name': 'efficientnet-b5', 'dropout_rate': 0.2}
            ]
        
        self.ensemble_method = ensemble_method
        self.num_classes = num_classes
        self.num_models = len(model_configs)
        
        # åˆ›å»ºå¤šä¸ªæ¨¡å‹
        self.models = nn.ModuleList()
        for i, config in enumerate(model_configs):
            model = PlantDiseaseEfficientNet(
                num_classes=num_classes,
                **config
            )
            self.models.append(model)
        
        # åˆ›å»ºé›†æˆå±‚
        if ensemble_method == 'weighted':
            self.ensemble_weights = nn.Parameter(torch.ones(self.num_models) / self.num_models)
        elif ensemble_method == 'stacking':
            # å †å å­¦ä¹ å™¨
            self.stacking_classifier = nn.Sequential(
                nn.Linear(num_classes * self.num_models, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(0.3),
                nn.Linear(256, num_classes)
            )
        
        logger.info(f"EnsembleEfficientNetåˆå§‹åŒ–å®Œæˆ: {self.num_models} ä¸ªæ¨¡å‹")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # è·å–æ‰€æœ‰æ¨¡å‹çš„è¾“å‡º
        model_outputs = []
        for model in self.models:
            output = model(x)
            model_outputs.append(output)
        
        # é›†æˆé¢„æµ‹
        if self.ensemble_method == 'voting':
            # ç®€å•å¹³å‡
            ensemble_output = torch.stack(model_outputs, dim=0).mean(dim=0)
        elif self.ensemble_method == 'weighted':
            # åŠ æƒå¹³å‡
            weights = F.softmax(self.ensemble_weights, dim=0)
            weighted_outputs = []
            for i, output in enumerate(model_outputs):
                weighted_outputs.append(output * weights[i])
            ensemble_output = torch.stack(weighted_outputs, dim=0).sum(dim=0)
        elif self.ensemble_method == 'stacking':
            # å †å å­¦ä¹ 
            stacked_input = torch.cat(model_outputs, dim=1)
            ensemble_output = self.stacking_classifier(stacked_input)
        
        return ensemble_output

class ModelFactory:
    """æ¨¡å‹å·¥å‚ç±»"""
    
    @staticmethod
    def create_model(model_type: str, 
                    num_classes: int = 38,
                    **kwargs) -> nn.Module:
        """
        åˆ›å»ºæ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            åˆ›å»ºçš„æ¨¡å‹
        """
        if model_type == 'efficientnet':
            return PlantDiseaseEfficientNet(num_classes=num_classes, **kwargs)
        elif model_type == 'multiscale':
            return MultiScaleEfficientNet(num_classes=num_classes, **kwargs)
        elif model_type == 'ensemble':
            return EnsembleEfficientNet(num_classes=num_classes, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    @staticmethod
    def get_model_info(model: nn.Module) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        
        info = {
            'model_type': type(model).__name__,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb,
            'num_classes': getattr(model, 'num_classes', 'unknown')
        }
        
        # æ·»åŠ æ¨¡å‹ç‰¹å®šä¿¡æ¯
        if hasattr(model, 'get_model_info'):
            info.update(model.get_model_info())
        
        return info

class ModelUtils:
    """æ¨¡å‹å·¥å…·ç±»"""
    
    @staticmethod
    def count_parameters(model: nn.Module) -> Dict[str, int]:
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params
        
        return {
            'total': total_params,
            'trainable': trainable_params,
            'frozen': frozen_params
        }
    
    @staticmethod
    def freeze_backbone(model: PlantDiseaseEfficientNet, freeze: bool = True):
        """å†»ç»“/è§£å†»éª¨å¹²ç½‘ç»œ"""
        for param in model.backbone.parameters():
            param.requires_grad = not freeze
        
        logger.info(f"éª¨å¹²ç½‘ç»œå·²{'å†»ç»“' if freeze else 'è§£å†»'}")
    
    @staticmethod
    def get_layer_names(model: nn.Module) -> List[str]:
        """è·å–æ¨¡å‹å±‚åç§°"""
        layer_names = []
        for name, _ in model.named_modules():
            if name:  # æ’é™¤æ ¹æ¨¡å—
                layer_names.append(name)
        return layer_names
    
    @staticmethod
    def calculate_model_flops(model: nn.Module, input_size: Tuple[int, int, int, int]) -> int:
        """è®¡ç®—æ¨¡å‹FLOPsï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ä¸“é—¨çš„å·¥å…·å¦‚thop
        total_params = sum(p.numel() for p in model.parameters())
        # ç²—ç•¥ä¼°è®¡ï¼šæ¯ä¸ªå‚æ•°å¤§çº¦å¯¹åº”2ä¸ªFLOPs
        estimated_flops = total_params * 2 * input_size[0]  # ä¹˜ä»¥batch size
        return estimated_flops

# ä¾¿æ·å‡½æ•°
def create_plant_disease_model(model_type: str = 'efficientnet',
                             num_classes: int = 38,
                             **kwargs) -> nn.Module:
    """åˆ›å»ºæ¤ç‰©ç—…å®³è¯†åˆ«æ¨¡å‹"""
    return ModelFactory.create_model(model_type, num_classes, **kwargs)

def load_pretrained_model(model_path: str, 
                         model_type: str = 'efficientnet',
                         num_classes: int = 38,
                         **kwargs) -> nn.Module:
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    model = create_plant_disease_model(model_type, num_classes, **kwargs)
    
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        logger.info(f"æ¨¡å‹æƒé‡å·²ä» {model_path} åŠ è½½")
    else:
        logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    return model

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹æ¶æ„
    print("ğŸ§ª EfficientNetæ¨¡å‹æ¶æ„æµ‹è¯•")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•åŸºç¡€EfficientNetæ¨¡å‹
        print("ğŸ“‹ æµ‹è¯•åŸºç¡€EfficientNetæ¨¡å‹...")
        model = create_plant_disease_model(
            'efficientnet', 
            num_classes=38,
            pretrained=True  # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        )
        
        model_info = ModelFactory.get_model_info(model)
        print(f"âœ… EfficientNetæ¨¡å‹åˆ›å»ºæˆåŠŸ:")
        print(f"   æ¨¡å‹ç±»å‹: {model_info['model_type']}")
        print(f"   æ€»å‚æ•°æ•°: {model_info['total_parameters']:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {model_info['trainable_parameters']:,}")
        print(f"   æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.2f} MB")
        print(f"   ç±»åˆ«æ•°: {model_info['num_classes']}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print(f"\nğŸ” æµ‹è¯•å‰å‘ä¼ æ’­...")
        test_input = torch.randn(2, 3, 224, 224)
        
        model.eval()
        with torch.no_grad():
            output = model(test_input)
            features = model.extract_features(test_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•å®Œæˆ:")
        print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"   è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
        
        # æµ‹è¯•å¤šå°ºåº¦æ¨¡å‹
        print(f"\nğŸ“ æµ‹è¯•å¤šå°ºåº¦æ¨¡å‹...")
        multiscale_model = create_plant_disease_model(
            'multiscale', 
            num_classes=38,
            scales=['efficientnet-b2', 'efficientnet-b4']
        )
        
        multiscale_info = ModelFactory.get_model_info(multiscale_model)
        print(f"âœ… å¤šå°ºåº¦æ¨¡å‹åˆ›å»ºæˆåŠŸ:")
        print(f"   æ€»å‚æ•°æ•°: {multiscale_info['total_parameters']:,}")
        print(f"   æ¨¡å‹å¤§å°: {multiscale_info['model_size_mb']:.2f} MB")
        
        # æµ‹è¯•é›†æˆæ¨¡å‹
        print(f"\nğŸ¯ æµ‹è¯•é›†æˆæ¨¡å‹...")
        ensemble_model = create_plant_disease_model(
            'ensemble',
            num_classes=38,
            model_configs=[
                {'model_name': 'efficientnet-b3', 'dropout_rate': 0.3},
                {'model_name': 'efficientnet-b4', 'dropout_rate': 0.2}
            ]
        )
        
        ensemble_info = ModelFactory.get_model_info(ensemble_model)
        print(f"âœ… é›†æˆæ¨¡å‹åˆ›å»ºæˆåŠŸ:")
        print(f"   æ€»å‚æ•°æ•°: {ensemble_info['total_parameters']:,}")
        print(f"   æ¨¡å‹å¤§å°: {ensemble_info['model_size_mb']:.2f} MB")
        
        # æµ‹è¯•æ¨¡å‹å·¥å…·
        print(f"\nğŸ”§ æµ‹è¯•æ¨¡å‹å·¥å…·...")
        param_stats = ModelUtils.count_parameters(model)
        layer_names = ModelUtils.get_layer_names(model)
        
        print(f"âœ… æ¨¡å‹å·¥å…·æµ‹è¯•å®Œæˆ:")
        print(f"   å‚æ•°ç»Ÿè®¡: {param_stats}")
        print(f"   å±‚æ•°é‡: {len(layer_names)}")
        print(f"   å‰5å±‚: {layer_names[:5]}")
        
        # æµ‹è¯•å†»ç»“/è§£å†»
        print(f"\nâ„ï¸ æµ‹è¯•å†»ç»“/è§£å†»åŠŸèƒ½...")
        ModelUtils.freeze_backbone(model, freeze=True)
        frozen_params = ModelUtils.count_parameters(model)
        
        ModelUtils.freeze_backbone(model, freeze=False)
        unfrozen_params = ModelUtils.count_parameters(model)
        
        print(f"âœ… å†»ç»“/è§£å†»æµ‹è¯•å®Œæˆ:")
        print(f"   å†»ç»“åå¯è®­ç»ƒå‚æ•°: {frozen_params['trainable']:,}")
        print(f"   è§£å†»åå¯è®­ç»ƒå‚æ•°: {unfrozen_params['trainable']:,}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… EfficientNetæ¨¡å‹æ¶æ„æµ‹è¯•å®Œæˆ")