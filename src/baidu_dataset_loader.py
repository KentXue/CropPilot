#!/usr/bin/env python3
"""
ç™¾åº¦AI Studioæ•°æ®é›†åŠ è½½å™¨
å®ç°ai_challenger_pdr2018æ•°æ®é›†çš„åŠ è½½å™¨ï¼ŒåŒ…å«ä¸PlantVillageçš„ç±»åˆ«æ˜ å°„
"""

import os
import sys
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from PIL import Image
    import torch
    from torch.utils.data import Dataset, DataLoader
    import numpy as np
    from torchvision import transforms
    PIL_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    PIL_AVAILABLE = False

from src.dataset_config import get_dataset_config
from src.plantvillage_loader import PlantVillageClassMapping

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaiduAIStudioClassMapping:
    """ç™¾åº¦AI Studioæ•°æ®é›†ç±»åˆ«æ˜ å°„ç®¡ç†"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç±»åˆ«æ˜ å°„"""
        # ç™¾åº¦AI Studioæ•°æ®é›†çš„æ•°å­—IDåˆ°ä¸­æ–‡ç±»åˆ«åç§°çš„æ˜ å°„
        # åŸºäºAI Challenger 2018æ¤ç‰©ç—…å®³è¯†åˆ«æ•°æ®é›†çš„61ä¸ªç±»åˆ«
        self.id_to_chinese = {
            0: 'è‹¹æœå¥åº·',
            1: 'è‹¹æœé»‘æ˜Ÿç—…',
            2: 'è‹¹æœé»‘è…ç—…',
            3: 'è‹¹æœé›ªæ¾é”ˆç—…',
            4: 'æ¨±æ¡ƒç™½ç²‰ç—…',
            5: 'æ¨±æ¡ƒå¥åº·',
            6: 'ç‰ç±³ç°æ–‘ç—…',
            7: 'ç‰ç±³æ™®é€šé”ˆç—…',
            8: 'ç‰ç±³åŒ—æ–¹å¶æ¯ç—…',
            9: 'ç‰ç±³å¥åº·',
            10: 'è‘¡è„é»‘è…ç—…',
            11: 'è‘¡è„é»‘éº»ç–¹ç—…',
            12: 'è‘¡è„å¶æ¯ç—…',
            13: 'è‘¡è„å¥åº·',
            14: 'æ¡ƒç»†èŒæ€§æ–‘ç‚¹ç—…',
            15: 'æ¡ƒå¥åº·',
            16: 'è¾£æ¤’ç»†èŒæ€§æ–‘ç‚¹ç—…',
            17: 'è¾£æ¤’å¥åº·',
            18: 'é©¬é“ƒè–¯æ—©ç–«ç—…',
            19: 'é©¬é“ƒè–¯æ™šç–«ç—…',
            20: 'é©¬é“ƒè–¯å¥åº·',
            21: 'è¦†ç›†å­å¥åº·',
            22: 'å¤§è±†å¥åº·',
            23: 'å—ç“œç™½ç²‰ç—…',
            24: 'è‰è“å¶ç„¦ç—…',
            25: 'è‰è“å¥åº·',
            26: 'ç•ªèŒ„ç»†èŒæ€§æ–‘ç‚¹ç—…',
            27: 'ç•ªèŒ„æ—©ç–«ç—…',
            28: 'ç•ªèŒ„æ™šç–«ç—…',
            29: 'ç•ªèŒ„å¶éœ‰ç—…',
            30: 'ç•ªèŒ„æ–‘ç‚¹ç—…',
            31: 'ç•ªèŒ„çº¢èœ˜è››',
            32: 'ç•ªèŒ„é¶æ–‘ç—…',
            33: 'ç•ªèŒ„é»„åŒ–æ›²å¶ç—…æ¯’',
            34: 'ç•ªèŒ„èŠ±å¶ç—…æ¯’',
            35: 'ç•ªèŒ„å¥åº·',
            36: 'æ©™å­é»„é¾™ç—…',
            37: 'è“è“å¥åº·',
            38: 'æ°´ç¨»ç¨»ç˜Ÿç—…',
            39: 'æ°´ç¨»è¤æ–‘ç—…',
            40: 'æ°´ç¨»å¥åº·',
            41: 'å°éº¦æ¡çº¹èŠ±å¶ç—…',
            42: 'å°éº¦å¶é”ˆç—…',
            43: 'å°éº¦å¥åº·',
            44: 'æ£‰èŠ±ç»†èŒæ€§ç–«ç—…',
            45: 'æ£‰èŠ±å¥åº·',
            46: 'èŒ„å­å¥åº·',
            47: 'èŒ„å­ç»†èŒæ€§æ–‘ç‚¹ç—…',
            48: 'é»„ç“œéœœéœ‰ç—…',
            49: 'é»„ç“œå¥åº·',
            50: 'è±†è§’é”ˆç—…',
            51: 'è±†è§’å¥åº·',
            52: 'ç™½èœè½¯è…ç—…',
            53: 'ç™½èœå¥åº·',
            54: 'èåœé»‘è…ç—…',
            55: 'èåœå¥åº·',
            56: 'èŠ±ç”Ÿå¶æ–‘ç—…',
            57: 'èŠ±ç”Ÿå¥åº·',
            58: 'å‘æ—¥è‘µé”ˆç—…',
            59: 'å‘æ—¥è‘µå¥åº·',
            60: 'å…¶ä»–ç—…å®³'
        }
        
        # ä¸­æ–‡ç±»åˆ«åˆ°PlantVillageè‹±æ–‡ç±»åˆ«çš„æ˜ å°„
        self.chinese_to_plantvillage = {
            # è‹¹æœç±»
            'è‹¹æœå¥åº·': 'Apple___healthy',
            'è‹¹æœé»‘æ˜Ÿç—…': 'Apple___Apple_scab',
            'è‹¹æœé»‘è…ç—…': 'Apple___Black_rot',
            'è‹¹æœé›ªæ¾é”ˆç—…': 'Apple___Cedar_apple_rust',
            
            # æ¨±æ¡ƒç±»
            'æ¨±æ¡ƒç™½ç²‰ç—…': 'Cherry_(including_sour)___Powdery_mildew',
            'æ¨±æ¡ƒå¥åº·': 'Cherry_(including_sour)___healthy',
            
            # ç‰ç±³ç±»
            'ç‰ç±³ç°æ–‘ç—…': 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',
            'ç‰ç±³æ™®é€šé”ˆç—…': 'Corn_(maize)___Common_rust_',
            'ç‰ç±³åŒ—æ–¹å¶æ¯ç—…': 'Corn_(maize)___Northern_Leaf_Blight',
            'ç‰ç±³å¥åº·': 'Corn_(maize)___healthy',
            
            # è‘¡è„ç±»
            'è‘¡è„é»‘è…ç—…': 'Grape___Black_rot',
            'è‘¡è„é»‘éº»ç–¹ç—…': 'Grape___Esca_(Black_Measles)',
            'è‘¡è„å¶æ¯ç—…': 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',
            'è‘¡è„å¥åº·': 'Grape___healthy',
            
            # æ¡ƒå­ç±»
            'æ¡ƒç»†èŒæ€§æ–‘ç‚¹ç—…': 'Peach___Bacterial_spot',
            'æ¡ƒå¥åº·': 'Peach___healthy',
            
            # è¾£æ¤’ç±»
            'è¾£æ¤’ç»†èŒæ€§æ–‘ç‚¹ç—…': 'Pepper,_bell___Bacterial_spot',
            'è¾£æ¤’å¥åº·': 'Pepper,_bell___healthy',
            
            # é©¬é“ƒè–¯ç±»
            'é©¬é“ƒè–¯æ—©ç–«ç—…': 'Potato___Early_blight',
            'é©¬é“ƒè–¯æ™šç–«ç—…': 'Potato___Late_blight',
            'é©¬é“ƒè–¯å¥åº·': 'Potato___healthy',
            
            # è¦†ç›†å­ç±»
            'è¦†ç›†å­å¥åº·': 'Raspberry___healthy',
            
            # å¤§è±†ç±»
            'å¤§è±†å¥åº·': 'Soybean___healthy',
            
            # å—ç“œç±»
            'å—ç“œç™½ç²‰ç—…': 'Squash___Powdery_mildew',
            
            # è‰è“ç±»
            'è‰è“å¶ç„¦ç—…': 'Strawberry___Leaf_scorch',
            'è‰è“å¥åº·': 'Strawberry___healthy',
            
            # ç•ªèŒ„ç±»
            'ç•ªèŒ„ç»†èŒæ€§æ–‘ç‚¹ç—…': 'Tomato___Bacterial_spot',
            'ç•ªèŒ„æ—©ç–«ç—…': 'Tomato___Early_blight',
            'ç•ªèŒ„æ™šç–«ç—…': 'Tomato___Late_blight',
            'ç•ªèŒ„å¶éœ‰ç—…': 'Tomato___Leaf_Mold',
            'ç•ªèŒ„æ–‘ç‚¹ç—…': 'Tomato___Septoria_leaf_spot',
            'ç•ªèŒ„çº¢èœ˜è››': 'Tomato___Spider_mites Two-spotted_spider_mite',
            'ç•ªèŒ„é¶æ–‘ç—…': 'Tomato___Target_Spot',
            'ç•ªèŒ„é»„åŒ–æ›²å¶ç—…æ¯’': 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',
            'ç•ªèŒ„èŠ±å¶ç—…æ¯’': 'Tomato___Tomato_mosaic_virus',
            'ç•ªèŒ„å¥åº·': 'Tomato___healthy',
            
            # æ©™å­ç±»
            'æ©™å­é»„é¾™ç—…': 'Orange___Haunglongbing_(Citrus_greening)',
            
            # è“è“ç±»
            'è“è“å¥åº·': 'Blueberry___healthy',
            
            # ä»¥ä¸‹ç±»åˆ«åœ¨PlantVillageä¸­æ²¡æœ‰å¯¹åº”ç±»åˆ«
            'æ°´ç¨»ç¨»ç˜Ÿç—…': None,
            'æ°´ç¨»è¤æ–‘ç—…': None,
            'æ°´ç¨»å¥åº·': None,
            'å°éº¦æ¡çº¹èŠ±å¶ç—…': None,
            'å°éº¦å¶é”ˆç—…': None,
            'å°éº¦å¥åº·': None,
            'æ£‰èŠ±ç»†èŒæ€§ç–«ç—…': None,
            'æ£‰èŠ±å¥åº·': None,
            'èŒ„å­å¥åº·': None,
            'èŒ„å­ç»†èŒæ€§æ–‘ç‚¹ç—…': None,
            'é»„ç“œéœœéœ‰ç—…': None,
            'é»„ç“œå¥åº·': None,
            'è±†è§’é”ˆç—…': None,
            'è±†è§’å¥åº·': None,
            'ç™½èœè½¯è…ç—…': None,
            'ç™½èœå¥åº·': None,
            'èåœé»‘è…ç—…': None,
            'èåœå¥åº·': None,
            'èŠ±ç”Ÿå¶æ–‘ç—…': None,
            'èŠ±ç”Ÿå¥åº·': None,
            'å‘æ—¥è‘µé”ˆç—…': None,
            'å‘æ—¥è‘µå¥åº·': None,
            'å…¶ä»–ç—…å®³': None
        }
        
        # åˆ›å»ºåå‘æ˜ å°„
        self.plantvillage_to_chinese = {v: k for k, v in self.chinese_to_plantvillage.items() if v is not None}
        
        # è·å–PlantVillageæ˜ å°„ç”¨äºä¸­æ–‡æ˜¾ç¤º
        self.plantvillage_mapping = PlantVillageClassMapping()
        
        # ç™¾åº¦æ•°æ®é›†ç‰¹æœ‰çš„ç±»åˆ«ï¼ˆPlantVillageä¸­æ²¡æœ‰çš„ï¼‰
        self.baidu_unique_classes = [k for k, v in self.chinese_to_plantvillage.items() if v is None]
        
        # å…±åŒç±»åˆ«
        self.common_classes = [k for k, v in self.chinese_to_plantvillage.items() if v is not None]
    
    def get_chinese_name(self, class_id: int) -> str:
        """æ ¹æ®æ•°å­—IDè·å–ä¸­æ–‡ç±»åˆ«åç§°"""
        return self.id_to_chinese.get(class_id, f'æœªçŸ¥ç±»åˆ«_{class_id}')
    
    def get_class_id(self, chinese_name: str) -> Optional[int]:
        """æ ¹æ®ä¸­æ–‡åç§°è·å–æ•°å­—ID"""
        for class_id, name in self.id_to_chinese.items():
            if name == chinese_name:
                return class_id
        return None
    
    def get_plantvillage_class(self, class_id: int) -> Optional[str]:
        """æ ¹æ®æ•°å­—IDè·å–å¯¹åº”çš„PlantVillageç±»åˆ«"""
        chinese_name = self.get_chinese_name(class_id)
        return self.chinese_to_plantvillage.get(chinese_name)
    
    def get_plantvillage_class_from_chinese(self, chinese_name: str) -> Optional[str]:
        """æ ¹æ®ä¸­æ–‡åç§°è·å–å¯¹åº”çš„PlantVillageç±»åˆ«"""
        return self.chinese_to_plantvillage.get(chinese_name)
    
    def is_common_class(self, class_id: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸PlantVillageå…±åŒçš„ç±»åˆ«"""
        chinese_name = self.get_chinese_name(class_id)
        return chinese_name in self.common_classes
    
    def is_baidu_unique(self, class_id: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç™¾åº¦ç‹¬æœ‰ç±»åˆ«"""
        chinese_name = self.get_chinese_name(class_id)
        return chinese_name in self.baidu_unique_classes
    
    def get_mapping_statistics(self) -> Dict[str, Any]:
        """è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_baidu_classes': len(self.id_to_chinese),
            'common_classes': len(self.common_classes),
            'baidu_unique_classes': len(self.baidu_unique_classes),
            'common_class_list': self.common_classes,
            'baidu_unique_list': self.baidu_unique_classes,
            'mapping_coverage': len(self.common_classes) / len(self.id_to_chinese),
            'class_id_range': f"0-{max(self.id_to_chinese.keys())}"
        }

class BaiduAIStudioDatasetLoader:
    """ç™¾åº¦AI Studioæ•°æ®é›†ä¸“ç”¨åŠ è½½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŠ è½½å™¨"""
        self.config = get_dataset_config()
        self.dataset_config = self.config.baidu_dataset
        self.class_mapping = BaiduAIStudioClassMapping()
        
        # æ•°æ®é›†ä¿¡æ¯
        self.dataset_info = None
        self.annotation_data = None
        self.class_distribution = None
        
        logger.info("ç™¾åº¦AI Studioæ•°æ®é›†åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_annotations(self) -> Dict[str, Any]:
        """åŠ è½½æ ‡æ³¨æ–‡ä»¶"""
        if not os.path.exists(self.dataset_config.path):
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.dataset_config.path}")
        
        logger.info("å¼€å§‹åŠ è½½ç™¾åº¦AI Studioæ ‡æ³¨æ–‡ä»¶...")
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = []
        txt_files = []
        
        for root, dirs, files in os.walk(self.dataset_config.path):
            for file in files:
                file_path = os.path.join(root, file)
                if file.endswith('.json'):
                    json_files.append(file_path)
                elif file.endswith('.txt') and 'README' not in file:
                    txt_files.append(file_path)
        
        logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶, {len(txt_files)} ä¸ªTXTæ–‡ä»¶")
        
        # åŠ è½½JSONæ ‡æ³¨
        all_annotations = []
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        all_annotations.extend(data)
                    elif isinstance(data, dict):
                        all_annotations.append(data)
                logger.info(f"æˆåŠŸåŠ è½½ {json_file}")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½JSONæ–‡ä»¶ {json_file}: {e}")
        
        # å¦‚æœæ²¡æœ‰JSONæ–‡ä»¶ï¼Œå°è¯•è§£æTXTæ–‡ä»¶
        if not all_annotations and txt_files:
            logger.info("æœªæ‰¾åˆ°JSONæ ‡æ³¨ï¼Œå°è¯•è§£æTXTæ–‡ä»¶...")
            all_annotations = self._parse_txt_annotations(txt_files)
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ ‡æ³¨ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­
        if not all_annotations:
            logger.info("æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­ç±»åˆ«...")
            all_annotations = self._infer_from_filenames()
        
        self.annotation_data = all_annotations
        logger.info(f"æ€»å…±åŠ è½½äº† {len(all_annotations)} ä¸ªæ ‡æ³¨")
        
        return {
            'total_annotations': len(all_annotations),
            'json_files': json_files,
            'txt_files': txt_files,
            'annotations': all_annotations[:5]  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
        }
    
    def _parse_txt_annotations(self, txt_files: List[str]) -> List[Dict[str, Any]]:
        """è§£æTXTæ ‡æ³¨æ–‡ä»¶"""
        annotations = []
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                for line in lines:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
                    parts = None
                    for sep in ['\t', ',', ' ', '|']:
                        if sep in line:
                            parts = line.split(sep)
                            break
                    
                    if parts and len(parts) >= 2:
                        image_name = parts[0].strip()
                        label = parts[1].strip()
                        
                        annotations.append({
                            'image': image_name,
                            'label': label,
                            'source': txt_file
                        })
                
                logger.info(f"ä» {txt_file} è§£æäº† {len([a for a in annotations if a.get('source') == txt_file])} ä¸ªæ ‡æ³¨")
                
            except Exception as e:
                logger.warning(f"æ— æ³•è§£æTXTæ–‡ä»¶ {txt_file}: {e}")
        
        return annotations
    
    def _infer_from_filenames(self) -> List[Dict[str, Any]]:
        """ä»æ–‡ä»¶åæ¨æ–­ç±»åˆ«"""
        annotations = []
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for root, dirs, files in os.walk(self.dataset_config.path):
            for file in files:
                if any(file.lower().endswith(ext) for ext in self.dataset_config.image_extensions):
                    image_files.append(os.path.join(root, file))
        
        # å°è¯•ä»æ–‡ä»¶åæˆ–ç›®å½•åæ¨æ–­ç±»åˆ«
        for img_path in image_files:
            rel_path = os.path.relpath(img_path, self.dataset_config.path)
            
            # ä»ç›®å½•ç»“æ„æ¨æ–­
            path_parts = rel_path.split(os.sep)
            
            # å‡è®¾ç±»åˆ«ä¿¡æ¯åœ¨ç›®å½•åæˆ–æ–‡ä»¶åä¸­
            possible_labels = []
            
            # æ£€æŸ¥ç›®å½•å
            for part in path_parts[:-1]:  # æ’é™¤æ–‡ä»¶å
                if any(keyword in part for keyword in ['train', 'val', 'test', 'images']):
                    continue
                possible_labels.append(part)
            
            # æ£€æŸ¥æ–‡ä»¶å
            filename = os.path.splitext(path_parts[-1])[0]
            if '_' in filename:
                possible_labels.extend(filename.split('_'))
            
            # é€‰æ‹©æœ€å¯èƒ½çš„æ ‡ç­¾
            label = 'unknown'
            if possible_labels:
                label = possible_labels[0]  # ç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ª
            
            annotations.append({
                'image': rel_path,
                'label': label,
                'source': 'filename_inference'
            })
        
        logger.info(f"ä»æ–‡ä»¶åæ¨æ–­äº† {len(annotations)} ä¸ªæ ‡æ³¨")
        return annotations
    
    def analyze_dataset(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®é›†ç»“æ„å’Œå†…å®¹"""
        if self.annotation_data is None:
            self.load_annotations()
        
        logger.info("å¼€å§‹åˆ†æç™¾åº¦AI Studioæ•°æ®é›†...")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        class_distribution = Counter()
        chinese_class_distribution = Counter()
        valid_annotations = []
        image_formats = Counter()
        
        # è·å–å›¾åƒç›®å½•
        image_dirs = []
        for root, dirs, files in os.walk(self.dataset_config.path):
            if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files):
                image_dirs.append(root)
        
        for ann in self.annotation_data:
            # è·å–å›¾åƒIDå’Œç—…å®³ç±»åˆ«ID
            image_id = ann.get('image_id', '')
            disease_class = ann.get('disease_class', -1)
            
            if not image_id or disease_class == -1:
                continue
            
            # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
            image_path = None
            for img_dir in image_dirs:
                potential_path = os.path.join(img_dir, image_id)
                if os.path.exists(potential_path):
                    image_path = potential_path
                    break
            
            if not image_path:
                continue
            
            # è·å–ä¸­æ–‡ç±»åˆ«åç§°
            chinese_name = self.class_mapping.get_chinese_name(disease_class)
            plantvillage_class = self.class_mapping.get_plantvillage_class(disease_class)
            
            # ç»Ÿè®¡æ ¼å¼
            ext = os.path.splitext(image_id)[1].lower()
            image_formats[ext] += 1
            
            # ç»Ÿè®¡ç±»åˆ«
            class_distribution[disease_class] += 1
            chinese_class_distribution[chinese_name] += 1
            
            valid_annotations.append({
                'image_path': image_path,
                'image_id': image_id,
                'disease_class_id': disease_class,
                'chinese_name': chinese_name,
                'plantvillage_class': plantvillage_class,
                'is_common': self.class_mapping.is_common_class(disease_class),
                'is_unique': self.class_mapping.is_baidu_unique(disease_class)
            })
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        self.dataset_info = {
            'dataset_name': 'Baidu_AI_Studio',
            'dataset_path': self.dataset_config.path,
            'total_annotations': len(self.annotation_data),
            'valid_annotations': len(valid_annotations),
            'total_classes': len(class_distribution),
            'class_distribution': dict(class_distribution),
            'chinese_class_distribution': dict(chinese_class_distribution),
            'image_formats': dict(image_formats),
            'mapping_stats': self.class_mapping.get_mapping_statistics(),
            'valid_data': valid_annotations
        }
        
        self.class_distribution = dict(chinese_class_distribution)
        
        logger.info(f"æ•°æ®é›†åˆ†æå®Œæˆ: {len(class_distribution)} ä¸ªç±»åˆ«, {len(valid_annotations)} ä¸ªæœ‰æ•ˆæ ‡æ³¨")
        
        return self.dataset_info
    
    def create_plantvillage_compatible_dataset(self) -> Dict[str, List[Dict[str, Any]]]:
        """åˆ›å»ºä¸PlantVillageå…¼å®¹çš„æ•°æ®é›†"""
        if self.dataset_info is None:
            self.analyze_dataset()
        
        logger.info("åˆ›å»ºä¸PlantVillageå…¼å®¹çš„æ•°æ®é›†...")
        
        compatible_data = {}
        incompatible_data = []
        
        for ann in self.dataset_info['valid_data']:
            if ann['is_common']:
                plantvillage_class = ann['plantvillage_class']
                if plantvillage_class not in compatible_data:
                    compatible_data[plantvillage_class] = []
                
                compatible_data[plantvillage_class].append({
                    'image_path': ann['image_path'],
                    'image_id': ann['image_id'],
                    'disease_class_id': ann['disease_class_id'],
                    'chinese_name': ann['chinese_name'],
                    'plantvillage_label': plantvillage_class,
                    'source': 'baidu'
                })
            else:
                incompatible_data.append(ann)
        
        logger.info(f"å…¼å®¹æ•°æ®: {len(compatible_data)} ä¸ªPlantVillageç±»åˆ«")
        logger.info(f"ä¸å…¼å®¹æ•°æ®: {len(incompatible_data)} ä¸ªæ ·æœ¬")
        
        return {
            'compatible': compatible_data,
            'incompatible': incompatible_data,
            'stats': {
                'compatible_classes': len(compatible_data),
                'compatible_samples': sum(len(samples) for samples in compatible_data.values()),
                'incompatible_samples': len(incompatible_data)
            }
        }
    
    def create_train_val_split(self, 
                              train_ratio: float = 0.8,
                              random_seed: int = 42) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """åˆ›å»ºè®­ç»ƒ/éªŒè¯é›†åˆ†å‰²"""
        compatible_data = self.create_plantvillage_compatible_dataset()
        
        random.seed(random_seed)
        
        train_split = {}
        val_split = {}
        
        for plantvillage_class, samples in compatible_data['compatible'].items():
            # éšæœºæ‰“ä¹±
            shuffled_samples = samples.copy()
            random.shuffle(shuffled_samples)
            
            # åˆ†å‰²
            split_idx = int(len(shuffled_samples) * train_ratio)
            train_split[plantvillage_class] = shuffled_samples[:split_idx]
            val_split[plantvillage_class] = shuffled_samples[split_idx:]
        
        train_total = sum(len(samples) for samples in train_split.values())
        val_total = sum(len(samples) for samples in val_split.values())
        
        logger.info(f"ç™¾åº¦æ•°æ®é›†åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {train_total} å¼ , éªŒè¯é›† {val_total} å¼ ")
        
        return train_split, val_split
    
    def generate_dataset_report(self, output_path: Optional[str] = None) -> str:
        """ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š"""
        if self.dataset_info is None:
            self.analyze_dataset()
        
        compatible_data = self.create_plantvillage_compatible_dataset()
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"# ç™¾åº¦AI Studioæ•°æ®é›†åˆ†ææŠ¥å‘Š",
            f"",
            f"## åŸºæœ¬ä¿¡æ¯",
            f"- **æ•°æ®é›†åç§°**: {self.dataset_info['dataset_name']}",
            f"- **æ•°æ®é›†è·¯å¾„**: {self.dataset_info['dataset_path']}",
            f"- **æ€»æ ‡æ³¨æ•°**: {self.dataset_info['total_annotations']}",
            f"- **æœ‰æ•ˆæ ‡æ³¨æ•°**: {self.dataset_info['valid_annotations']}",
            f"- **æ€»ç±»åˆ«æ•°**: {self.dataset_info['total_classes']}",
            f"",
            f"## ä¸PlantVillageçš„å…¼å®¹æ€§åˆ†æ",
        ]
        
        mapping_stats = self.dataset_info['mapping_stats']
        compat_stats = compatible_data['stats']
        
        report_lines.extend([
            f"- **æ˜ å°„è¦†ç›–ç‡**: {mapping_stats['mapping_coverage']:.2%}",
            f"- **å…±åŒç±»åˆ«æ•°**: {mapping_stats['common_classes']} / {mapping_stats['total_baidu_classes']}",
            f"- **å…¼å®¹æ ·æœ¬æ•°**: {compat_stats['compatible_samples']}",
            f"- **ä¸å…¼å®¹æ ·æœ¬æ•°**: {compat_stats['incompatible_samples']}",
            f"",
            f"## ç±»åˆ«åˆ†å¸ƒ",
            f""
        ])
        
        # æŒ‰æ ·æœ¬æ•°é‡æ’åºæ˜¾ç¤ºç±»åˆ«
        sorted_classes = sorted(
            self.dataset_info['chinese_class_distribution'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        report_lines.append("| ä¸­æ–‡ç±»åˆ« | PlantVillageç±»åˆ« | æ ·æœ¬æ•°é‡ | å…¼å®¹æ€§ |")
        report_lines.append("|----------|------------------|----------|--------|")
        
        for chinese_class, count in sorted_classes:
            pv_class = self.class_mapping.get_plantvillage_class_from_chinese(chinese_class)
            compatibility = "âœ… å…¼å®¹" if pv_class else "âŒ ä¸å…¼å®¹"
            pv_display = pv_class if pv_class else "æ— å¯¹åº”ç±»åˆ«"
            
            report_lines.append(
                f"| {chinese_class} | {pv_display} | {count} | {compatibility} |"
            )
        
        # æ·»åŠ å…¼å®¹ç±»åˆ«è¯¦æƒ…
        if compatible_data['compatible']:
            report_lines.extend([
                f"",
                f"## å…¼å®¹ç±»åˆ«è¯¦æƒ…",
                f""
            ])
            
            for pv_class, samples in compatible_data['compatible'].items():
                chinese_classes = list(set(s['chinese_name'] for s in samples))
                report_lines.append(f"- **{pv_class}**: {len(samples)} ä¸ªæ ·æœ¬")
                report_lines.append(f"  - ä¸­æ–‡ç±»åˆ«: {', '.join(chinese_classes)}")
        
        # æ·»åŠ ä¸å…¼å®¹ç±»åˆ«
        if compatible_data['incompatible']:
            unique_classes = list(set(ann['chinese_name'] for ann in compatible_data['incompatible']))
            report_lines.extend([
                f"",
                f"## ç™¾åº¦ç‹¬æœ‰ç±»åˆ«",
                f""
            ])
            
            for unique_class in unique_classes:
                count = sum(1 for ann in compatible_data['incompatible'] if ann['chinese_name'] == unique_class)
                report_lines.append(f"- **{unique_class}**: {count} ä¸ªæ ·æœ¬")
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"ç™¾åº¦æ•°æ®é›†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return report_content

# ä¾¿æ·å‡½æ•°
def create_baidu_loader() -> BaiduAIStudioDatasetLoader:
    """åˆ›å»ºç™¾åº¦AI Studioæ•°æ®é›†åŠ è½½å™¨"""
    return BaiduAIStudioDatasetLoader()

def get_baidu_class_mapping() -> BaiduAIStudioClassMapping:
    """è·å–ç™¾åº¦AI Studioç±»åˆ«æ˜ å°„"""
    return BaiduAIStudioClassMapping()

if __name__ == "__main__":
    # æµ‹è¯•ç™¾åº¦AI StudioåŠ è½½å™¨
    print("ğŸ§ª ç™¾åº¦AI Studioæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•")
    print("=" * 60)
    
    if not PIL_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•ç±»åˆ«æ˜ å°„
        print("ğŸ“‹ æµ‹è¯•ç±»åˆ«æ˜ å°„...")
        mapping = get_baidu_class_mapping()
        stats = mapping.get_mapping_statistics()
        
        print(f"âœ… ç±»åˆ«æ˜ å°„ç»Ÿè®¡:")
        print(f"   æ€»ç™¾åº¦ç±»åˆ«æ•°: {stats['total_baidu_classes']}")
        print(f"   å…±åŒç±»åˆ«æ•°: {stats['common_classes']}")
        print(f"   ç™¾åº¦ç‹¬æœ‰ç±»åˆ«æ•°: {stats['baidu_unique_classes']}")
        print(f"   æ˜ å°„è¦†ç›–ç‡: {stats['mapping_coverage']:.2%}")
        
        # æµ‹è¯•æ•°æ®é›†åŠ è½½å™¨
        print(f"\nğŸ” æµ‹è¯•ç™¾åº¦æ•°æ®é›†åŠ è½½å™¨...")
        loader = create_baidu_loader()
        
        # åŠ è½½æ ‡æ³¨
        ann_info = loader.load_annotations()
        print(f"âœ… æ ‡æ³¨åŠ è½½å®Œæˆ:")
        print(f"   æ€»æ ‡æ³¨æ•°: {ann_info['total_annotations']}")
        print(f"   JSONæ–‡ä»¶: {len(ann_info['json_files'])}")
        print(f"   TXTæ–‡ä»¶: {len(ann_info['txt_files'])}")
        
        # åˆ†ææ•°æ®é›†
        dataset_info = loader.analyze_dataset()
        print(f"âœ… æ•°æ®é›†åˆ†æå®Œæˆ:")
        print(f"   æœ‰æ•ˆæ ‡æ³¨æ•°: {dataset_info['valid_annotations']}")
        print(f"   æ€»ç±»åˆ«æ•°: {dataset_info['total_classes']}")
        
        # æµ‹è¯•å…¼å®¹æ€§åˆ†æ
        print(f"\nğŸ“Š æµ‹è¯•å…¼å®¹æ€§åˆ†æ...")
        compatible_data = loader.create_plantvillage_compatible_dataset()
        stats = compatible_data['stats']
        
        print(f"âœ… å…¼å®¹æ€§åˆ†æå®Œæˆ:")
        print(f"   å…¼å®¹ç±»åˆ«æ•°: {stats['compatible_classes']}")
        print(f"   å…¼å®¹æ ·æœ¬æ•°: {stats['compatible_samples']}")
        print(f"   ä¸å…¼å®¹æ ·æœ¬æ•°: {stats['incompatible_samples']}")
        
        # ç”ŸæˆæŠ¥å‘Š
        print(f"\nğŸ“„ ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š...")
        report_path = "Baidu_AI_Studio_dataset_report.md"
        report = loader.generate_dataset_report(report_path)
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… ç™¾åº¦AI Studioæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•å®Œæˆ")