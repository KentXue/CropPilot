#!/usr/bin/env python3
"""
æ•°æ®é›†ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†PlantVillageã€ç™¾åº¦AI Studioå’Œç‰©å€™æ•°æ®é›†
"""

import os
import sys
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import defaultdict, Counter
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from PIL import Image
    import torch
    from torch.utils.data import Dataset, DataLoader
    import numpy as np
    PIL_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    PIL_AVAILABLE = False

from src.dataset_config import get_dataset_config, ImageDatasetConfig, PhenologyDatasetConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PlantDiseaseDataset(Dataset):
    """æ¤ç‰©ç—…å®³æ•°æ®é›†ç±»"""
    
    def __init__(self, 
                 dataset_config: ImageDatasetConfig,
                 transform=None,
                 max_samples_per_class: Optional[int] = None,
                 class_subset: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            dataset_config: æ•°æ®é›†é…ç½®
            transform: å›¾åƒå˜æ¢
            max_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
            class_subset: ç±»åˆ«å­é›†ï¼ˆåªåŠ è½½æŒ‡å®šç±»åˆ«ï¼‰
        """
        self.config = dataset_config
        self.transform = transform
        self.max_samples_per_class = max_samples_per_class
        self.class_subset = class_subset
        
        # å­˜å‚¨æ•°æ®
        self.samples = []  # (image_path, class_index)
        self.classes = []  # ç±»åˆ«åç§°åˆ—è¡¨
        self.class_to_idx = {}  # ç±»åˆ«åç§°åˆ°ç´¢å¼•çš„æ˜ å°„
        
        # åŠ è½½æ•°æ®
        self._load_dataset()
        
        logger.info(f"æ•°æ®é›† {self.config.name} åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬, {len(self.classes)} ä¸ªç±»åˆ«")
    
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        if not os.path.exists(self.config.path):
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.config.path}")
        
        if self.config.has_subdirectories:
            self._load_from_subdirectories()
        else:
            self._load_from_annotations()
    
    def _load_from_subdirectories(self):
        """ä»å­ç›®å½•ç»“æ„åŠ è½½æ•°æ®ï¼ˆPlantVillageæ ¼å¼ï¼‰"""
        class_dirs = [d for d in os.listdir(self.config.path) 
                     if os.path.isdir(os.path.join(self.config.path, d))]
        
        # è¿‡æ»¤ç±»åˆ«å­é›†
        if self.class_subset:
            class_dirs = [d for d in class_dirs if d in self.class_subset]
        
        class_dirs.sort()
        self.classes = class_dirs
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}
        
        # æ”¶é›†æ ·æœ¬
        for class_name in self.classes:
            class_path = os.path.join(self.config.path, class_name)
            class_idx = self.class_to_idx[class_name]
            
            # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_files = []
            for ext in self.config.image_extensions:
                pattern = os.path.join(class_path, f"*{ext}")
                import glob
                image_files.extend(glob.glob(pattern))
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
            if self.max_samples_per_class and len(image_files) > self.max_samples_per_class:
                image_files = random.sample(image_files, self.max_samples_per_class)
            
            # æ·»åŠ æ ·æœ¬
            for img_path in image_files:
                self.samples.append((img_path, class_idx))
    
    def _load_from_annotations(self):
        """ä»æ ‡æ³¨æ–‡ä»¶åŠ è½½æ•°æ®ï¼ˆç™¾åº¦AI Studioæ ¼å¼ï¼‰"""
        # æŸ¥æ‰¾JSONæ ‡æ³¨æ–‡ä»¶
        annotation_files = []
        for root, dirs, files in os.walk(self.config.path):
            for file in files:
                if file.endswith('.json'):
                    annotation_files.append(os.path.join(root, file))
        
        if not annotation_files:
            logger.warning(f"åœ¨ {self.config.path} ä¸­æœªæ‰¾åˆ°JSONæ ‡æ³¨æ–‡ä»¶ï¼Œå°è¯•ç›´æ¥åŠ è½½å›¾åƒ")
            self._load_images_directly()
            return
        
        # åŠ è½½æ ‡æ³¨æ•°æ®
        all_annotations = []
        for ann_file in annotation_files:
            try:
                with open(ann_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        all_annotations.extend(data)
                    elif isinstance(data, dict):
                        all_annotations.append(data)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½æ ‡æ³¨æ–‡ä»¶ {ann_file}: {e}")
        
        if not all_annotations:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ ‡æ³¨æ•°æ®ï¼Œå°è¯•ç›´æ¥åŠ è½½å›¾åƒ")
            self._load_images_directly()
            return
        
        # å¤„ç†æ ‡æ³¨æ•°æ®
        self._process_annotations(all_annotations)
    
    def _load_images_directly(self):
        """ç›´æ¥åŠ è½½å›¾åƒæ–‡ä»¶ï¼ˆæ— æ ‡æ³¨ï¼‰"""
        image_files = []
        for root, dirs, files in os.walk(self.config.path):
            for file in files:
                if any(file.lower().endswith(ext) for ext in self.config.image_extensions):
                    image_files.append(os.path.join(root, file))
        
        # åˆ›å»ºå•ä¸€ç±»åˆ«
        self.classes = ['unknown']
        self.class_to_idx = {'unknown': 0}
        
        # æ·»åŠ æ ·æœ¬
        for img_path in image_files:
            self.samples.append((img_path, 0))
    
    def _process_annotations(self, annotations: List[Dict]):
        """å¤„ç†æ ‡æ³¨æ•°æ®"""
        # æå–æ‰€æœ‰ç±»åˆ«
        all_classes = set()
        valid_samples = []
        
        for ann in annotations:
            # å°è¯•ä¸åŒçš„æ ‡æ³¨æ ¼å¼
            image_path = None
            label = None
            
            # æ ¼å¼1: {"image": "path", "label": "class"}
            if 'image' in ann and 'label' in ann:
                image_path = ann['image']
                label = ann['label']
            # æ ¼å¼2: {"filename": "path", "class": "label"}
            elif 'filename' in ann and 'class' in ann:
                image_path = ann['filename']
                label = ann['class']
            # æ ¼å¼3: {"image_path": "path", "disease": "label"}
            elif 'image_path' in ann and 'disease' in ann:
                image_path = ann['image_path']
                label = ann['disease']
            
            if image_path and label:
                # æ„å»ºå®Œæ•´è·¯å¾„
                full_path = os.path.join(self.config.path, image_path)
                if os.path.exists(full_path):
                    all_classes.add(label)
                    valid_samples.append((full_path, label))
        
        # è®¾ç½®ç±»åˆ«æ˜ å°„
        self.classes = sorted(list(all_classes))
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}
        
        # è½¬æ¢æ ·æœ¬
        for img_path, label in valid_samples:
            class_idx = self.class_to_idx[label]
            self.samples.append((img_path, class_idx))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        img_path, class_idx = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½å›¾åƒ {img_path}: {e}")
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºå¤‡ç”¨
            image = Image.new('RGB', self.config.input_size, (0, 0, 0))
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        return image, class_idx
    
    def get_class_distribution(self) -> Dict[str, int]:
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        distribution = Counter()
        for _, class_idx in self.samples:
            class_name = self.classes[class_idx]
            distribution[class_name] += 1
        return dict(distribution)
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'name': self.config.name,
            'path': self.config.path,
            'total_samples': len(self.samples),
            'num_classes': len(self.classes),
            'classes': self.classes,
            'class_distribution': self.get_class_distribution(),
            'input_size': self.config.input_size
        }

class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨"""
        self.config = get_dataset_config()
        self.datasets = {}
        self.dataset_info = {}
        
        logger.info("æ•°æ®é›†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_dataset(self, 
                    dataset_name: str,
                    transform=None,
                    max_samples_per_class: Optional[int] = None,
                    class_subset: Optional[List[str]] = None) -> PlantDiseaseDataset:
        """
        åŠ è½½æŒ‡å®šæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§° ('color', 'grayscale', 'segmented', 'baidu')
            transform: å›¾åƒå˜æ¢
            max_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°
            class_subset: ç±»åˆ«å­é›†
            
        Returns:
            PlantDiseaseDatasetå®ä¾‹
        """
        # è·å–æ•°æ®é›†é…ç½®
        if dataset_name in ['color', 'grayscale', 'segmented']:
            dataset_config = self.config.plantvillage_datasets[dataset_name]
        elif dataset_name == 'baidu':
            dataset_config = self.config.baidu_dataset
        else:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®é›†åç§°: {dataset_name}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = PlantDiseaseDataset(
            dataset_config=dataset_config,
            transform=transform,
            max_samples_per_class=max_samples_per_class,
            class_subset=class_subset
        )
        
        # ç¼“å­˜æ•°æ®é›†
        cache_key = f"{dataset_name}_{max_samples_per_class}_{len(class_subset) if class_subset else 'all'}"
        self.datasets[cache_key] = dataset
        self.dataset_info[cache_key] = dataset.get_dataset_info()
        
        return dataset
    
    def create_combined_dataset(self, 
                              dataset_names: List[str],
                              transform=None,
                              max_samples_per_class: Optional[int] = None) -> PlantDiseaseDataset:
        """
        åˆ›å»ºç»„åˆæ•°æ®é›†
        
        Args:
            dataset_names: è¦ç»„åˆçš„æ•°æ®é›†åç§°åˆ—è¡¨
            transform: å›¾åƒå˜æ¢
            max_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°
            
        Returns:
            ç»„åˆçš„PlantDiseaseDataset
        """
        # åŠ è½½å„ä¸ªæ•°æ®é›†
        datasets = []
        for name in dataset_names:
            dataset = self.load_dataset(name, transform, max_samples_per_class)
            datasets.append(dataset)
        
        # åˆå¹¶æ•°æ®é›†ï¼ˆè¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘ï¼‰
        if len(datasets) == 1:
            return datasets[0]
        
        # åˆ›å»ºåˆå¹¶åçš„æ•°æ®é›†
        primary_dataset = datasets[0]
        combined_samples = []
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬
        for dataset in datasets:
            combined_samples.extend(dataset.samples)
        
        # åˆ›å»ºæ–°çš„æ•°æ®é›†å®ä¾‹
        combined_config = primary_dataset.config
        combined_dataset = PlantDiseaseDataset(combined_config, transform)
        combined_dataset.samples = combined_samples
        combined_dataset.classes = primary_dataset.classes
        combined_dataset.class_to_idx = primary_dataset.class_to_idx
        
        return combined_dataset
    
    def get_dataset_statistics(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'available_datasets': list(self.config.get_all_image_datasets().keys()),
            'path_validation': self.config.validate_paths(),
            'loaded_datasets': list(self.dataset_info.keys()),
            'dataset_details': self.dataset_info
        }
        return stats
    
    def create_data_loaders(self,
                          dataset: PlantDiseaseDataset,
                          batch_size: int = 32,
                          train_split: float = 0.8,
                          shuffle: bool = True,
                          num_workers: int = 0) -> Tuple[DataLoader, DataLoader]:
        """
        åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: æ•°æ®é›†
            batch_size: æ‰¹å¤§å°
            train_split: è®­ç»ƒé›†æ¯”ä¾‹
            shuffle: æ˜¯å¦æ‰“ä¹±
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            
        Returns:
            (train_loader, val_loader)
        """
        # åˆ†å‰²æ•°æ®é›†
        total_size = len(dataset)
        train_size = int(total_size * train_split)
        val_size = total_size - train_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        return train_loader, val_loader

# å…¨å±€æ•°æ®é›†ç®¡ç†å™¨å®ä¾‹
dataset_manager = DatasetManager()

def get_dataset_manager() -> DatasetManager:
    """è·å–å…¨å±€æ•°æ®é›†ç®¡ç†å™¨å®ä¾‹"""
    return dataset_manager

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†ç®¡ç†å™¨
    print("ğŸ§ª æ•°æ®é›†ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 50)
    
    if not PIL_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    manager = get_dataset_manager()
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_dataset_statistics()
    print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   å¯ç”¨æ•°æ®é›†: {stats['available_datasets']}")
    print(f"   è·¯å¾„éªŒè¯: {stats['path_validation']}")
    
    # æµ‹è¯•åŠ è½½å°æ ·æœ¬æ•°æ®é›†
    try:
        print(f"\nğŸ” æµ‹è¯•åŠ è½½coloræ•°æ®é›† (æ¯ç±»æœ€å¤š10ä¸ªæ ·æœ¬)...")
        dataset = manager.load_dataset('color', max_samples_per_class=10)
        info = dataset.get_dataset_info()
        
        print(f"âœ… åŠ è½½æˆåŠŸ:")
        print(f"   æ•°æ®é›†: {info['name']}")
        print(f"   æ ·æœ¬æ•°: {info['total_samples']}")
        print(f"   ç±»åˆ«æ•°: {info['num_classes']}")
        print(f"   å‰5ä¸ªç±»åˆ«: {list(info['classes'])[:5]}")
        
        # æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬
        if len(dataset) > 0:
            sample_image, sample_label = dataset[0]
            print(f"   æ ·æœ¬å½¢çŠ¶: {sample_image.size if hasattr(sample_image, 'size') else 'N/A'}")
            print(f"   æ ·æœ¬æ ‡ç­¾: {dataset.classes[sample_label]}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print(f"\nâœ… æ•°æ®é›†ç®¡ç†å™¨æµ‹è¯•å®Œæˆ")