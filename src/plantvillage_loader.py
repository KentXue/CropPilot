#!/usr/bin/env python3
"""
PlantVillageæ•°æ®é›†åŠ è½½å™¨
å®ç°PlantVillageæ•°æ®é›†çš„ä¸“ç”¨åŠ è½½å™¨ï¼ŒåŒ…å«è‹±æ–‡åˆ°ä¸­æ–‡çš„ç±»åˆ«æ˜ å°„
"""

import os
import sys
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from PIL import Image
    import torch
    from torch.utils.data import Dataset, DataLoader
    import numpy as np
    from torchvision import transforms
    PIL_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    PIL_AVAILABLE = False

from src.dataset_config import get_dataset_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PlantVillageClassMapping:
    """PlantVillageç±»åˆ«æ˜ å°„ç®¡ç†"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç±»åˆ«æ˜ å°„"""
        self.english_to_chinese = {
            # è‹¹æœç±»
            'Apple___Apple_scab': 'è‹¹æœ_è‹¹æœé»‘æ˜Ÿç—…',
            'Apple___Black_rot': 'è‹¹æœ_é»‘è…ç—…',
            'Apple___Cedar_apple_rust': 'è‹¹æœ_é›ªæ¾è‹¹æœé”ˆç—…',
            'Apple___healthy': 'è‹¹æœ_å¥åº·',
            
            # è“è“ç±»
            'Blueberry___healthy': 'è“è“_å¥åº·',
            
            # æ¨±æ¡ƒç±»
            'Cherry_(including_sour)___Powdery_mildew': 'æ¨±æ¡ƒ_ç™½ç²‰ç—…',
            'Cherry_(including_sour)___healthy': 'æ¨±æ¡ƒ_å¥åº·',
            
            # ç‰ç±³ç±»
            'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 'ç‰ç±³_ç°æ–‘ç—…',
            'Corn_(maize)___Common_rust_': 'ç‰ç±³_æ™®é€šé”ˆç—…',
            'Corn_(maize)___Northern_Leaf_Blight': 'ç‰ç±³_åŒ—æ–¹å¶æ¯ç—…',
            'Corn_(maize)___healthy': 'ç‰ç±³_å¥åº·',
            
            # è‘¡è„ç±»
            'Grape___Black_rot': 'è‘¡è„_é»‘è…ç—…',
            'Grape___Esca_(Black_Measles)': 'è‘¡è„_é»‘éº»ç–¹ç—…',
            'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 'è‘¡è„_å¶æ¯ç—…',
            'Grape___healthy': 'è‘¡è„_å¥åº·',
            
            # æ©™å­ç±»
            'Orange___Haunglongbing_(Citrus_greening)': 'æ©™å­_é»„é¾™ç—…',
            
            # æ¡ƒå­ç±»
            'Peach___Bacterial_spot': 'æ¡ƒå­_ç»†èŒæ€§æ–‘ç‚¹ç—…',
            'Peach___healthy': 'æ¡ƒå­_å¥åº·',
            
            # è¾£æ¤’ç±»
            'Pepper,_bell___Bacterial_spot': 'ç”œæ¤’_ç»†èŒæ€§æ–‘ç‚¹ç—…',
            'Pepper,_bell___healthy': 'ç”œæ¤’_å¥åº·',
            
            # é©¬é“ƒè–¯ç±»
            'Potato___Early_blight': 'é©¬é“ƒè–¯_æ—©ç–«ç—…',
            'Potato___Late_blight': 'é©¬é“ƒè–¯_æ™šç–«ç—…',
            'Potato___healthy': 'é©¬é“ƒè–¯_å¥åº·',
            
            # è¦†ç›†å­ç±»
            'Raspberry___healthy': 'è¦†ç›†å­_å¥åº·',
            
            # å¤§è±†ç±»
            'Soybean___healthy': 'å¤§è±†_å¥åº·',
            
            # å—ç“œç±»
            'Squash___Powdery_mildew': 'å—ç“œ_ç™½ç²‰ç—…',
            
            # è‰è“ç±»
            'Strawberry___Leaf_scorch': 'è‰è“_å¶ç„¦ç—…',
            'Strawberry___healthy': 'è‰è“_å¥åº·',
            
            # ç•ªèŒ„ç±»
            'Tomato___Bacterial_spot': 'ç•ªèŒ„_ç»†èŒæ€§æ–‘ç‚¹ç—…',
            'Tomato___Early_blight': 'ç•ªèŒ„_æ—©ç–«ç—…',
            'Tomato___Late_blight': 'ç•ªèŒ„_æ™šç–«ç—…',
            'Tomato___Leaf_Mold': 'ç•ªèŒ„_å¶éœ‰ç—…',
            'Tomato___Septoria_leaf_spot': 'ç•ªèŒ„_æ–‘ç‚¹ç—…',
            'Tomato___Spider_mites Two-spotted_spider_mite': 'ç•ªèŒ„_çº¢èœ˜è››',
            'Tomato___Target_Spot': 'ç•ªèŒ„_é¶æ–‘ç—…',
            'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 'ç•ªèŒ„_é»„åŒ–æ›²å¶ç—…æ¯’',
            'Tomato___Tomato_mosaic_virus': 'ç•ªèŒ„_èŠ±å¶ç—…æ¯’',
            'Tomato___healthy': 'ç•ªèŒ„_å¥åº·'
        }
        
        # åˆ›å»ºåå‘æ˜ å°„
        self.chinese_to_english = {v: k for k, v in self.english_to_chinese.items()}
        
        # ä½œç‰©åˆ†ç±»
        self.crop_categories = {
            'æœæ ‘ç±»': ['è‹¹æœ', 'è“è“', 'æ¨±æ¡ƒ', 'è‘¡è„', 'æ©™å­', 'æ¡ƒå­', 'è¦†ç›†å­', 'è‰è“'],
            'ç²®é£Ÿä½œç‰©': ['ç‰ç±³', 'å¤§è±†', 'é©¬é“ƒè–¯'],
            'è”¬èœç±»': ['ç”œæ¤’', 'å—ç“œ', 'ç•ªèŒ„']
        }
        
        # ç—…å®³ç±»å‹åˆ†ç±»
        self.disease_categories = {
            'çœŸèŒç—…å®³': ['é»‘æ˜Ÿç—…', 'é»‘è…ç—…', 'é”ˆç—…', 'ç™½ç²‰ç—…', 'å¶æ¯ç—…', 'æ—©ç–«ç—…', 'æ™šç–«ç—…', 'å¶éœ‰ç—…', 'æ–‘ç‚¹ç—…', 'é¶æ–‘ç—…'],
            'ç»†èŒç—…å®³': ['ç»†èŒæ€§æ–‘ç‚¹ç—…'],
            'ç—…æ¯’ç—…å®³': ['é»„åŒ–æ›²å¶ç—…æ¯’', 'èŠ±å¶ç—…æ¯’'],
            'è™«å®³': ['çº¢èœ˜è››'],
            'ç”Ÿç†ç—…å®³': ['å¶ç„¦ç—…', 'é»„é¾™ç—…'],
            'å¥åº·': ['å¥åº·']
        }
    
    def get_chinese_name(self, english_name: str) -> str:
        """è·å–ä¸­æ–‡åç§°"""
        return self.english_to_chinese.get(english_name, english_name)
    
    def get_english_name(self, chinese_name: str) -> str:
        """è·å–è‹±æ–‡åç§°"""
        return self.chinese_to_english.get(chinese_name, chinese_name)
    
    def get_crop_name(self, class_name: str) -> str:
        """ä»ç±»åˆ«åç§°ä¸­æå–ä½œç‰©åç§°"""
        if '_' in class_name:
            return class_name.split('_')[0]
        return class_name.split('___')[0] if '___' in class_name else class_name
    
    def get_disease_name(self, class_name: str) -> str:
        """ä»ç±»åˆ«åç§°ä¸­æå–ç—…å®³åç§°"""
        if '_' in class_name:
            return class_name.split('_')[1]
        return class_name.split('___')[1] if '___' in class_name else 'unknown'
    
    def get_crop_category(self, crop_name: str) -> str:
        """è·å–ä½œç‰©ç±»åˆ«"""
        for category, crops in self.crop_categories.items():
            if crop_name in crops:
                return category
        return 'å…¶ä»–'
    
    def get_disease_category(self, disease_name: str) -> str:
        """è·å–ç—…å®³ç±»åˆ«"""
        for category, diseases in self.disease_categories.items():
            if any(disease in disease_name for disease in diseases):
                return category
        return 'å…¶ä»–'
    
    def get_all_classes(self) -> List[str]:
        """è·å–æ‰€æœ‰ç±»åˆ«ï¼ˆè‹±æ–‡ï¼‰"""
        return list(self.english_to_chinese.keys())
    
    def get_all_chinese_classes(self) -> List[str]:
        """è·å–æ‰€æœ‰ç±»åˆ«ï¼ˆä¸­æ–‡ï¼‰"""
        return list(self.english_to_chinese.values())
    
    def get_class_statistics(self) -> Dict[str, Any]:
        """è·å–ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯"""
        crop_count = defaultdict(int)
        disease_count = defaultdict(int)
        category_count = defaultdict(int)
        
        for english_name, chinese_name in self.english_to_chinese.items():
            crop = self.get_crop_name(chinese_name)
            disease = self.get_disease_name(chinese_name)
            category = self.get_crop_category(crop)
            
            crop_count[crop] += 1
            disease_count[disease] += 1
            category_count[category] += 1
        
        return {
            'total_classes': len(self.english_to_chinese),
            'crop_distribution': dict(crop_count),
            'disease_distribution': dict(disease_count),
            'category_distribution': dict(category_count),
            'crop_categories': self.crop_categories,
            'disease_categories': self.disease_categories
        }

class PlantVillageDatasetLoader:
    """PlantVillageæ•°æ®é›†ä¸“ç”¨åŠ è½½å™¨"""
    
    def __init__(self, dataset_type: str = 'color'):
        """
        åˆå§‹åŒ–åŠ è½½å™¨
        
        Args:
            dataset_type: æ•°æ®é›†ç±»å‹ ('color', 'grayscale', 'segmented')
        """
        self.dataset_type = dataset_type
        self.config = get_dataset_config()
        self.class_mapping = PlantVillageClassMapping()
        
        # è·å–æ•°æ®é›†é…ç½®
        if dataset_type not in self.config.plantvillage_datasets:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
        
        self.dataset_config = self.config.plantvillage_datasets[dataset_type]
        
        # æ•°æ®é›†ä¿¡æ¯
        self.dataset_info = None
        self.class_distribution = None
        
        logger.info(f"PlantVillage {dataset_type} æ•°æ®é›†åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_dataset(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®é›†ç»“æ„å’Œå†…å®¹"""
        if not os.path.exists(self.dataset_config.path):
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.dataset_config.path}")
        
        logger.info(f"å¼€å§‹åˆ†æ {self.dataset_type} æ•°æ®é›†...")
        
        # è·å–æ‰€æœ‰ç±»åˆ«ç›®å½•
        class_dirs = [d for d in os.listdir(self.dataset_config.path) 
                     if os.path.isdir(os.path.join(self.dataset_config.path, d))]
        class_dirs.sort()
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å›¾åƒæ•°é‡
        class_stats = {}
        total_images = 0
        
        for class_name in class_dirs:
            class_path = os.path.join(self.dataset_config.path, class_name)
            
            # ç»Ÿè®¡å›¾åƒæ–‡ä»¶
            image_count = 0
            image_formats = defaultdict(int)
            
            for file in os.listdir(class_path):
                file_path = os.path.join(class_path, file)
                if os.path.isfile(file_path):
                    ext = os.path.splitext(file)[1].lower()
                    if ext in self.dataset_config.image_extensions:
                        image_count += 1
                        image_formats[ext] += 1
            
            # è·å–ä¸­æ–‡åç§°
            chinese_name = self.class_mapping.get_chinese_name(class_name)
            crop_name = self.class_mapping.get_crop_name(chinese_name)
            disease_name = self.class_mapping.get_disease_name(chinese_name)
            
            class_stats[class_name] = {
                'chinese_name': chinese_name,
                'crop': crop_name,
                'disease': disease_name,
                'image_count': image_count,
                'image_formats': dict(image_formats)
            }
            
            total_images += image_count
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        self.dataset_info = {
            'dataset_type': self.dataset_type,
            'dataset_path': self.dataset_config.path,
            'total_classes': len(class_dirs),
            'total_images': total_images,
            'class_statistics': class_stats,
            'class_mapping_stats': self.class_mapping.get_class_statistics(),
            'input_size': self.dataset_config.input_size
        }
        
        # ä¿å­˜ç±»åˆ«åˆ†å¸ƒ
        self.class_distribution = {k: v['image_count'] for k, v in class_stats.items()}
        
        logger.info(f"æ•°æ®é›†åˆ†æå®Œæˆ: {len(class_dirs)} ä¸ªç±»åˆ«, {total_images} å¼ å›¾åƒ")
        
        return self.dataset_info
    
    def create_train_val_split(self, 
                              train_ratio: float = 0.8,
                              stratified: bool = True,
                              random_seed: int = 42) -> Tuple[Dict[str, List[str]], Dict[str, List[str]]]:
        """
        åˆ›å»ºè®­ç»ƒ/éªŒè¯é›†åˆ†å‰²
        
        Args:
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            stratified: æ˜¯å¦åˆ†å±‚é‡‡æ ·
            random_seed: éšæœºç§å­
            
        Returns:
            (train_split, val_split) - æ¯ä¸ªéƒ½æ˜¯ {class_name: [image_paths]} çš„å­—å…¸
        """
        if self.dataset_info is None:
            self.analyze_dataset()
        
        random.seed(random_seed)
        
        train_split = {}
        val_split = {}
        
        for class_name, class_info in self.dataset_info['class_statistics'].items():
            class_path = os.path.join(self.dataset_config.path, class_name)
            
            # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_files = []
            for file in os.listdir(class_path):
                file_path = os.path.join(class_path, file)
                if os.path.isfile(file_path):
                    ext = os.path.splitext(file)[1].lower()
                    if ext in self.dataset_config.image_extensions:
                        image_files.append(file_path)
            
            # éšæœºæ‰“ä¹±
            random.shuffle(image_files)
            
            # åˆ†å‰²
            split_idx = int(len(image_files) * train_ratio)
            train_split[class_name] = image_files[:split_idx]
            val_split[class_name] = image_files[split_idx:]
        
        # ç»Ÿè®¡åˆ†å‰²ç»“æœ
        train_total = sum(len(files) for files in train_split.values())
        val_total = sum(len(files) for files in val_split.values())
        
        logger.info(f"æ•°æ®é›†åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {train_total} å¼ , éªŒè¯é›† {val_total} å¼ ")
        
        return train_split, val_split
    
    def generate_dataset_report(self, output_path: Optional[str] = None) -> str:
        """
        ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›å­—ç¬¦ä¸²
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        if self.dataset_info is None:
            self.analyze_dataset()
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"# PlantVillage {self.dataset_type.upper()} æ•°æ®é›†åˆ†ææŠ¥å‘Š",
            f"",
            f"## åŸºæœ¬ä¿¡æ¯",
            f"- **æ•°æ®é›†ç±»å‹**: {self.dataset_type}",
            f"- **æ•°æ®é›†è·¯å¾„**: {self.dataset_info['dataset_path']}",
            f"- **æ€»ç±»åˆ«æ•°**: {self.dataset_info['total_classes']}",
            f"- **æ€»å›¾åƒæ•°**: {self.dataset_info['total_images']}",
            f"- **è¾“å…¥å°ºå¯¸**: {self.dataset_info['input_size']}",
            f"",
            f"## ç±»åˆ«æ˜ å°„ç»Ÿè®¡",
        ]
        
        mapping_stats = self.dataset_info['class_mapping_stats']
        report_lines.extend([
            f"- **æ€»ç±»åˆ«æ•°**: {mapping_stats['total_classes']}",
            f"- **ä½œç‰©ç±»åˆ«åˆ†å¸ƒ**: {mapping_stats['category_distribution']}",
            f"- **ä¸»è¦ä½œç‰©**: {', '.join(list(mapping_stats['crop_distribution'].keys())[:10])}",
            f"",
            f"## ç±»åˆ«è¯¦ç»†ä¿¡æ¯",
            f""
        ])
        
        # æŒ‰å›¾åƒæ•°é‡æ’åºæ˜¾ç¤ºç±»åˆ«
        sorted_classes = sorted(
            self.dataset_info['class_statistics'].items(),
            key=lambda x: x[1]['image_count'],
            reverse=True
        )
        
        report_lines.append("| è‹±æ–‡åç§° | ä¸­æ–‡åç§° | ä½œç‰© | ç—…å®³ | å›¾åƒæ•°é‡ |")
        report_lines.append("|----------|----------|------|------|----------|")
        
        for class_name, class_info in sorted_classes:
            report_lines.append(
                f"| {class_name} | {class_info['chinese_name']} | "
                f"{class_info['crop']} | {class_info['disease']} | {class_info['image_count']} |"
            )
        
        # æ·»åŠ åˆ†å¸ƒç»Ÿè®¡
        report_lines.extend([
            f"",
            f"## æ•°æ®åˆ†å¸ƒåˆ†æ",
            f"",
            f"### ä½œç‰©åˆ†å¸ƒ",
        ])
        
        for crop, count in mapping_stats['crop_distribution'].items():
            report_lines.append(f"- **{crop}**: {count} ä¸ªç±»åˆ«")
        
        report_lines.extend([
            f"",
            f"### ç—…å®³ç±»å‹åˆ†å¸ƒ",
        ])
        
        for disease_type, diseases in mapping_stats['disease_categories'].items():
            count = sum(1 for class_info in self.dataset_info['class_statistics'].values()
                       if any(disease in class_info['disease'] for disease in diseases))
            report_lines.append(f"- **{disease_type}**: {count} ä¸ªç±»åˆ«")
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"æ•°æ®é›†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return report_content
    
    def get_default_transforms(self, is_training: bool = True):
        """è·å–é»˜è®¤çš„å›¾åƒå˜æ¢"""
        if not PIL_AVAILABLE:
            return None
        
        if is_training:
            return transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.RandomCrop(224),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            return transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

# ä¾¿æ·å‡½æ•°
def create_plantvillage_loader(dataset_type: str = 'color') -> PlantVillageDatasetLoader:
    """åˆ›å»ºPlantVillageæ•°æ®é›†åŠ è½½å™¨"""
    return PlantVillageDatasetLoader(dataset_type)

def get_plantvillage_class_mapping() -> PlantVillageClassMapping:
    """è·å–PlantVillageç±»åˆ«æ˜ å°„"""
    return PlantVillageClassMapping()

if __name__ == "__main__":
    # æµ‹è¯•PlantVillageåŠ è½½å™¨
    print("ğŸ§ª PlantVillageæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•")
    print("=" * 60)
    
    if not PIL_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•ç±»åˆ«æ˜ å°„
        print("ğŸ“‹ æµ‹è¯•ç±»åˆ«æ˜ å°„...")
        mapping = get_plantvillage_class_mapping()
        stats = mapping.get_class_statistics()
        
        print(f"âœ… ç±»åˆ«æ˜ å°„ç»Ÿè®¡:")
        print(f"   æ€»ç±»åˆ«æ•°: {stats['total_classes']}")
        print(f"   ä½œç‰©ç±»åˆ«: {stats['category_distribution']}")
        print(f"   å‰5ä¸ªç±»åˆ«æ˜ å°„:")
        for i, (eng, chn) in enumerate(list(mapping.english_to_chinese.items())[:5]):
            print(f"     {eng} -> {chn}")
        
        # æµ‹è¯•æ•°æ®é›†åŠ è½½å™¨
        print(f"\nğŸ” æµ‹è¯•coloræ•°æ®é›†åŠ è½½å™¨...")
        loader = create_plantvillage_loader('color')
        
        # åˆ†ææ•°æ®é›†
        dataset_info = loader.analyze_dataset()
        print(f"âœ… æ•°æ®é›†åˆ†æå®Œæˆ:")
        print(f"   æ•°æ®é›†ç±»å‹: {dataset_info['dataset_type']}")
        print(f"   æ€»ç±»åˆ«æ•°: {dataset_info['total_classes']}")
        print(f"   æ€»å›¾åƒæ•°: {dataset_info['total_images']}")
        
        # æµ‹è¯•è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®é›†åˆ†å‰²...")
        train_split, val_split = loader.create_train_val_split(train_ratio=0.8)
        
        train_total = sum(len(files) for files in train_split.values())
        val_total = sum(len(files) for files in val_split.values())
        
        print(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {train_total} å¼ å›¾åƒ")
        print(f"   éªŒè¯é›†: {val_total} å¼ å›¾åƒ")
        print(f"   åˆ†å‰²æ¯”ä¾‹: {train_total/(train_total+val_total):.2f}")
        
        # ç”ŸæˆæŠ¥å‘Š
        print(f"\nğŸ“„ ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š...")
        report_path = f"PlantVillage_{loader.dataset_type}_dataset_report.md"
        report = loader.generate_dataset_report(report_path)
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… PlantVillageæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•å®Œæˆ")