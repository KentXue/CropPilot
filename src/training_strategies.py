#!/usr/bin/env python3
"""
è®­ç»ƒç­–ç•¥ä¼˜åŒ–æ¨¡å—
å®ç°ç±»åˆ«æƒé‡å¹³è¡¡ã€æ¸è¿›å¼è®­ç»ƒã€è¶…å‚æ•°ä¼˜åŒ–ç­‰é«˜çº§è®­ç»ƒç­–ç•¥
"""

import os
import sys
import json
import math
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from collections import Counter
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, WeightedRandomSampler
    import numpy as np
    from sklearn.utils.class_weight import compute_class_weight
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProgressiveTrainingConfig:
    """æ¸è¿›å¼è®­ç»ƒé…ç½®"""
    stages: List[Dict[str, Any]]
    warmup_epochs: int = 5
    freeze_backbone_epochs: int = 10
    
class ClassBalanceStrategy:
    """ç±»åˆ«å¹³è¡¡ç­–ç•¥"""
    
    def __init__(self, strategy: str = 'weighted_loss'):
        """
        åˆå§‹åŒ–ç±»åˆ«å¹³è¡¡ç­–ç•¥
        
        Args:
            strategy: å¹³è¡¡ç­–ç•¥ ('weighted_loss', 'weighted_sampling', 'focal_loss')
        """
        self.strategy = strategy
        self.class_weights = None
        self.sample_weights = None
        
    def compute_class_weights(self, labels: List[int], num_classes: int) -> torch.Tensor:
        """è®¡ç®—ç±»åˆ«æƒé‡"""
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        class_counts = Counter(labels)
        
        # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½æœ‰è®¡æ•°
        for i in range(num_classes):
            if i not in class_counts:
                class_counts[i] = 1
        
        # è®¡ç®—æƒé‡
        total_samples = len(labels)
        weights = []
        
        for i in range(num_classes):
            count = class_counts[i]
            weight = total_samples / (num_classes * count)
            weights.append(weight)
        
        self.class_weights = torch.FloatTensor(weights)
        logger.info(f"ç±»åˆ«æƒé‡è®¡ç®—å®Œæˆï¼Œæƒé‡èŒƒå›´: [{self.class_weights.min():.3f}, {self.class_weights.max():.3f}]")
        
        return self.class_weights
    
    def compute_sample_weights(self, labels: List[int]) -> torch.Tensor:
        """è®¡ç®—æ ·æœ¬æƒé‡ç”¨äºåŠ æƒé‡‡æ ·"""
        if self.class_weights is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨compute_class_weights")
        
        sample_weights = []
        for label in labels:
            sample_weights.append(self.class_weights[label].item())
        
        self.sample_weights = torch.FloatTensor(sample_weights)
        return self.sample_weights
    
    def create_weighted_sampler(self, labels: List[int], num_classes: int) -> WeightedRandomSampler:
        """åˆ›å»ºåŠ æƒé‡‡æ ·å™¨"""
        self.compute_class_weights(labels, num_classes)
        sample_weights = self.compute_sample_weights(labels)
        
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True
        )
        
        logger.info("åŠ æƒé‡‡æ ·å™¨åˆ›å»ºå®Œæˆ")
        return sampler

class FocalLoss(nn.Module):
    """Focal Losså®ç°"""
    
    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):
        """
        åˆå§‹åŒ–Focal Loss
        
        Args:
            alpha: å¹³è¡¡å› å­
            gamma: èšç„¦å‚æ•°
            reduction: å‡å°‘æ–¹å¼
        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class ProgressiveTrainer:
    """æ¸è¿›å¼è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, config: ProgressiveTrainingConfig):
        """
        åˆå§‹åŒ–æ¸è¿›å¼è®­ç»ƒå™¨
        
        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            config: æ¸è¿›å¼è®­ç»ƒé…ç½®
        """
        self.model = model
        self.config = config
        self.current_stage = 0
        
    def freeze_backbone(self):
        """å†»ç»“éª¨å¹²ç½‘ç»œ"""
        if hasattr(self.model, 'backbone'):
            for param in self.model.backbone.parameters():
                param.requires_grad = False
            logger.info("éª¨å¹²ç½‘ç»œå·²å†»ç»“")
        
    def unfreeze_backbone(self):
        """è§£å†»éª¨å¹²ç½‘ç»œ"""
        if hasattr(self.model, 'backbone'):
            for param in self.model.backbone.parameters():
                param.requires_grad = True
            logger.info("éª¨å¹²ç½‘ç»œå·²è§£å†»")
    
    def get_stage_config(self, stage: int) -> Dict[str, Any]:
        """è·å–é˜¶æ®µé…ç½®"""
        if stage < len(self.config.stages):
            return self.config.stages[stage]
        else:
            return self.config.stages[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªé˜¶æ®µçš„é…ç½®
    
    def should_advance_stage(self, epoch: int, val_acc: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        stage_config = self.get_stage_config(self.current_stage)
        
        # æ£€æŸ¥è½®æ•°æ¡ä»¶
        if epoch >= stage_config.get('min_epochs', 10):
            # æ£€æŸ¥å‡†ç¡®ç‡æ¡ä»¶
            target_acc = stage_config.get('target_accuracy', 0.0)
            if val_acc >= target_acc:
                return True
        
        return False
    
    def advance_stage(self, optimizer: optim.Optimizer) -> bool:
        """è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if self.current_stage < len(self.config.stages) - 1:
            self.current_stage += 1
            stage_config = self.get_stage_config(self.current_stage)
            
            # æ›´æ–°å­¦ä¹ ç‡
            new_lr = stage_config.get('learning_rate', 0.001)
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lr
            
            # è§£å†»ç­–ç•¥
            if stage_config.get('unfreeze_backbone', False):
                self.unfreeze_backbone()
            
            logger.info(f"è¿›å…¥è®­ç»ƒé˜¶æ®µ {self.current_stage + 1}, å­¦ä¹ ç‡: {new_lr}")
            return True
        
        return False

class GradientAccumulator:
    """æ¢¯åº¦ç´¯ç§¯å™¨"""
    
    def __init__(self, accumulation_steps: int = 4):
        """
        åˆå§‹åŒ–æ¢¯åº¦ç´¯ç§¯å™¨
        
        Args:
            accumulation_steps: ç´¯ç§¯æ­¥æ•°
        """
        self.accumulation_steps = accumulation_steps
        self.current_step = 0
        
    def should_step(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        self.current_step += 1
        if self.current_step >= self.accumulation_steps:
            self.current_step = 0
            return True
        return False
    
    def scale_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """ç¼©æ”¾æŸå¤±"""
        return loss / self.accumulation_steps

class LearningRateScheduler:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer: optim.Optimizer, strategy: str = 'cosine_warmup'):
        """
        åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        
        Args:
            optimizer: ä¼˜åŒ–å™¨
            strategy: è°ƒåº¦ç­–ç•¥
        """
        self.optimizer = optimizer
        self.strategy = strategy
        self.base_lr = optimizer.param_groups[0]['lr']
        
    def warmup_lr(self, epoch: int, warmup_epochs: int) -> float:
        """é¢„çƒ­å­¦ä¹ ç‡"""
        if epoch < warmup_epochs:
            return self.base_lr * (epoch + 1) / warmup_epochs
        return self.base_lr
    
    def cosine_annealing_lr(self, epoch: int, total_epochs: int, min_lr: float = 1e-6) -> float:
        """ä½™å¼¦é€€ç«å­¦ä¹ ç‡"""
        return min_lr + (self.base_lr - min_lr) * (1 + math.cos(math.pi * epoch / total_epochs)) / 2
    
    def step_lr(self, epoch: int, step_size: int = 30, gamma: float = 0.1) -> float:
        """é˜¶æ¢¯å­¦ä¹ ç‡"""
        return self.base_lr * (gamma ** (epoch // step_size))
    
    def update_lr(self, epoch: int, **kwargs):
        """æ›´æ–°å­¦ä¹ ç‡"""
        if self.strategy == 'cosine_warmup':
            warmup_epochs = kwargs.get('warmup_epochs', 5)
            total_epochs = kwargs.get('total_epochs', 100)
            
            if epoch < warmup_epochs:
                new_lr = self.warmup_lr(epoch, warmup_epochs)
            else:
                new_lr = self.cosine_annealing_lr(epoch - warmup_epochs, total_epochs - warmup_epochs)
        
        elif self.strategy == 'step':
            new_lr = self.step_lr(epoch, kwargs.get('step_size', 30), kwargs.get('gamma', 0.1))
        
        else:
            new_lr = self.base_lr
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr

class TrainingOptimizer:
    """è®­ç»ƒä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒä¼˜åŒ–å™¨"""
        self.strategies = {}
        
    def optimize_batch_size(self, model: nn.Module, device: torch.device, 
                          input_size: Tuple[int, int, int] = (3, 224, 224),
                          max_memory_gb: float = 6.0) -> int:
        """ä¼˜åŒ–æ‰¹å¤§å°"""
        logger.info("å¼€å§‹æ‰¹å¤§å°ä¼˜åŒ–...")
        
        model.eval()
        optimal_batch_size = 1
        
        # æµ‹è¯•ä¸åŒçš„æ‰¹å¤§å°
        test_batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]
        
        for batch_size in test_batch_sizes:
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(batch_size, *input_size, device=device)
                
                # å‰å‘ä¼ æ’­æµ‹è¯•
                with torch.no_grad():
                    _ = model(test_input)
                
                # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
                if device.type == 'cuda':
                    memory_used = torch.cuda.memory_allocated(device) / 1024**3
                    if memory_used > max_memory_gb * 0.8:  # ä½¿ç”¨80%çš„æ˜¾å­˜ä½œä¸ºä¸Šé™
                        break
                
                optimal_batch_size = batch_size
                
                # æ¸…ç†
                del test_input
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    break
                else:
                    raise e
        
        logger.info(f"ä¼˜åŒ–æ‰¹å¤§å°: {optimal_batch_size}")
        return optimal_batch_size
    
    def create_progressive_config(self, num_classes: int) -> ProgressiveTrainingConfig:
        """åˆ›å»ºæ¸è¿›å¼è®­ç»ƒé…ç½®"""
        stages = [
            {
                'name': 'warmup',
                'min_epochs': 5,
                'learning_rate': 0.0001,
                'target_accuracy': 0.1,
                'freeze_backbone': True
            },
            {
                'name': 'fine_tune_head',
                'min_epochs': 10,
                'learning_rate': 0.001,
                'target_accuracy': 0.3,
                'freeze_backbone': True
            },
            {
                'name': 'full_training',
                'min_epochs': 20,
                'learning_rate': 0.0005,
                'target_accuracy': 0.8,
                'unfreeze_backbone': True
            }
        ]
        
        return ProgressiveTrainingConfig(
            stages=stages,
            warmup_epochs=5,
            freeze_backbone_epochs=15
        )
    
    def suggest_hyperparameters(self, dataset_size: int, num_classes: int, 
                              gpu_memory_gb: float) -> Dict[str, Any]:
        """å»ºè®®è¶…å‚æ•°"""
        suggestions = {}
        
        # æ‰¹å¤§å°å»ºè®®
        if gpu_memory_gb >= 24:
            suggestions['batch_size'] = min(64, dataset_size // 100)
        elif gpu_memory_gb >= 12:
            suggestions['batch_size'] = min(32, dataset_size // 200)
        elif gpu_memory_gb >= 8:
            suggestions['batch_size'] = min(16, dataset_size // 400)
        else:
            suggestions['batch_size'] = min(8, dataset_size // 800)
        
        # å­¦ä¹ ç‡å»ºè®®
        if dataset_size < 1000:
            suggestions['learning_rate'] = 0.0001
        elif dataset_size < 10000:
            suggestions['learning_rate'] = 0.001
        else:
            suggestions['learning_rate'] = 0.01
        
        # è½®æ•°å»ºè®®
        if dataset_size < 1000:
            suggestions['num_epochs'] = 100
        elif dataset_size < 10000:
            suggestions['num_epochs'] = 50
        else:
            suggestions['num_epochs'] = 30
        
        # å…¶ä»–å»ºè®®
        suggestions['weight_decay'] = 1e-4
        suggestions['dropout_rate'] = 0.3 if num_classes > 20 else 0.2
        suggestions['label_smoothing'] = 0.1 if num_classes > 10 else 0.0
        
        return suggestions

def create_balanced_dataloader(dataset, labels: List[int], batch_size: int, 
                             num_classes: int, strategy: str = 'weighted_sampling') -> DataLoader:
    """åˆ›å»ºå¹³è¡¡çš„æ•°æ®åŠ è½½å™¨"""
    balance_strategy = ClassBalanceStrategy(strategy)
    
    if strategy == 'weighted_sampling':
        sampler = balance_strategy.create_weighted_sampler(labels, num_classes)
        return DataLoader(dataset, batch_size=batch_size, sampler=sampler)
    else:
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

def create_progressive_trainer(model: nn.Module, num_classes: int) -> ProgressiveTrainer:
    """åˆ›å»ºæ¸è¿›å¼è®­ç»ƒå™¨"""
    optimizer = TrainingOptimizer()
    config = optimizer.create_progressive_config(num_classes)
    return ProgressiveTrainer(model, config)

if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒç­–ç•¥ä¼˜åŒ–
    print("ğŸ§ª è®­ç»ƒç­–ç•¥ä¼˜åŒ–æµ‹è¯•")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•ç±»åˆ«å¹³è¡¡ç­–ç•¥
        print("ğŸ“Š æµ‹è¯•ç±»åˆ«å¹³è¡¡ç­–ç•¥...")
        labels = [0] * 100 + [1] * 50 + [2] * 200  # ä¸å¹³è¡¡æ•°æ®
        balance_strategy = ClassBalanceStrategy()
        
        class_weights = balance_strategy.compute_class_weights(labels, 3)
        print(f"âœ… ç±»åˆ«æƒé‡: {class_weights}")
        
        # æµ‹è¯•Focal Loss
        print(f"\nğŸ¯ æµ‹è¯•Focal Loss...")
        focal_loss = FocalLoss(alpha=1.0, gamma=2.0)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        pred = torch.randn(10, 3)
        target = torch.randint(0, 3, (10,))
        
        loss = focal_loss(pred, target)
        print(f"âœ… Focal Loss: {loss.item():.4f}")
        
        # æµ‹è¯•æ¢¯åº¦ç´¯ç§¯
        print(f"\nğŸ“ˆ æµ‹è¯•æ¢¯åº¦ç´¯ç§¯...")
        accumulator = GradientAccumulator(accumulation_steps=4)
        
        for i in range(10):
            should_step = accumulator.should_step()
            if should_step:
                print(f"   æ­¥éª¤ {i+1}: æ‰§è¡Œä¼˜åŒ–")
        
        # æµ‹è¯•è®­ç»ƒä¼˜åŒ–å™¨
        print(f"\nâš™ï¸ æµ‹è¯•è®­ç»ƒä¼˜åŒ–å™¨...")
        optimizer = TrainingOptimizer()
        
        suggestions = optimizer.suggest_hyperparameters(
            dataset_size=10000,
            num_classes=38,
            gpu_memory_gb=6.0
        )
        
        print(f"âœ… è¶…å‚æ•°å»ºè®®:")
        for key, value in suggestions.items():
            print(f"   {key}: {value}")
        
        # æµ‹è¯•æ¸è¿›å¼è®­ç»ƒé…ç½®
        print(f"\nğŸš€ æµ‹è¯•æ¸è¿›å¼è®­ç»ƒé…ç½®...")
        config = optimizer.create_progressive_config(38)
        
        print(f"âœ… æ¸è¿›å¼è®­ç»ƒé˜¶æ®µ:")
        for i, stage in enumerate(config.stages):
            print(f"   é˜¶æ®µ {i+1}: {stage['name']} - LR: {stage['learning_rate']}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… è®­ç»ƒç­–ç•¥ä¼˜åŒ–æµ‹è¯•å®Œæˆ")