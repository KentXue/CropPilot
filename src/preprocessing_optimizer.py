#!/usr/bin/env python3
"""
å›¾åƒé¢„å¤„ç†æ€§èƒ½ä¼˜åŒ–å™¨
å®ç°æ‰¹é‡é¢„å¤„ç†ã€å¤šçº¿ç¨‹æ•°æ®åŠ è½½å’Œå†…å­˜ä¼˜åŒ–
"""

import os
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from multiprocessing import cpu_count
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union, Callable, Iterator
import logging
from dataclasses import dataclass
from queue import Queue
import gc

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import numpy as np
    import cv2
    import torch
    from torch.utils.data import Dataset, DataLoader
    import albumentations as A
    from PIL import Image
    import psutil
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

from src.image_preprocessing import PlantDiseasePreprocessor, PreprocessingMode
from src.data_augmentation import PlantDiseaseAugmentation, AugmentationStrategy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    total_images: int = 0
    processing_time: float = 0.0
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    images_per_second: float = 0.0
    avg_image_size_mb: float = 0.0
    cache_hit_rate: float = 0.0

class ImageCache:
    """å›¾åƒç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size_mb: int = 1024):
        """
        åˆå§‹åŒ–ç¼“å­˜
        
        Args:
            max_size_mb: æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆMBï¼‰
        """
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.cache = {}
        self.access_times = {}
        self.current_size = 0
        self.hits = 0
        self.misses = 0
        self._lock = threading.Lock()
    
    def get(self, key: str) -> Optional[np.ndarray]:
        """è·å–ç¼“å­˜çš„å›¾åƒ"""
        with self._lock:
            if key in self.cache:
                self.access_times[key] = time.time()
                self.hits += 1
                return self.cache[key].copy()
            else:
                self.misses += 1
                return None
    
    def put(self, key: str, image: np.ndarray) -> None:
        """æ·»åŠ å›¾åƒåˆ°ç¼“å­˜"""
        with self._lock:
            image_size = image.nbytes
            
            # å¦‚æœå›¾åƒå¤ªå¤§ï¼Œä¸ç¼“å­˜
            if image_size > self.max_size_bytes * 0.1:
                return
            
            # æ¸…ç†ç©ºé—´
            while self.current_size + image_size > self.max_size_bytes and self.cache:
                self._evict_lru()
            
            # æ·»åŠ åˆ°ç¼“å­˜
            self.cache[key] = image.copy()
            self.access_times[key] = time.time()
            self.current_size += image_size
    
    def _evict_lru(self) -> None:
        """ç§»é™¤æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹ç›®"""
        if not self.cache:
            return
        
        lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        image_size = self.cache[lru_key].nbytes
        
        del self.cache[lru_key]
        del self.access_times[lru_key]
        self.current_size -= image_size
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.access_times.clear()
            self.current_size = 0
            self.hits = 0
            self.misses = 0

class OptimizedImageDataset(Dataset):
    """ä¼˜åŒ–çš„å›¾åƒæ•°æ®é›†"""
    
    def __init__(self,
                 image_paths: List[str],
                 labels: List[int],
                 preprocessor: PlantDiseasePreprocessor,
                 cache_enabled: bool = True,
                 prefetch_factor: int = 2):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æ•°æ®é›†
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            preprocessor: é¢„å¤„ç†å™¨
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            prefetch_factor: é¢„å–å› å­
        """
        self.image_paths = image_paths
        self.labels = labels
        self.preprocessor = preprocessor
        self.cache_enabled = cache_enabled
        self.prefetch_factor = prefetch_factor
        
        # åˆå§‹åŒ–ç¼“å­˜
        if cache_enabled:
            self.cache = ImageCache(max_size_mb=512)
        else:
            self.cache = None
        
        # é¢„åŠ è½½ç»Ÿè®¡ä¿¡æ¯
        self._analyze_dataset()
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        sample_size = min(100, len(self.image_paths))
        sample_paths = self.image_paths[:sample_size]
        
        total_size = 0
        valid_images = 0
        
        for path in sample_paths:
            try:
                if os.path.exists(path):
                    size = os.path.getsize(path)
                    total_size += size
                    valid_images += 1
            except:
                continue
        
        self.avg_image_size = total_size / valid_images if valid_images > 0 else 0
        logger.info(f"æ•°æ®é›†åˆ†æå®Œæˆ: å¹³å‡å›¾åƒå¤§å° {self.avg_image_size/1024/1024:.2f} MB")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        # å°è¯•ä»ç¼“å­˜è·å–
        image = None
        if self.cache_enabled and self.cache:
            image = self.cache.get(image_path)
        
        # å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½å›¾åƒ
        if image is None:
            image = self._load_image(image_path)
            
            # æ·»åŠ åˆ°ç¼“å­˜
            if self.cache_enabled and self.cache and image is not None:
                self.cache.put(image_path, image)
        
        # é¢„å¤„ç†
        if image is not None:
            try:
                processed_image = self.preprocessor.preprocess_image(image)
                return processed_image, label
            except Exception as e:
                logger.warning(f"é¢„å¤„ç†å¤±è´¥ {image_path}: {e}")
                # è¿”å›é›¶å¼ é‡ä½œä¸ºå¤‡ç”¨
                return torch.zeros(3, 224, 224), label
        else:
            return torch.zeros(3, 224, 224), label
    
    def _load_image(self, image_path: str) -> Optional[np.ndarray]:
        """åŠ è½½å•å¼ å›¾åƒ"""
        try:
            image = cv2.imread(image_path)
            if image is not None:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                return image
        except Exception as e:
            logger.warning(f"å›¾åƒåŠ è½½å¤±è´¥ {image_path}: {e}")
        return None

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, 
                 batch_size: int = 32,
                 num_workers: int = None,
                 use_multiprocessing: bool = False):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            batch_size: æ‰¹å¤§å°
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            use_multiprocessing: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
        """
        self.batch_size = batch_size
        self.num_workers = num_workers or min(cpu_count(), 8)
        self.use_multiprocessing = use_multiprocessing
        
        logger.info(f"æ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–: batch_size={batch_size}, workers={self.num_workers}")
    
    def process_batch_parallel(self,
                             image_paths: List[str],
                             preprocessor: PlantDiseasePreprocessor) -> List[torch.Tensor]:
        """
        å¹¶è¡Œå¤„ç†å›¾åƒæ‰¹æ¬¡
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            preprocessor: é¢„å¤„ç†å™¨
            
        Returns:
            å¤„ç†åçš„å›¾åƒå¼ é‡åˆ—è¡¨
        """
        if self.use_multiprocessing:
            executor_class = ProcessPoolExecutor
        else:
            executor_class = ThreadPoolExecutor
        
        results = []
        
        with executor_class(max_workers=self.num_workers) as executor:
            # æäº¤ä»»åŠ¡
            futures = []
            for path in image_paths:
                future = executor.submit(self._process_single_image, path, preprocessor)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in futures:
                try:
                    result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                    results.append(result)
                except Exception as e:
                    logger.warning(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
                    results.append(torch.zeros(3, 224, 224))
        
        return results
    
    def _process_single_image(self, 
                            image_path: str, 
                            preprocessor: PlantDiseasePreprocessor) -> torch.Tensor:
        """å¤„ç†å•å¼ å›¾åƒ"""
        try:
            return preprocessor.preprocess_image(image_path)
        except Exception as e:
            logger.warning(f"å›¾åƒå¤„ç†å¤±è´¥ {image_path}: {e}")
            return torch.zeros(3, 224, 224)

class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨"""
        self.initial_memory = self._get_memory_usage()
        
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def optimize_memory(self) -> Dict[str, float]:
        """æ‰§è¡Œå†…å­˜ä¼˜åŒ–"""
        before_memory = self._get_memory_usage()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†PyTorchç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        after_memory = self._get_memory_usage()
        freed_memory = before_memory - after_memory
        
        return {
            'before_mb': before_memory,
            'after_mb': after_memory,
            'freed_mb': freed_memory
        }
    
    def monitor_memory_usage(self, func: Callable, *args, **kwargs) -> Tuple[Any, Dict[str, float]]:
        """ç›‘æ§å‡½æ•°æ‰§è¡Œæ—¶çš„å†…å­˜ä½¿ç”¨"""
        start_memory = self._get_memory_usage()
        
        result = func(*args, **kwargs)
        
        end_memory = self._get_memory_usage()
        peak_memory = max(start_memory, end_memory)
        
        return result, {
            'start_mb': start_memory,
            'end_mb': end_memory,
            'peak_mb': peak_memory,
            'delta_mb': end_memory - start_memory
        }

class PreprocessingOptimizer:
    """é¢„å¤„ç†æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.memory_optimizer = MemoryOptimizer()
        self.performance_history = []
        
    def create_optimized_dataloader(self,
                                  image_paths: List[str],
                                  labels: List[int],
                                  preprocessor: PlantDiseasePreprocessor,
                                  batch_size: int = 32,
                                  num_workers: int = None,
                                  pin_memory: bool = None,
                                  prefetch_factor: int = 2) -> DataLoader:
        """
        åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            preprocessor: é¢„å¤„ç†å™¨
            batch_size: æ‰¹å¤§å°
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            pin_memory: æ˜¯å¦å›ºå®šå†…å­˜
            prefetch_factor: é¢„å–å› å­
            
        Returns:
            ä¼˜åŒ–çš„DataLoader
        """
        # è‡ªåŠ¨ç¡®å®šæœ€ä¼˜å‚æ•°
        if num_workers is None:
            num_workers = min(cpu_count(), 8)
        
        if pin_memory is None:
            pin_memory = torch.cuda.is_available()
        
        # åˆ›å»ºä¼˜åŒ–æ•°æ®é›†
        dataset = OptimizedImageDataset(
            image_paths=image_paths,
            labels=labels,
            preprocessor=preprocessor,
            cache_enabled=True,
            prefetch_factor=prefetch_factor
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=pin_memory,
            prefetch_factor=prefetch_factor,
            persistent_workers=num_workers > 0,
            drop_last=False
        )
        
        logger.info(f"ä¼˜åŒ–æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ: batch_size={batch_size}, workers={num_workers}")
        
        return dataloader
    
    def benchmark_preprocessing(self,
                              image_paths: List[str],
                              preprocessor: PlantDiseasePreprocessor,
                              batch_sizes: List[int] = None,
                              num_workers_list: List[int] = None) -> Dict[str, Any]:
        """
        åŸºå‡†æµ‹è¯•é¢„å¤„ç†æ€§èƒ½
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            preprocessor: é¢„å¤„ç†å™¨
            batch_sizes: æ‰¹å¤§å°åˆ—è¡¨
            num_workers_list: å·¥ä½œè¿›ç¨‹æ•°åˆ—è¡¨
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        if batch_sizes is None:
            batch_sizes = [16, 32, 64, 128]
        
        if num_workers_list is None:
            num_workers_list = [0, 2, 4, 8]
        
        results = {}
        sample_paths = image_paths[:min(1000, len(image_paths))]  # é™åˆ¶æ ·æœ¬æ•°é‡
        labels = [0] * len(sample_paths)  # è™šæ‹Ÿæ ‡ç­¾
        
        for batch_size in batch_sizes:
            for num_workers in num_workers_list:
                config_name = f"batch_{batch_size}_workers_{num_workers}"
                
                try:
                    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                    dataloader = self.create_optimized_dataloader(
                        sample_paths, labels, preprocessor,
                        batch_size=batch_size,
                        num_workers=num_workers
                    )
                    
                    # æµ‹è¯•æ€§èƒ½
                    metrics = self._measure_dataloader_performance(dataloader)
                    results[config_name] = metrics
                    
                    logger.info(f"é…ç½® {config_name}: {metrics.images_per_second:.2f} images/sec")
                    
                except Exception as e:
                    logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥ {config_name}: {e}")
                    results[config_name] = None
        
        # æ‰¾åˆ°æœ€ä¼˜é…ç½®
        best_config = max(
            [(k, v) for k, v in results.items() if v is not None],
            key=lambda x: x[1].images_per_second,
            default=(None, None)
        )
        
        return {
            'results': results,
            'best_config': best_config[0] if best_config[0] else None,
            'best_performance': best_config[1] if best_config[1] else None
        }
    
    def _measure_dataloader_performance(self, dataloader: DataLoader) -> PerformanceMetrics:
        """æµ‹é‡æ•°æ®åŠ è½½å™¨æ€§èƒ½"""
        start_time = time.time()
        start_memory = self.memory_optimizer._get_memory_usage()
        
        total_images = 0
        batch_count = 0
        max_batches = 10  # é™åˆ¶æµ‹è¯•æ‰¹æ¬¡æ•°
        
        try:
            for batch_idx, (images, labels) in enumerate(dataloader):
                if batch_idx >= max_batches:
                    break
                
                total_images += len(images)
                batch_count += 1
                
                # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
                _ = images.mean()
        
        except Exception as e:
            logger.warning(f"æ€§èƒ½æµ‹è¯•ä¸­æ–­: {e}")
        
        end_time = time.time()
        end_memory = self.memory_optimizer._get_memory_usage()
        
        processing_time = end_time - start_time
        images_per_second = total_images / processing_time if processing_time > 0 else 0
        
        return PerformanceMetrics(
            total_images=total_images,
            processing_time=processing_time,
            memory_usage_mb=end_memory - start_memory,
            images_per_second=images_per_second,
            avg_image_size_mb=0,  # ç®€åŒ–
            cache_hit_rate=0      # ç®€åŒ–
        )
    
    def optimize_for_hardware(self) -> Dict[str, Any]:
        """æ ¹æ®ç¡¬ä»¶é…ç½®ä¼˜åŒ–å‚æ•°"""
        # è·å–ç³»ç»Ÿä¿¡æ¯
        cpu_count_val = cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        has_gpu = torch.cuda.is_available()
        
        # æ¨èé…ç½®
        recommendations = {
            'cpu_cores': cpu_count_val,
            'memory_gb': memory_gb,
            'has_gpu': has_gpu,
            'recommended_batch_size': 32 if memory_gb >= 8 else 16,
            'recommended_num_workers': min(cpu_count_val, 8),
            'recommended_cache_size_mb': min(int(memory_gb * 1024 * 0.1), 1024),
            'pin_memory': has_gpu,
            'prefetch_factor': 2 if memory_gb >= 16 else 1
        }
        
        if has_gpu:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            recommendations['gpu_memory_gb'] = gpu_memory
            recommendations['recommended_batch_size'] = min(
                recommendations['recommended_batch_size'],
                int(gpu_memory * 32)  # ç»éªŒå…¬å¼
            )
        
        return recommendations

# ä¾¿æ·å‡½æ•°
def create_preprocessing_optimizer() -> PreprocessingOptimizer:
    """åˆ›å»ºé¢„å¤„ç†ä¼˜åŒ–å™¨"""
    return PreprocessingOptimizer()

def create_optimized_dataloader(image_paths: List[str],
                              labels: List[int],
                              preprocessor: PlantDiseasePreprocessor,
                              **kwargs) -> DataLoader:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºä¼˜åŒ–æ•°æ®åŠ è½½å™¨"""
    optimizer = create_preprocessing_optimizer()
    return optimizer.create_optimized_dataloader(image_paths, labels, preprocessor, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•é¢„å¤„ç†ä¼˜åŒ–å™¨
    print("ğŸ§ª é¢„å¤„ç†æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•ä¼˜åŒ–å™¨åˆ›å»º
        print("ğŸ“‹ æµ‹è¯•ä¼˜åŒ–å™¨åˆ›å»º...")
        optimizer = create_preprocessing_optimizer()
        print(f"âœ… ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç¡¬ä»¶ä¼˜åŒ–å»ºè®®
        print(f"\nğŸ”§ æµ‹è¯•ç¡¬ä»¶ä¼˜åŒ–å»ºè®®...")
        hardware_config = optimizer.optimize_for_hardware()
        print(f"âœ… ç¡¬ä»¶é…ç½®åˆ†æå®Œæˆ:")
        print(f"   CPUæ ¸å¿ƒæ•°: {hardware_config['cpu_cores']}")
        print(f"   å†…å­˜: {hardware_config['memory_gb']:.1f} GB")
        print(f"   GPUå¯ç”¨: {hardware_config['has_gpu']}")
        print(f"   æ¨èæ‰¹å¤§å°: {hardware_config['recommended_batch_size']}")
        print(f"   æ¨èå·¥ä½œè¿›ç¨‹: {hardware_config['recommended_num_workers']}")
        
        # æµ‹è¯•å†…å­˜ä¼˜åŒ–
        print(f"\nğŸ’¾ æµ‹è¯•å†…å­˜ä¼˜åŒ–...")
        memory_optimizer = MemoryOptimizer()
        memory_stats = memory_optimizer.optimize_memory()
        print(f"âœ… å†…å­˜ä¼˜åŒ–å®Œæˆ:")
        print(f"   ä¼˜åŒ–å‰: {memory_stats['before_mb']:.1f} MB")
        print(f"   ä¼˜åŒ–å: {memory_stats['after_mb']:.1f} MB")
        print(f"   é‡Šæ”¾å†…å­˜: {memory_stats['freed_mb']:.1f} MB")
        
        # æµ‹è¯•å›¾åƒç¼“å­˜
        print(f"\nğŸ—„ï¸ æµ‹è¯•å›¾åƒç¼“å­˜...")
        cache = ImageCache(max_size_mb=100)
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        
        # æµ‹è¯•ç¼“å­˜æ“ä½œ
        cache.put("test_image", test_image)
        cached_image = cache.get("test_image")
        
        print(f"âœ… å›¾åƒç¼“å­˜æµ‹è¯•å®Œæˆ:")
        print(f"   ç¼“å­˜å¤§å°: {cache.current_size / 1024 / 1024:.2f} MB")
        print(f"   å‘½ä¸­ç‡: {cache.get_hit_rate():.2%}")
        print(f"   å›¾åƒåŒ¹é…: {np.array_equal(test_image, cached_image)}")
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†å™¨
        print(f"\nâš¡ æµ‹è¯•æ‰¹é‡å¤„ç†å™¨...")
        from src.image_preprocessing import create_plant_preprocessor, PreprocessingMode
        
        batch_processor = BatchProcessor(batch_size=4, num_workers=2)
        preprocessor = create_plant_preprocessor(PreprocessingMode.VALIDATION)
        
        # åˆ›å»ºè™šæ‹Ÿå›¾åƒè·¯å¾„ï¼ˆå®é™…æµ‹è¯•ä¸­åº”ä½¿ç”¨çœŸå®è·¯å¾„ï¼‰
        dummy_paths = ["dummy_path"] * 4
        print(f"âœ… æ‰¹é‡å¤„ç†å™¨åˆ›å»ºå®Œæˆ:")
        print(f"   æ‰¹å¤§å°: {batch_processor.batch_size}")
        print(f"   å·¥ä½œè¿›ç¨‹: {batch_processor.num_workers}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… é¢„å¤„ç†æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ")