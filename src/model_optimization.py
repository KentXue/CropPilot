#!/usr/bin/env python3
"""
æ¨¡å‹æ€§èƒ½è°ƒä¼˜æ¨¡å—
å®ç°è¶…å‚æ•°æœç´¢ã€æ¨¡å‹é›†æˆã€æ¨ç†ä¼˜åŒ–ç­‰é«˜çº§æ€§èƒ½è°ƒä¼˜åŠŸèƒ½
"""

import os
import sys
import json
import time
import itertools
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import logging
from dataclasses import dataclass, asdict
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader
    import numpy as np
    from sklearn.model_selection import ParameterGrid
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

from src.model_architecture import ModelFactory, create_plant_disease_model
from src.model_trainer import ModelTrainer, TrainingConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class HyperparameterConfig:
    """è¶…å‚æ•°é…ç½®"""
    learning_rates: List[float]
    batch_sizes: List[int]
    weight_decays: List[float]
    dropout_rates: List[float]
    optimizers: List[str]
    schedulers: List[str]
    
    def to_grid(self) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸ºå‚æ•°ç½‘æ ¼"""
        param_dict = {
            'learning_rate': self.learning_rates,
            'batch_size': self.batch_sizes,
            'weight_decay': self.weight_decays,
            'dropout_rate': self.dropout_rates,
            'optimizer_type': self.optimizers,
            'scheduler_type': self.schedulers
        }
        return list(ParameterGrid(param_dict))

class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 search_strategy: str = 'grid',
                 max_trials: int = 50,
                 early_stopping_patience: int = 3):
        """
        åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨
        
        Args:
            search_strategy: æœç´¢ç­–ç•¥ ('grid', 'random', 'bayesian')
            max_trials: æœ€å¤§è¯•éªŒæ¬¡æ•°
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
        """
        self.search_strategy = search_strategy
        self.max_trials = max_trials
        self.early_stopping_patience = early_stopping_patience
        self.trial_results = []
        
    def create_default_search_space(self) -> HyperparameterConfig:
        """åˆ›å»ºé»˜è®¤æœç´¢ç©ºé—´"""
        return HyperparameterConfig(
            learning_rates=[0.0001, 0.001, 0.01],
            batch_sizes=[8, 16, 32],
            weight_decays=[1e-5, 1e-4, 1e-3],
            dropout_rates=[0.2, 0.3, 0.4],
            optimizers=['adam', 'adamw'],
            schedulers=['cosine', 'step']
        )
    
    def grid_search(self, config: HyperparameterConfig) -> List[Dict[str, Any]]:
        """ç½‘æ ¼æœç´¢"""
        param_grid = config.to_grid()
        
        # é™åˆ¶è¯•éªŒæ¬¡æ•°
        if len(param_grid) > self.max_trials:
            param_grid = random.sample(param_grid, self.max_trials)
            logger.info(f"å‚æ•°ç»„åˆè¿‡å¤šï¼Œéšæœºé€‰æ‹© {self.max_trials} ä¸ªç»„åˆè¿›è¡Œæœç´¢")
        
        return param_grid
    
    def random_search(self, config: HyperparameterConfig) -> List[Dict[str, Any]]:
        """éšæœºæœç´¢"""
        param_combinations = []
        
        for _ in range(self.max_trials):
            params = {
                'learning_rate': random.choice(config.learning_rates),
                'batch_size': random.choice(config.batch_sizes),
                'weight_decay': random.choice(config.weight_decays),
                'dropout_rate': random.choice(config.dropout_rates),
                'optimizer_type': random.choice(config.optimizers),
                'scheduler_type': random.choice(config.schedulers)
            }
            param_combinations.append(params)
        
        return param_combinations
    
    def optimize(self, 
                 base_config: TrainingConfig,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 search_config: Optional[HyperparameterConfig] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            base_config: åŸºç¡€è®­ç»ƒé…ç½®
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            search_config: æœç´¢é…ç½®
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        if search_config is None:
            search_config = self.create_default_search_space()
        
        # è·å–å‚æ•°ç»„åˆ
        if self.search_strategy == 'grid':
            param_combinations = self.grid_search(search_config)
        elif self.search_strategy == 'random':
            param_combinations = self.random_search(search_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç­–ç•¥: {self.search_strategy}")
        
        logger.info(f"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œå…± {len(param_combinations)} ä¸ªç»„åˆ")
        
        best_score = 0.0
        best_params = None
        no_improvement_count = 0
        
        for i, params in enumerate(param_combinations):
            logger.info(f"è¯•éªŒ {i+1}/{len(param_combinations)}: {params}")
            
            try:
                # åˆ›å»ºè®­ç»ƒé…ç½®
                trial_config = TrainingConfig()
                for key, value in asdict(base_config).items():
                    setattr(trial_config, key, value)
                
                # æ›´æ–°è¶…å‚æ•°
                for key, value in params.items():
                    setattr(trial_config, key, value)
                
                # çŸ­è®­ç»ƒç”¨äºè¯„ä¼°
                trial_config.num_epochs = 5
                trial_config.save_dir = f"hyperopt_trial_{i}"
                
                # è®­ç»ƒæ¨¡å‹
                trainer = ModelTrainer(trial_config)
                model = trainer.setup_model()
                
                training_results = trainer.train(train_loader, val_loader)
                
                # è®°å½•ç»“æœ
                trial_result = {
                    'trial_id': i,
                    'params': params,
                    'val_acc': training_results['best_val_acc'],
                    'training_time': training_results['total_time']
                }
                
                self.trial_results.append(trial_result)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³ç»“æœ
                if training_results['best_val_acc'] > best_score:
                    best_score = training_results['best_val_acc']
                    best_params = params
                    no_improvement_count = 0
                    logger.info(f"æ–°çš„æœ€ä½³ç»“æœ: {best_score:.4f}")
                else:
                    no_improvement_count += 1
                
                # æ—©åœæ£€æŸ¥
                if no_improvement_count >= self.early_stopping_patience:
                    logger.info(f"è¿ç»­ {self.early_stopping_patience} æ¬¡æ— æ”¹å–„ï¼Œæå‰åœæ­¢")
                    break
                
                # æ¸…ç†
                del trainer, model
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                import shutil
                if os.path.exists(trial_config.save_dir):
                    shutil.rmtree(trial_config.save_dir)
                
            except Exception as e:
                logger.error(f"è¯•éªŒ {i+1} å¤±è´¥: {e}")
                continue
        
        # æ•´ç†ç»“æœ
        optimization_results = {
            'best_params': best_params,
            'best_score': best_score,
            'total_trials': len(self.trial_results),
            'trial_results': self.trial_results
        }
        
        logger.info(f"è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_score:.4f}")
        logger.info(f"æœ€ä½³å‚æ•°: {best_params}")
        
        return optimization_results

class ModelEnsemble:
    """æ¨¡å‹é›†æˆå™¨"""
    
    def __init__(self, ensemble_method: str = 'voting'):
        """
        åˆå§‹åŒ–æ¨¡å‹é›†æˆå™¨
        
        Args:
            ensemble_method: é›†æˆæ–¹æ³• ('voting', 'weighted', 'stacking')
        """
        self.ensemble_method = ensemble_method
        self.models = []
        self.weights = []
        
    def add_model(self, model: nn.Module, weight: float = 1.0):
        """æ·»åŠ æ¨¡å‹åˆ°é›†æˆ"""
        self.models.append(model)
        self.weights.append(weight)
        
    def predict(self, x: torch.Tensor) -> torch.Tensor:
        """é›†æˆé¢„æµ‹"""
        if not self.models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        predictions = []
        
        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(x)
                predictions.append(torch.softmax(pred, dim=1))
        
        # é›†æˆé¢„æµ‹
        if self.ensemble_method == 'voting':
            # ç®€å•å¹³å‡
            ensemble_pred = torch.stack(predictions).mean(dim=0)
        elif self.ensemble_method == 'weighted':
            # åŠ æƒå¹³å‡
            weights = torch.tensor(self.weights, device=x.device)
            weights = weights / weights.sum()
            
            weighted_preds = []
            for i, pred in enumerate(predictions):
                weighted_preds.append(pred * weights[i])
            
            ensemble_pred = torch.stack(weighted_preds).sum(dim=0)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é›†æˆæ–¹æ³•: {self.ensemble_method}")
        
        return ensemble_pred
    
    def evaluate_ensemble(self, data_loader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°é›†æˆæ¨¡å‹"""
        correct = 0
        total = 0
        
        for data, targets in data_loader:
            predictions = self.predict(data)
            predicted = predictions.argmax(dim=1)
            
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
        
        accuracy = 100.0 * correct / total
        
        return {
            'accuracy': accuracy,
            'correct': correct,
            'total': total
        }

class InferenceOptimizer:
    """æ¨ç†ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨ç†ä¼˜åŒ–å™¨"""
        self.optimized_models = {}
        
    def optimize_for_inference(self, model: nn.Module, 
                             example_input: torch.Tensor,
                             optimization_level: str = 'basic') -> nn.Module:
        """
        ä¼˜åŒ–æ¨¡å‹ç”¨äºæ¨ç†
        
        Args:
            model: è¦ä¼˜åŒ–çš„æ¨¡å‹
            example_input: ç¤ºä¾‹è¾“å…¥
            optimization_level: ä¼˜åŒ–çº§åˆ« ('basic', 'advanced')
            
        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹
        """
        model.eval()
        
        if optimization_level == 'basic':
            # åŸºç¡€ä¼˜åŒ–ï¼šJITç¼–è¯‘
            try:
                optimized_model = torch.jit.trace(model, example_input)
                logger.info("JITç¼–è¯‘ä¼˜åŒ–å®Œæˆ")
                return optimized_model
            except Exception as e:
                logger.warning(f"JITç¼–è¯‘å¤±è´¥: {e}")
                return model
        
        elif optimization_level == 'advanced':
            # é«˜çº§ä¼˜åŒ–ï¼šé‡åŒ– + JIT
            try:
                # åŠ¨æ€é‡åŒ–
                quantized_model = torch.quantization.quantize_dynamic(
                    model, {nn.Linear}, dtype=torch.qint8
                )
                
                # JITç¼–è¯‘
                optimized_model = torch.jit.trace(quantized_model, example_input)
                
                logger.info("é‡åŒ– + JITç¼–è¯‘ä¼˜åŒ–å®Œæˆ")
                return optimized_model
            except Exception as e:
                logger.warning(f"é«˜çº§ä¼˜åŒ–å¤±è´¥: {e}")
                return self.optimize_for_inference(model, example_input, 'basic')
        
        return model
    
    def benchmark_inference(self, model: nn.Module, 
                          test_input: torch.Tensor,
                          num_runs: int = 100) -> Dict[str, float]:
        """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        model.eval()
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model(test_input)
        
        # åŒæ­¥GPU
        if test_input.device.type == 'cuda':
            torch.cuda.synchronize()
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(test_input)
        
        if test_input.device.type == 'cuda':
            torch.cuda.synchronize()
        
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time = total_time / num_runs
        throughput = test_input.size(0) / avg_time
        
        return {
            'avg_inference_time_ms': avg_time * 1000,
            'throughput_images_per_sec': throughput,
            'total_time_sec': total_time
        }
    
    def compare_optimizations(self, original_model: nn.Module,
                            example_input: torch.Tensor) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•çš„æ€§èƒ½"""
        results = {}
        
        # åŸå§‹æ¨¡å‹
        original_benchmark = self.benchmark_inference(original_model, example_input)
        results['original'] = original_benchmark
        
        # åŸºç¡€ä¼˜åŒ–
        basic_optimized = self.optimize_for_inference(original_model, example_input, 'basic')
        basic_benchmark = self.benchmark_inference(basic_optimized, example_input)
        results['basic_optimized'] = basic_benchmark
        
        # é«˜çº§ä¼˜åŒ–
        advanced_optimized = self.optimize_for_inference(original_model, example_input, 'advanced')
        advanced_benchmark = self.benchmark_inference(advanced_optimized, example_input)
        results['advanced_optimized'] = advanced_benchmark
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        results['speedup'] = {
            'basic': original_benchmark['avg_inference_time_ms'] / basic_benchmark['avg_inference_time_ms'],
            'advanced': original_benchmark['avg_inference_time_ms'] / advanced_benchmark['avg_inference_time_ms']
        }
        
        return results

class PerformanceTuner:
    """æ€§èƒ½è°ƒä¼˜å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½è°ƒä¼˜å™¨"""
        self.hyperopt = HyperparameterOptimizer()
        self.ensemble = ModelEnsemble()
        self.inference_opt = InferenceOptimizer()
        
    def full_optimization_pipeline(self, 
                                 base_config: TrainingConfig,
                                 train_loader: DataLoader,
                                 val_loader: DataLoader,
                                 test_loader: DataLoader) -> Dict[str, Any]:
        """å®Œæ•´çš„ä¼˜åŒ–æµç¨‹"""
        results = {}
        
        logger.info("å¼€å§‹å®Œæ•´ä¼˜åŒ–æµç¨‹...")
        
        # 1. è¶…å‚æ•°ä¼˜åŒ–
        logger.info("æ­¥éª¤1: è¶…å‚æ•°ä¼˜åŒ–")
        hyperopt_results = self.hyperopt.optimize(base_config, train_loader, val_loader)
        results['hyperparameter_optimization'] = hyperopt_results
        
        # 2. ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        logger.info("æ­¥éª¤2: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        best_params = hyperopt_results['best_params']
        
        final_config = TrainingConfig()
        for key, value in asdict(base_config).items():
            setattr(final_config, key, value)
        
        for key, value in best_params.items():
            setattr(final_config, key, value)
        
        final_config.num_epochs = base_config.num_epochs  # æ¢å¤å®Œæ•´è®­ç»ƒè½®æ•°
        final_config.save_dir = "optimized_model"
        
        trainer = ModelTrainer(final_config)
        model = trainer.setup_model()
        training_results = trainer.train(train_loader, val_loader)
        
        results['final_training'] = training_results
        
        # 3. æ¨ç†ä¼˜åŒ–
        logger.info("æ­¥éª¤3: æ¨ç†ä¼˜åŒ–")
        example_input = next(iter(test_loader))[0][:1]  # å•ä¸ªæ ·æœ¬
        
        inference_results = self.inference_opt.compare_optimizations(model, example_input)
        results['inference_optimization'] = inference_results
        
        # 4. æ¨¡å‹è¯„ä¼°
        logger.info("æ­¥éª¤4: æœ€ç»ˆè¯„ä¼°")
        from src.model_evaluator import create_evaluator
        
        evaluator = create_evaluator()
        metrics, _ = evaluator.evaluate_model(model, test_loader, return_predictions=False)
        
        results['final_evaluation'] = {
            'accuracy': metrics.accuracy,
            'f1_macro': metrics.f1_macro,
            'f1_weighted': metrics.f1_weighted
        }
        
        logger.info("å®Œæ•´ä¼˜åŒ–æµç¨‹å®Œæˆ")
        return results
    
    def save_optimization_report(self, results: Dict[str, Any], save_path: str):
        """ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'optimization_results': results,
            'summary': {
                'best_hyperparams': results.get('hyperparameter_optimization', {}).get('best_params'),
                'final_accuracy': results.get('final_evaluation', {}).get('accuracy'),
                'inference_speedup': results.get('inference_optimization', {}).get('speedup'),
                'training_time': results.get('final_training', {}).get('total_time')
            }
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {save_path}")

# ä¾¿æ·å‡½æ•°
def create_hyperparameter_optimizer(strategy: str = 'random', max_trials: int = 20) -> HyperparameterOptimizer:
    """åˆ›å»ºè¶…å‚æ•°ä¼˜åŒ–å™¨"""
    return HyperparameterOptimizer(search_strategy=strategy, max_trials=max_trials)

def create_model_ensemble(models: List[nn.Module], method: str = 'voting') -> ModelEnsemble:
    """åˆ›å»ºæ¨¡å‹é›†æˆ"""
    ensemble = ModelEnsemble(ensemble_method=method)
    for model in models:
        ensemble.add_model(model)
    return ensemble

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹æ€§èƒ½è°ƒä¼˜
    print("ğŸ§ª æ¨¡å‹æ€§èƒ½è°ƒä¼˜æµ‹è¯•")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•è¶…å‚æ•°é…ç½®
        print("âš™ï¸ æµ‹è¯•è¶…å‚æ•°é…ç½®...")
        config = HyperparameterConfig(
            learning_rates=[0.001, 0.01],
            batch_sizes=[8, 16],
            weight_decays=[1e-4, 1e-3],
            dropout_rates=[0.2, 0.3],
            optimizers=['adam', 'adamw'],
            schedulers=['cosine', 'step']
        )
        
        param_grid = config.to_grid()
        print(f"âœ… å‚æ•°ç½‘æ ¼å¤§å°: {len(param_grid)}")
        print(f"   ç¤ºä¾‹å‚æ•°: {param_grid[0]}")
        
        # æµ‹è¯•è¶…å‚æ•°ä¼˜åŒ–å™¨
        print(f"\nğŸ” æµ‹è¯•è¶…å‚æ•°ä¼˜åŒ–å™¨...")
        optimizer = HyperparameterOptimizer(search_strategy='random', max_trials=5)
        
        default_config = optimizer.create_default_search_space()
        print(f"âœ… é»˜è®¤æœç´¢ç©ºé—´åˆ›å»ºå®Œæˆ")
        print(f"   å­¦ä¹ ç‡èŒƒå›´: {default_config.learning_rates}")
        print(f"   æ‰¹å¤§å°èŒƒå›´: {default_config.batch_sizes}")
        
        # æµ‹è¯•æ¨¡å‹é›†æˆ
        print(f"\nğŸ¤ æµ‹è¯•æ¨¡å‹é›†æˆ...")
        ensemble = ModelEnsemble(ensemble_method='voting')
        
        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹
        model1 = create_plant_disease_model('efficientnet', pretrained=False)
        model2 = create_plant_disease_model('efficientnet', pretrained=False)
        
        ensemble.add_model(model1, weight=1.0)
        ensemble.add_model(model2, weight=1.0)
        
        # æµ‹è¯•é¢„æµ‹
        test_input = torch.randn(2, 3, 224, 224)
        ensemble_pred = ensemble.predict(test_input)
        
        print(f"âœ… é›†æˆé¢„æµ‹å®Œæˆ")
        print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {ensemble_pred.shape}")
        
        # æµ‹è¯•æ¨ç†ä¼˜åŒ–
        print(f"\nğŸš€ æµ‹è¯•æ¨ç†ä¼˜åŒ–...")
        inference_opt = InferenceOptimizer()
        
        # åŸºå‡†æµ‹è¯•
        benchmark_results = inference_opt.benchmark_inference(model1, test_input, num_runs=10)
        
        print(f"âœ… æ¨ç†åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {benchmark_results['avg_inference_time_ms']:.2f}ms")
        print(f"   ååé‡: {benchmark_results['throughput_images_per_sec']:.1f} images/sec")
        
        # æµ‹è¯•JITä¼˜åŒ–
        try:
            optimized_model = inference_opt.optimize_for_inference(model1, test_input, 'basic')
            print(f"âœ… JITä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  JITä¼˜åŒ–è·³è¿‡: {e}")
        
        # æµ‹è¯•æ€§èƒ½è°ƒä¼˜å™¨
        print(f"\nğŸ¯ æµ‹è¯•æ€§èƒ½è°ƒä¼˜å™¨...")
        tuner = PerformanceTuner()
        
        print(f"âœ… æ€§èƒ½è°ƒä¼˜å™¨åˆ›å»ºå®Œæˆ")
        print(f"   åŒ…å«ç»„ä»¶: è¶…å‚æ•°ä¼˜åŒ–ã€æ¨¡å‹é›†æˆã€æ¨ç†ä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… æ¨¡å‹æ€§èƒ½è°ƒä¼˜æµ‹è¯•å®Œæˆ")