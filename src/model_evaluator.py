#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°å’ŒéªŒè¯ç³»ç»Ÿ
å®ç°ModelEvaluatorç±»ï¼Œæ”¯æŒå‡†ç¡®ç‡ã€F1åˆ†æ•°ã€æ··æ·†çŸ©é˜µå’Œäº¤å‰éªŒè¯
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import logging
from dataclasses import dataclass, asdict
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset, Subset
    import numpy as np
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score,
        confusion_matrix, classification_report, roc_auc_score,
        precision_recall_curve, roc_curve, auc
    )
    from sklearn.model_selection import StratifiedKFold
    import matplotlib.pyplot as plt
    import seaborn as sns
    from tqdm import tqdm
    import pandas as pd
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    accuracy: float = 0.0
    precision_macro: float = 0.0
    precision_micro: float = 0.0
    precision_weighted: float = 0.0
    recall_macro: float = 0.0
    recall_micro: float = 0.0
    recall_weighted: float = 0.0
    f1_macro: float = 0.0
    f1_micro: float = 0.0
    f1_weighted: float = 0.0
    auc_macro: float = 0.0
    auc_micro: float = 0.0
    top_k_accuracy: Dict[int, float] = None
    
    def __post_init__(self):
        if self.top_k_accuracy is None:
            self.top_k_accuracy = {}

@dataclass
class ClassMetrics:
    """å•ä¸ªç±»åˆ«çš„æŒ‡æ ‡"""
    class_name: str
    class_id: int
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    support: int = 0
    auc: float = 0.0

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, 
                 class_names: Optional[List[str]] = None,
                 device: str = 'auto'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            device: è®¡ç®—è®¾å¤‡
        """
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…å¿…è¦ä¾èµ–")
        
        self.class_names = class_names
        self.device = self._setup_device(device)
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.last_predictions = None
        self.last_targets = None
        self.last_probabilities = None
        self.evaluation_history = []
        
        logger.info(f"ModelEvaluatoråˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¾å¤‡"""
        if device == 'auto':
            if torch.cuda.is_available():
                return torch.device('cuda')
            else:
                return torch.device('cpu')
        else:
            return torch.device(device)
    
    def evaluate_model(self, 
                      model: nn.Module,
                      data_loader: DataLoader,
                      criterion: Optional[nn.Module] = None,
                      return_predictions: bool = True) -> Tuple[EvaluationMetrics, Optional[Dict[str, np.ndarray]]]:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            model: è¦è¯„ä¼°çš„æ¨¡å‹
            data_loader: æ•°æ®åŠ è½½å™¨
            criterion: æŸå¤±å‡½æ•°
            return_predictions: æ˜¯å¦è¿”å›é¢„æµ‹ç»“æœ
            
        Returns:
            (è¯„ä¼°æŒ‡æ ‡, é¢„æµ‹ç»“æœå­—å…¸)
        """
        model.eval()
        
        all_predictions = []
        all_targets = []
        all_probabilities = []
        total_loss = 0.0
        
        with torch.no_grad():
            for data, targets in tqdm(data_loader, desc='Evaluating'):
                data, targets = data.to(self.device), targets.to(self.device)
                
                outputs = model(data)
                
                # è®¡ç®—æŸå¤±
                if criterion is not None:
                    loss = criterion(outputs, targets)
                    total_loss += loss.item()
                
                # è·å–é¢„æµ‹å’Œæ¦‚ç‡
                probabilities = torch.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        predictions = np.array(all_predictions)
        targets = np.array(all_targets)
        probabilities = np.array(all_probabilities)
        
        # å­˜å‚¨ç»“æœ
        self.last_predictions = predictions
        self.last_targets = targets
        self.last_probabilities = probabilities
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_metrics(targets, predictions, probabilities)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        if criterion is not None:
            avg_loss = total_loss / len(data_loader)
            logger.info(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # å‡†å¤‡è¿”å›ç»“æœ
        results = None
        if return_predictions:
            results = {
                'predictions': predictions,
                'targets': targets,
                'probabilities': probabilities
            }
        
        return metrics, results
    
    def _calculate_metrics(self, 
                          targets: np.ndarray, 
                          predictions: np.ndarray, 
                          probabilities: np.ndarray) -> EvaluationMetrics:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        
        # åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(targets, predictions)
        
        # ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision_macro = precision_score(targets, predictions, average='macro', zero_division=0)
        precision_micro = precision_score(targets, predictions, average='micro', zero_division=0)
        precision_weighted = precision_score(targets, predictions, average='weighted', zero_division=0)
        
        recall_macro = recall_score(targets, predictions, average='macro', zero_division=0)
        recall_micro = recall_score(targets, predictions, average='micro', zero_division=0)
        recall_weighted = recall_score(targets, predictions, average='weighted', zero_division=0)
        
        f1_macro = f1_score(targets, predictions, average='macro', zero_division=0)
        f1_micro = f1_score(targets, predictions, average='micro', zero_division=0)
        f1_weighted = f1_score(targets, predictions, average='weighted', zero_division=0)
        
        # AUCæŒ‡æ ‡ï¼ˆå¤šåˆ†ç±»ï¼‰
        auc_macro = 0.0
        auc_micro = 0.0
        
        try:
            # è®¡ç®—å¤šåˆ†ç±»AUC
            n_classes = probabilities.shape[1]
            
            # å®å¹³å‡AUC
            auc_scores = []
            for i in range(n_classes):
                if len(np.unique(targets == i)) > 1:  # ç¡®ä¿ç±»åˆ«å­˜åœ¨
                    auc_score = roc_auc_score((targets == i).astype(int), probabilities[:, i])
                    auc_scores.append(auc_score)
            
            if auc_scores:
                auc_macro = np.mean(auc_scores)
            
            # å¾®å¹³å‡AUC
            if n_classes > 2:
                # å¯¹äºå¤šåˆ†ç±»ï¼Œä½¿ç”¨one-vs-restæ–¹å¼
                targets_onehot = np.eye(n_classes)[targets]
                auc_micro = roc_auc_score(targets_onehot, probabilities, average='micro', multi_class='ovr')
        
        except Exception as e:
            logger.warning(f"AUCè®¡ç®—å¤±è´¥: {e}")
        
        # Top-kå‡†ç¡®ç‡
        top_k_accuracy = {}
        for k in [1, 3, 5]:
            if k <= probabilities.shape[1]:
                top_k_preds = np.argsort(probabilities, axis=1)[:, -k:]
                top_k_acc = np.mean([targets[i] in top_k_preds[i] for i in range(len(targets))])
                top_k_accuracy[k] = top_k_acc
        
        return EvaluationMetrics(
            accuracy=accuracy,
            precision_macro=precision_macro,
            precision_micro=precision_micro,
            precision_weighted=precision_weighted,
            recall_macro=recall_macro,
            recall_micro=recall_micro,
            recall_weighted=recall_weighted,
            f1_macro=f1_macro,
            f1_micro=f1_micro,
            f1_weighted=f1_weighted,
            auc_macro=auc_macro,
            auc_micro=auc_micro,
            top_k_accuracy=top_k_accuracy
        )
    
    def get_class_metrics(self) -> List[ClassMetrics]:
        """è·å–æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡"""
        if self.last_predictions is None or self.last_targets is None:
            raise ValueError("è¯·å…ˆè¿è¡Œevaluate_model")
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class = precision_score(
            self.last_targets, self.last_predictions, 
            average=None, zero_division=0
        )
        recall_per_class = recall_score(
            self.last_targets, self.last_predictions, 
            average=None, zero_division=0
        )
        f1_per_class = f1_score(
            self.last_targets, self.last_predictions, 
            average=None, zero_division=0
        )
        
        # è®¡ç®—æ”¯æŒæ•°ï¼ˆæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ï¼‰
        unique_targets, support_counts = np.unique(self.last_targets, return_counts=True)
        support_dict = dict(zip(unique_targets, support_counts))
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„AUC
        auc_per_class = []
        n_classes = self.last_probabilities.shape[1]
        
        for i in range(n_classes):
            try:
                if len(np.unique(self.last_targets == i)) > 1:
                    auc_score = roc_auc_score(
                        (self.last_targets == i).astype(int), 
                        self.last_probabilities[:, i]
                    )
                    auc_per_class.append(auc_score)
                else:
                    auc_per_class.append(0.0)
            except:
                auc_per_class.append(0.0)
        
        # åˆ›å»ºç±»åˆ«æŒ‡æ ‡åˆ—è¡¨
        class_metrics = []
        for i in range(len(precision_per_class)):
            class_name = self.class_names[i] if self.class_names else f'Class_{i}'
            
            metrics = ClassMetrics(
                class_name=class_name,
                class_id=i,
                precision=precision_per_class[i],
                recall=recall_per_class[i],
                f1_score=f1_per_class[i],
                support=support_dict.get(i, 0),
                auc=auc_per_class[i]
            )
            class_metrics.append(metrics)
        
        return class_metrics
    
    def plot_confusion_matrix(self, 
                            save_path: Optional[str] = None,
                            normalize: bool = True,
                            figsize: Tuple[int, int] = (12, 10)) -> None:
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        if self.last_predictions is None or self.last_targets is None:
            raise ValueError("è¯·å…ˆè¿è¡Œevaluate_model")
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.last_targets, self.last_predictions)
        
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            title = 'Normalized Confusion Matrix'
            fmt = '.2f'
        else:
            title = 'Confusion Matrix'
            fmt = 'd'
        
        # ç»˜åˆ¶
        plt.figure(figsize=figsize)
        
        # ä½¿ç”¨ç±»åˆ«åç§°ä½œä¸ºæ ‡ç­¾
        labels = self.class_names if self.class_names else [f'Class_{i}' for i in range(len(cm))]
        
        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',
                   xticklabels=labels, yticklabels=labels)
        
        plt.title(title)
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def plot_roc_curves(self, 
                       save_path: Optional[str] = None,
                       figsize: Tuple[int, int] = (12, 8)) -> None:
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if self.last_predictions is None or self.last_targets is None:
            raise ValueError("è¯·å…ˆè¿è¡Œevaluate_model")
        
        n_classes = self.last_probabilities.shape[1]
        
        plt.figure(figsize=figsize)
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶ROCæ›²çº¿
        for i in range(min(n_classes, 10)):  # æœ€å¤šæ˜¾ç¤º10ä¸ªç±»åˆ«
            # äºŒåˆ†ç±»æ ‡ç­¾
            y_true = (self.last_targets == i).astype(int)
            y_score = self.last_probabilities[:, i]
            
            # è®¡ç®—ROCæ›²çº¿
            fpr, tpr, _ = roc_curve(y_true, y_score)
            roc_auc = auc(fpr, tpr)
            
            # ç»˜åˆ¶
            class_name = self.class_names[i] if self.class_names else f'Class {i}'
            plt.plot(fpr, tpr, linewidth=2, 
                    label=f'{class_name} (AUC = {roc_auc:.2f})')
        
        # ç»˜åˆ¶å¯¹è§’çº¿
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ROCæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def plot_precision_recall_curves(self, 
                                    save_path: Optional[str] = None,
                                    figsize: Tuple[int, int] = (12, 8)) -> None:
        """ç»˜åˆ¶ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿"""
        if self.last_predictions is None or self.last_targets is None:
            raise ValueError("è¯·å…ˆè¿è¡Œevaluate_model")
        
        n_classes = self.last_probabilities.shape[1]
        
        plt.figure(figsize=figsize)
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶PRæ›²çº¿
        for i in range(min(n_classes, 10)):  # æœ€å¤šæ˜¾ç¤º10ä¸ªç±»åˆ«
            # äºŒåˆ†ç±»æ ‡ç­¾
            y_true = (self.last_targets == i).astype(int)
            y_score = self.last_probabilities[:, i]
            
            # è®¡ç®—PRæ›²çº¿
            precision, recall, _ = precision_recall_curve(y_true, y_score)
            pr_auc = auc(recall, precision)
            
            # ç»˜åˆ¶
            class_name = self.class_names[i] if self.class_names else f'Class {i}'
            plt.plot(recall, precision, linewidth=2,
                    label=f'{class_name} (AUC = {pr_auc:.2f})')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curves')
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"PRæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def cross_validate(self, 
                      model_factory: Callable,
                      dataset: Dataset,
                      k_folds: int = 5,
                      batch_size: int = 32,
                      num_workers: int = 0) -> Dict[str, Any]:
        """
        KæŠ˜äº¤å‰éªŒè¯
        
        Args:
            model_factory: æ¨¡å‹å·¥å‚å‡½æ•°ï¼Œè¿”å›æ–°çš„æ¨¡å‹å®ä¾‹
            dataset: æ•°æ®é›†
            k_folds: æŠ˜æ•°
            batch_size: æ‰¹å¤§å°
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            
        Returns:
            äº¤å‰éªŒè¯ç»“æœ
        """
        logger.info(f"å¼€å§‹{k_folds}æŠ˜äº¤å‰éªŒè¯...")
        
        # è·å–æ‰€æœ‰æ ‡ç­¾ç”¨äºåˆ†å±‚é‡‡æ ·
        all_targets = []
        for i in range(len(dataset)):
            _, target = dataset[i]
            all_targets.append(target)
        
        all_targets = np.array(all_targets)
        
        # åˆ›å»ºåˆ†å±‚KæŠ˜
        skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
        
        fold_results = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(all_targets)), all_targets)):
            logger.info(f"æ‰§è¡Œç¬¬ {fold + 1}/{k_folds} æŠ˜...")
            
            # åˆ›å»ºæ•°æ®å­é›†
            val_subset = Subset(dataset, val_idx)
            val_loader = DataLoader(val_subset, batch_size=batch_size, 
                                  shuffle=False, num_workers=num_workers)
            
            # åˆ›å»ºæ–°æ¨¡å‹
            model = model_factory()
            model.to(self.device)
            
            # è¯„ä¼°æ¨¡å‹ï¼ˆè¿™é‡Œå‡è®¾æ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼‰
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ éœ€è¦åœ¨è¿™é‡Œè®­ç»ƒæ¨¡å‹
            metrics, _ = self.evaluate_model(model, val_loader, return_predictions=False)
            
            fold_result = {
                'fold': fold + 1,
                'metrics': asdict(metrics),
                'val_size': len(val_idx)
            }
            fold_results.append(fold_result)
            
            logger.info(f"ç¬¬ {fold + 1} æŠ˜ç»“æœ - å‡†ç¡®ç‡: {metrics.accuracy:.4f}, F1: {metrics.f1_macro:.4f}")
        
        # è®¡ç®—å¹³å‡ç»“æœ
        avg_metrics = {}
        for key in fold_results[0]['metrics'].keys():
            if key != 'top_k_accuracy':
                values = [result['metrics'][key] for result in fold_results]
                avg_metrics[key] = np.mean(values)
                avg_metrics[f'{key}_std'] = np.std(values)
        
        # å¤„ç†top_k_accuracy
        if 'top_k_accuracy' in fold_results[0]['metrics']:
            avg_top_k = {}
            for k in fold_results[0]['metrics']['top_k_accuracy'].keys():
                values = [result['metrics']['top_k_accuracy'][k] for result in fold_results]
                avg_top_k[k] = np.mean(values)
            avg_metrics['top_k_accuracy'] = avg_top_k
        
        cv_results = {
            'fold_results': fold_results,
            'average_metrics': avg_metrics,
            'k_folds': k_folds,
            'total_samples': len(dataset)
        }
        
        logger.info(f"äº¤å‰éªŒè¯å®Œæˆ - å¹³å‡å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f} Â± {avg_metrics['accuracy_std']:.4f}")
        
        return cv_results
    
    def generate_classification_report(self) -> str:
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
        if self.last_predictions is None or self.last_targets is None:
            raise ValueError("è¯·å…ˆè¿è¡Œevaluate_model")
        
        target_names = self.class_names if self.class_names else None
        
        report = classification_report(
            self.last_targets, 
            self.last_predictions,
            target_names=target_names,
            digits=4
        )
        
        return report
    
    def save_evaluation_report(self, 
                             metrics: EvaluationMetrics,
                             save_dir: str,
                             model_name: str = 'model') -> None:
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æŒ‡æ ‡
        metrics_dict = asdict(metrics)
        with open(save_dir / f'{model_name}_metrics.json', 'w') as f:
            json.dump(metrics_dict, f, indent=2)
        
        # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
        if self.last_predictions is not None:
            report = self.generate_classification_report()
            with open(save_dir / f'{model_name}_classification_report.txt', 'w') as f:
                f.write(report)
        
        # ä¿å­˜ç±»åˆ«æŒ‡æ ‡
        if self.last_predictions is not None:
            class_metrics = self.get_class_metrics()
            class_metrics_dict = [asdict(cm) for cm in class_metrics]
            with open(save_dir / f'{model_name}_class_metrics.json', 'w') as f:
                json.dump(class_metrics_dict, f, indent=2)
        
        # ç»˜åˆ¶å¹¶ä¿å­˜å›¾è¡¨
        if self.last_predictions is not None:
            self.plot_confusion_matrix(save_path=save_dir / f'{model_name}_confusion_matrix.png')
            self.plot_roc_curves(save_path=save_dir / f'{model_name}_roc_curves.png')
            self.plot_precision_recall_curves(save_path=save_dir / f'{model_name}_pr_curves.png')
        
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}")

# ä¾¿æ·å‡½æ•°
def create_evaluator(class_names: Optional[List[str]] = None, 
                    device: str = 'auto') -> ModelEvaluator:
    """åˆ›å»ºæ¨¡å‹è¯„ä¼°å™¨"""
    return ModelEvaluator(class_names=class_names, device=device)

def evaluate_model_simple(model: nn.Module, 
                         data_loader: DataLoader,
                         class_names: Optional[List[str]] = None) -> EvaluationMetrics:
    """ç®€å•æ¨¡å‹è¯„ä¼°"""
    evaluator = create_evaluator(class_names=class_names)
    metrics, _ = evaluator.evaluate_model(model, data_loader, return_predictions=False)
    return metrics

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹è¯„ä¼°ç³»ç»Ÿ
    print("ğŸ§ª æ¨¡å‹è¯„ä¼°ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•è¯„ä¼°å™¨åˆ›å»º
        print("ğŸ“‹ æµ‹è¯•è¯„ä¼°å™¨åˆ›å»º...")
        class_names = [f'Class_{i}' for i in range(10)]
        evaluator = create_evaluator(class_names=class_names)
        
        print(f"âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ:")
        print(f"   è®¾å¤‡: {evaluator.device}")
        print(f"   ç±»åˆ«æ•°: {len(class_names)}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        print(f"\nğŸ” æµ‹è¯•æŒ‡æ ‡è®¡ç®—...")
        
        # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
        n_samples = 1000
        n_classes = 10
        
        targets = np.random.randint(0, n_classes, n_samples)
        predictions = np.random.randint(0, n_classes, n_samples)
        probabilities = np.random.rand(n_samples, n_classes)
        probabilities = probabilities / probabilities.sum(axis=1, keepdims=True)
        
        # è®¾ç½®æ¨¡æ‹Ÿç»“æœ
        evaluator.last_targets = targets
        evaluator.last_predictions = predictions
        evaluator.last_probabilities = probabilities
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = evaluator._calculate_metrics(targets, predictions, probabilities)
        
        print(f"âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆ:")
        print(f"   å‡†ç¡®ç‡: {metrics.accuracy:.4f}")
        print(f"   F1åˆ†æ•°(å®): {metrics.f1_macro:.4f}")
        print(f"   F1åˆ†æ•°(å¾®): {metrics.f1_micro:.4f}")
        print(f"   AUC(å®): {metrics.auc_macro:.4f}")
        print(f"   Top-1å‡†ç¡®ç‡: {metrics.top_k_accuracy.get(1, 0):.4f}")
        print(f"   Top-3å‡†ç¡®ç‡: {metrics.top_k_accuracy.get(3, 0):.4f}")
        
        # æµ‹è¯•ç±»åˆ«æŒ‡æ ‡
        print(f"\nğŸ“Š æµ‹è¯•ç±»åˆ«æŒ‡æ ‡...")
        class_metrics = evaluator.get_class_metrics()
        
        print(f"âœ… ç±»åˆ«æŒ‡æ ‡è®¡ç®—å®Œæˆ:")
        print(f"   ç±»åˆ«æ•°: {len(class_metrics)}")
        print(f"   å‰3ä¸ªç±»åˆ«:")
        for i, cm in enumerate(class_metrics[:3]):
            print(f"     {cm.class_name}: P={cm.precision:.3f}, R={cm.recall:.3f}, F1={cm.f1_score:.3f}")
        
        # æµ‹è¯•åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“„ æµ‹è¯•åˆ†ç±»æŠ¥å‘Š...")
        report = evaluator.generate_classification_report()
        print(f"âœ… åˆ†ç±»æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        print(f"   æŠ¥å‘Šé•¿åº¦: {len(report)} å­—ç¬¦")
        print(f"   å‰200å­—ç¬¦: {report[:200]}...")
        
        # æµ‹è¯•è¯„ä¼°æŒ‡æ ‡æ•°æ®ç±»
        print(f"\nğŸ·ï¸ æµ‹è¯•è¯„ä¼°æŒ‡æ ‡æ•°æ®ç±»...")
        metrics_dict = asdict(metrics)
        print(f"âœ… æŒ‡æ ‡åºåˆ—åŒ–å®Œæˆ:")
        print(f"   æŒ‡æ ‡æ•°é‡: {len(metrics_dict)}")
        print(f"   ä¸»è¦æŒ‡æ ‡: accuracy, f1_macro, precision_macro")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… æ¨¡å‹è¯„ä¼°ç³»ç»Ÿæµ‹è¯•å®Œæˆ")