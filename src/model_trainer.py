#!/usr/bin/env python3
"""
æ¨¡å‹è®­ç»ƒæ¡†æ¶
å®ç°ModelTrainerç±»ï¼Œæ”¯æŒè®­ç»ƒè¿›åº¦ç›‘æ§ã€æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
import math

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader
    from torch.utils.tensorboard import SummaryWriter
    import numpy as np
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

from src.model_architecture import PlantDiseaseEfficientNet, ModelFactory

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # åŸºç¡€é…ç½®
    num_epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 0.001
    weight_decay: float = 1e-4
    
    # æ¨¡å‹é…ç½®
    model_type: str = 'efficientnet'
    model_name: str = 'efficientnet-b4'
    num_classes: int = 38
    pretrained: bool = True
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer_type: str = 'adamw'  # 'adam', 'adamw', 'sgd'
    momentum: float = 0.9
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler_type: str = 'cosine'  # 'step', 'cosine', 'plateau'
    step_size: int = 30
    gamma: float = 0.1
    min_lr: float = 1e-6
    
    # æ—©åœé…ç½®
    early_stopping: bool = True
    patience: int = 10
    min_delta: float = 0.001
    
    # ä¿å­˜é…ç½®
    save_dir: str = 'checkpoints'
    save_best_only: bool = True
    save_frequency: int = 5
    
    # å…¶ä»–é…ç½®
    device: str = 'auto'  # 'auto', 'cpu', 'cuda'
    mixed_precision: bool = True
    gradient_clip_norm: float = 1.0
    
    # æ•°æ®å¢å¼º
    label_smoothing: float = 0.1
    mixup_alpha: float = 0.2
    cutmix_alpha: float = 1.0

@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡"""
    epoch: int = 0
    train_loss: float = 0.0
    train_acc: float = 0.0
    val_loss: float = 0.0
    val_acc: float = 0.0
    learning_rate: float = 0.0
    epoch_time: float = 0.0
    best_val_acc: float = 0.0
    best_epoch: int = 0

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience: int = 10, min_delta: float = 0.001, mode: str = 'max'):
        """
        åˆå§‹åŒ–æ—©åœ
        
        Args:
            patience: è€å¿ƒå€¼ï¼ˆå¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„å°±åœæ­¢ï¼‰
            min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
            mode: 'max' è¡¨ç¤ºæŒ‡æ ‡è¶Šå¤§è¶Šå¥½ï¼Œ'min' è¡¨ç¤ºè¶Šå°è¶Šå¥½
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.best_score = None
        self.counter = 0
        self.early_stop = False
        
    def __call__(self, score: float) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Args:
            score: å½“å‰æŒ‡æ ‡å€¼
            
        Returns:
            æ˜¯å¦åº”è¯¥æ—©åœ
        """
        if self.best_score is None:
            self.best_score = score
        elif self._is_better(score, self.best_score):
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        
        return self.early_stop
    
    def _is_better(self, current: float, best: float) -> bool:
        """åˆ¤æ–­å½“å‰åˆ†æ•°æ˜¯å¦æ›´å¥½"""
        if self.mode == 'max':
            return current > best + self.min_delta
        else:
            return current < best - self.min_delta

class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°"""
    
    def __init__(self, num_classes: int, smoothing: float = 0.1):
        """
        åˆå§‹åŒ–æ ‡ç­¾å¹³æ»‘æŸå¤±
        
        Args:
            num_classes: ç±»åˆ«æ•°
            smoothing: å¹³æ»‘å‚æ•°
        """
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
        
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        pred = pred.log_softmax(dim=-1)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.num_classes - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        
        return torch.mean(torch.sum(-true_dist * pred, dim=-1))

class MixupCutmixLoss:
    """Mixupå’ŒCutmixæŸå¤±å‡½æ•°"""
    
    def __init__(self, criterion: nn.Module):
        """
        åˆå§‹åŒ–Mixup/CutmixæŸå¤±
        
        Args:
            criterion: åŸºç¡€æŸå¤±å‡½æ•°
        """
        self.criterion = criterion
    
    def __call__(self, pred: torch.Tensor, target_a: torch.Tensor, 
                 target_b: torch.Tensor, lam: float) -> torch.Tensor:
        """è®¡ç®—æ··åˆæŸå¤±"""
        return lam * self.criterion(pred, target_a) + (1 - lam) * self.criterion(pred, target_b)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…å¿…è¦ä¾èµ–")
        
        self.config = config
        self.device = self._setup_device()
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.scaler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_acc = 0.0
        self.best_epoch = 0
        self.training_history = []
        
        # æ—©åœ
        if config.early_stopping:
            self.early_stopping = EarlyStopping(
                patience=config.patience,
                min_delta=config.min_delta,
                mode='max'
            )
        else:
            self.early_stopping = None
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(config.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=self.save_dir / 'logs')
        
        logger.info(f"ModelTraineråˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¾å¤‡"""
        if self.config.device == 'auto':
            if torch.cuda.is_available():
                device = torch.device('cuda')
                logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            else:
                device = torch.device('cpu')
                logger.info("ä½¿ç”¨CPU")
        else:
            device = torch.device(self.config.device)
        
        return device
    
    def setup_model(self, model: Optional[nn.Module] = None) -> nn.Module:
        """
        è®¾ç½®æ¨¡å‹
        
        Args:
            model: å¤–éƒ¨æä¾›çš„æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™æ ¹æ®é…ç½®åˆ›å»º
            
        Returns:
            è®¾ç½®å¥½çš„æ¨¡å‹
        """
        if model is None:
            # æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹
            model = ModelFactory.create_model(
                model_type=self.config.model_type,
                num_classes=self.config.num_classes,
                model_name=self.config.model_name,
                pretrained=self.config.pretrained
            )
        
        self.model = model.to(self.device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self._setup_optimizer()
        
        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        self._setup_scheduler()
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        self._setup_criterion()
        
        # è®¾ç½®æ··åˆç²¾åº¦
        if self.config.mixed_precision and self.device.type == 'cuda':
            self.scaler = torch.cuda.amp.GradScaler()
        
        logger.info(f"æ¨¡å‹è®¾ç½®å®Œæˆ - å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        return self.model
    
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        if self.config.optimizer_type.lower() == 'adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer_type.lower() == 'adamw':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer_type.lower() == 'sgd':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.config.learning_rate,
                momentum=self.config.momentum,
                weight_decay=self.config.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {self.config.optimizer_type}")
    
    def _setup_scheduler(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.config.scheduler_type.lower() == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.step_size,
                gamma=self.config.gamma
            )
        elif self.config.scheduler_type.lower() == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.num_epochs,
                eta_min=self.config.min_lr
            )
        elif self.config.scheduler_type.lower() == 'plateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='max',
                factor=self.config.gamma,
                patience=self.config.patience // 2,
                min_lr=self.config.min_lr
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {self.config.scheduler_type}")
    
    def _setup_criterion(self):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        if self.config.label_smoothing > 0:
            self.criterion = LabelSmoothingLoss(
                num_classes=self.config.num_classes,
                smoothing=self.config.label_smoothing
            )
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        self.criterion = self.criterion.to(self.device)
    
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            (å¹³å‡æŸå¤±, å‡†ç¡®ç‡)
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {self.current_epoch+1}/{self.config.num_epochs}')
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config.gradient_clip_norm > 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.gradient_clip_norm
                    )
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config.gradient_clip_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.gradient_clip_norm
                    )
                
                self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100. * correct / total:.2f}%'
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:
        """
        éªŒè¯ä¸€ä¸ªepoch
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            (å¹³å‡æŸå¤±, å‡†ç¡®ç‡)
        """
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in tqdm(val_loader, desc='Validation'):
                data, target = data.to(self.device), target.to(self.device)
                
                if self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, Any]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            è®­ç»ƒå†å²
        """
        logger.info("å¼€å§‹è®­ç»ƒ...")
        start_time = time.time()
        
        for epoch in range(self.config.num_epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.config.scheduler_type.lower() == 'plateau':
                self.scheduler.step(val_acc)
            else:
                self.scheduler.step()
            
            current_lr = self.optimizer.param_groups[0]['lr']
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•æŒ‡æ ‡
            metrics = TrainingMetrics(
                epoch=epoch + 1,
                train_loss=train_loss,
                train_acc=train_acc,
                val_loss=val_loss,
                val_acc=val_acc,
                learning_rate=current_lr,
                epoch_time=epoch_time,
                best_val_acc=self.best_val_acc,
                best_epoch=self.best_epoch
            )
            
            self.training_history.append(asdict(metrics))
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.best_epoch = epoch + 1
                metrics.best_val_acc = self.best_val_acc
                metrics.best_epoch = self.best_epoch
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if self.config.save_best_only:
                    self.save_checkpoint('best_model.pth', metrics)
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.config.save_frequency == 0:
                self.save_checkpoint(f'checkpoint_epoch_{epoch+1}.pth', metrics)
            
            # TensorBoardè®°å½•
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            self.writer.add_scalar('Loss/Val', val_loss, epoch)
            self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
            self.writer.add_scalar('Accuracy/Val', val_acc, epoch)
            self.writer.add_scalar('Learning_Rate', current_lr, epoch)
            
            # æ‰“å°è¿›åº¦
            logger.info(
                f'Epoch {epoch+1}/{self.config.num_epochs} - '
                f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - '
                f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% - '
                f'LR: {current_lr:.6f} - Time: {epoch_time:.2f}s'
            )
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping is not None:
                if self.early_stopping(val_acc):
                    logger.info(f'æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ')
                    break
        
        total_time = time.time() - start_time
        logger.info(f'è®­ç»ƒå®Œæˆ - æ€»æ—¶é—´: {total_time:.2f}s, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%')
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        # å…³é—­TensorBoard
        self.writer.close()
        
        return {
            'training_history': self.training_history,
            'best_val_acc': self.best_val_acc,
            'best_epoch': self.best_epoch,
            'total_time': total_time
        }
    
    def save_checkpoint(self, filename: str, metrics: TrainingMetrics):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': metrics.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_acc': self.best_val_acc,
            'best_epoch': self.best_epoch,
            'config': asdict(self.config),
            'metrics': asdict(metrics)
        }
        
        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint, self.save_dir / filename)
        logger.info(f'æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}')
    
    def load_checkpoint(self, filename: str) -> Dict[str, Any]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = self.save_dir / filename
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if 'scaler_state_dict' in checkpoint and self.scaler is not None:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        self.current_epoch = checkpoint['epoch']
        self.best_val_acc = checkpoint['best_val_acc']
        self.best_epoch = checkpoint['best_epoch']
        
        logger.info(f'æ£€æŸ¥ç‚¹å·²åŠ è½½: {filename}')
        
        return checkpoint
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_file = self.save_dir / 'training_history.json'
        with open(history_file, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.training_history:
            return
        
        epochs = [h['epoch'] for h in self.training_history]
        train_losses = [h['train_loss'] for h in self.training_history]
        val_losses = [h['val_loss'] for h in self.training_history]
        train_accs = [h['train_acc'] for h in self.training_history]
        val_accs = [h['val_acc'] for h in self.training_history]
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, train_losses, label='Train Loss')
        ax1.plot(epochs, val_losses, label='Val Loss')
        ax1.set_title('Loss Curves')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, train_accs, label='Train Acc')
        ax2.plot(epochs, val_accs, label='Val Acc')
        ax2.set_title('Accuracy Curves')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        lrs = [h['learning_rate'] for h in self.training_history]
        ax3.plot(epochs, lrs)
        ax3.set_title('Learning Rate')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.set_yscale('log')
        ax3.grid(True)
        
        # æ¯è½®æ—¶é—´
        times = [h['epoch_time'] for h in self.training_history]
        ax4.plot(epochs, times)
        ax4.set_title('Epoch Time')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Time (s)')
        ax4.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()

# ä¾¿æ·å‡½æ•°
def create_trainer(config: TrainingConfig) -> ModelTrainer:
    """åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨"""
    return ModelTrainer(config)

def create_default_config(**kwargs) -> TrainingConfig:
    """åˆ›å»ºé»˜è®¤è®­ç»ƒé…ç½®"""
    config = TrainingConfig()
    for key, value in kwargs.items():
        if hasattr(config, key):
            setattr(config, key, value)
    return config

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹è®­ç»ƒæ¡†æ¶
    print("ğŸ§ª æ¨¡å‹è®­ç»ƒæ¡†æ¶æµ‹è¯•")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    try:
        # æµ‹è¯•è®­ç»ƒé…ç½®
        print("ğŸ“‹ æµ‹è¯•è®­ç»ƒé…ç½®...")
        config = create_default_config(
            num_epochs=5,
            batch_size=16,
            learning_rate=0.001,
            model_name='efficientnet-b4',
            pretrained=False  # é¿å…ä¸‹è½½
        )
        
        print(f"âœ… è®­ç»ƒé…ç½®åˆ›å»ºæˆåŠŸ:")
        print(f"   è½®æ•°: {config.num_epochs}")
        print(f"   æ‰¹å¤§å°: {config.batch_size}")
        print(f"   å­¦ä¹ ç‡: {config.learning_rate}")
        print(f"   æ¨¡å‹: {config.model_name}")
        
        # æµ‹è¯•è®­ç»ƒå™¨åˆ›å»º
        print(f"\nğŸ”§ æµ‹è¯•è®­ç»ƒå™¨åˆ›å»º...")
        trainer = create_trainer(config)
        
        print(f"âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ:")
        print(f"   è®¾å¤‡: {trainer.device}")
        print(f"   ä¿å­˜ç›®å½•: {trainer.save_dir}")
        
        # æµ‹è¯•æ¨¡å‹è®¾ç½®
        print(f"\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹è®¾ç½®...")
        model = trainer.setup_model()
        
        print(f"âœ… æ¨¡å‹è®¾ç½®å®Œæˆ:")
        print(f"   æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   ä¼˜åŒ–å™¨: {type(trainer.optimizer).__name__}")
        print(f"   è°ƒåº¦å™¨: {type(trainer.scheduler).__name__}")
        print(f"   æŸå¤±å‡½æ•°: {type(trainer.criterion).__name__}")
        
        # æµ‹è¯•æ—©åœæœºåˆ¶
        print(f"\nâ¹ï¸ æµ‹è¯•æ—©åœæœºåˆ¶...")
        early_stopping = EarlyStopping(patience=3, min_delta=0.01)
        
        # æ¨¡æ‹ŸéªŒè¯å‡†ç¡®ç‡å˜åŒ–
        val_accs = [0.85, 0.87, 0.86, 0.86, 0.85, 0.84]
        for i, acc in enumerate(val_accs):
            should_stop = early_stopping(acc)
            print(f"   Epoch {i+1}: Val Acc = {acc:.2f}, Should Stop = {should_stop}")
            if should_stop:
                break
        
        # æµ‹è¯•æ ‡ç­¾å¹³æ»‘æŸå¤±
        print(f"\nğŸ¯ æµ‹è¯•æ ‡ç­¾å¹³æ»‘æŸå¤±...")
        label_smooth_loss = LabelSmoothingLoss(num_classes=38, smoothing=0.1)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        pred = torch.randn(4, 38)
        target = torch.randint(0, 38, (4,))
        
        loss = label_smooth_loss(pred, target)
        print(f"âœ… æ ‡ç­¾å¹³æ»‘æŸå¤±æµ‹è¯•å®Œæˆ:")
        print(f"   è¾“å…¥å½¢çŠ¶: {pred.shape}")
        print(f"   ç›®æ ‡å½¢çŠ¶: {target.shape}")
        print(f"   æŸå¤±å€¼: {loss.item():.4f}")
        
        # æ¸…ç†
        trainer.writer.close()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… æ¨¡å‹è®­ç»ƒæ¡†æ¶æµ‹è¯•å®Œæˆ")