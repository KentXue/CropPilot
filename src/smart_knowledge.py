# æ™ºèƒ½çŸ¥è¯†åº“æ£€ç´¢æ¨¡å—
# åŸºäºChromaDBå’Œsentence-transformerså®ç°è¯­ä¹‰æœç´¢

import os
import sys
from typing import List, Dict, Any
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    SMART_KNOWLEDGE_AVAILABLE = True
except ImportError as e:
    print(f"æ™ºèƒ½çŸ¥è¯†åº“ä¾èµ–æœªå®‰è£…: {e}")
    print("è¯·è¿è¡Œ: pip install chromadb sentence-transformers")
    SMART_KNOWLEDGE_AVAILABLE = False

class SmartKnowledgeBase:
    """æ™ºèƒ½çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self):
        if not SMART_KNOWLEDGE_AVAILABLE:
            self.available = False
            return
            
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆè½»é‡çº§ï¼Œæ— éœ€GPUï¼‰
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # åˆ›å»ºæˆ–è¿æ¥æœ¬åœ°å‘é‡æ•°æ®åº“
            db_path = os.path.join(os.path.dirname(__file__), '..', 'vector_db')
            os.makedirs(db_path, exist_ok=True)
            
            self.client = chromadb.PersistentClient(path=db_path)
            self.collection = self.client.get_or_create_collection(
                name="agriculture_knowledge",
                metadata={"description": "å†œä¸šçŸ¥è¯†åº“"}
            )
            
            # åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆå¦‚æœä¸ºç©ºï¼‰
            if self.collection.count() == 0:
                self._initialize_knowledge_base()
                
            self.available = True
            print("æ™ºèƒ½çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"æ™ºèƒ½çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.available = False
    
    def _initialize_knowledge_base(self):
        """åˆå§‹åŒ–å†œä¸šçŸ¥è¯†åº“"""
        print("æ­£åœ¨åˆå§‹åŒ–å†œä¸šçŸ¥è¯†åº“...")
        
        # å†œä¸šçŸ¥è¯†æ–‡æ¡£
        knowledge_docs = [
            {
                "content": "æ°´ç¨»åˆ†è˜–æœŸæ˜¯æ°´ç¨»ç”Ÿé•¿çš„å…³é”®æ—¶æœŸï¼Œæ­¤æ—¶éœ€è¦ä¿æŒæµ…æ°´å±‚3-5cmï¼Œä¿ƒè¿›åˆ†è˜–ã€‚æ–½è‚¥æ–¹é¢ï¼Œæ¯äº©è¿½æ–½å°¿ç´ 5-8å…¬æ–¤ï¼Œä¿ƒè¿›åˆ†è˜–å‘ç”Ÿã€‚æ³¨æ„é˜²æ²»ç¨»é£è™±å’Œçº¹æ¯ç—…ã€‚",
                "source": "æ°´ç¨»æ ½åŸ¹æŠ€æœ¯æ‰‹å†Œ",
                "crop": "æ°´ç¨»",
                "stage": "åˆ†è˜–æœŸ"
            },
            {
                "content": "æ°´ç¨»æ‹”èŠ‚æœŸè¦æ§åˆ¶æ°®è‚¥æ–½ç”¨ï¼Œé˜²æ­¢å¾’é•¿å€’ä¼ã€‚ä¿æŒé€‚åº¦æ°´å±‚ï¼Œé¿å…è¿‡æ·±æˆ–è¿‡æµ…ã€‚æ­¤æœŸæ˜¯å†³å®šç©—æ•°çš„å…³é”®æœŸï¼Œè¦åŠ å¼ºç”°é—´ç®¡ç†ã€‚",
                "source": "æ°´ç¨»æ ½åŸ¹æŠ€æœ¯æ‰‹å†Œ", 
                "crop": "æ°´ç¨»",
                "stage": "æ‹”èŠ‚æœŸ"
            },
            {
                "content": "æ°´ç¨»æŠ½ç©—æœŸéœ€è¦å……è¶³çš„æ°´åˆ†ä¾›åº”ï¼Œä¿æŒæ°´å±‚5-7cmã€‚å¶é¢å–·æ–½ç£·é…¸äºŒæ°¢é’¾ï¼Œæé«˜ç»“å®ç‡ã€‚æ³¨æ„é˜²æ²»ç¨»ç˜Ÿç—…å’Œè¤é£è™±ã€‚",
                "source": "æ°´ç¨»æ ½åŸ¹æŠ€æœ¯æ‰‹å†Œ",
                "crop": "æ°´ç¨»", 
                "stage": "æŠ½ç©—æœŸ"
            },
            {
                "content": "ç‰ç±³è‹—æœŸç®¡ç†è¦ç‚¹ï¼šä¿æŒåœŸå£¤æ¹¿æ¶¦ä½†ä¸ç§¯æ°´ï¼ŒåŸºè‚¥ä¸ºä¸»ï¼Œå¯é€‚å½“è¿½æ–½å°‘é‡æ°®è‚¥ã€‚æ³¨æ„é˜²æ²»åœ°ä¸‹å®³è™«å¦‚è›´è¬ã€é‡‘é’ˆè™«ç­‰ã€‚",
                "source": "ç‰ç±³æ ½åŸ¹æŠ€æœ¯æŒ‡å—",
                "crop": "ç‰ç±³",
                "stage": "è‹—æœŸ"
            },
            {
                "content": "ç‰ç±³æ‹”èŠ‚æœŸæ˜¯éœ€æ°´éœ€è‚¥çš„å…³é”®æœŸï¼Œè¿½æ–½æ°®è‚¥ä¿ƒè¿›èŒç§†ç”Ÿé•¿ã€‚ä¿æŒå……è¶³æ°´åˆ†ï¼Œä½†è¦æ³¨æ„æ’æ°´é˜²æ¶ã€‚æ­¤æœŸè¦é˜²æ²»ç‰ç±³èŸè™«å®³ã€‚",
                "source": "ç‰ç±³æ ½åŸ¹æŠ€æœ¯æŒ‡å—",
                "crop": "ç‰ç±³", 
                "stage": "æ‹”èŠ‚æœŸ"
            },
            {
                "content": "ç‰ç±³æŠ½é›„æœŸéœ€è¦å¤§é‡æ°´åˆ†ï¼Œæ˜¯å†³å®šäº§é‡çš„å…³é”®æœŸã€‚å¢æ–½ç£·é’¾è‚¥ï¼Œä¿ƒè¿›æˆç²‰ç»“å®ã€‚æ³¨æ„é˜²æ²»ç‰ç±³å¤§æ–‘ç—…å’Œå°æ–‘ç—…ã€‚",
                "source": "ç‰ç±³æ ½åŸ¹æŠ€æœ¯æŒ‡å—",
                "crop": "ç‰ç±³",
                "stage": "æŠ½é›„æœŸ"
            },
            {
                "content": "ä½œç‰©å¶ç‰‡å‘é»„å¯èƒ½çš„åŸå› ï¼š1.ç¼ºæ°®è‚¥å¯¼è‡´çš„ç”Ÿç†æ€§é»„åŒ–ï¼›2.æ ¹ç³»å—æŸå½±å“å…»åˆ†å¸æ”¶ï¼›3.ç—…å®³æ„ŸæŸ“å¦‚çº¹æ¯ç—…ã€å¶æ¯ç—…ï¼›4.è™«å®³å±å®³å¦‚èšœè™«ã€çº¢èœ˜è››ã€‚éœ€è¦æ ¹æ®å…·ä½“ç—‡çŠ¶åˆ¤æ–­åŸå› ã€‚",
                "source": "ä½œç‰©ç—…è™«å®³è¯Šæ–­æ‰‹å†Œ",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            },
            {
                "content": "åœŸå£¤pHå€¼è¿‡é«˜æˆ–è¿‡ä½éƒ½ä¼šå½±å“ä½œç‰©ç”Ÿé•¿ã€‚pHå€¼6.0-7.0æœ€é€‚å®œå¤§å¤šæ•°ä½œç‰©ã€‚pHè¿‡ä½å¯æ–½ç”¨çŸ³ç°è°ƒèŠ‚ï¼ŒpHè¿‡é«˜å¯æ–½ç”¨ç¡«ç£ºæˆ–æœ‰æœºè‚¥æ”¹è‰¯ã€‚",
                "source": "åœŸå£¤æ”¹è‰¯æŠ€æœ¯æ‰‹å†Œ", 
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            },
            {
                "content": "é«˜æ¸©å¹²æ—±æ¡ä»¶ä¸‹çš„åº”å¯¹æªæ–½ï¼š1.åŠæ—¶çŒæº‰ï¼Œä¿æŒåœŸå£¤æ¹¿æ¶¦ï¼›2.å¶é¢å–·æ°´é™æ¸©ï¼›3.è¦†ç›–é®é˜³ç½‘æˆ–ç§¸ç§†ï¼›4.å¶é¢å–·æ–½æŠ—æ—±å‰‚ï¼›5.é€‚å½“ä¿®å‰ªå‡å°‘è’¸è…¾ã€‚",
                "source": "å†œä¸šæ°”è±¡ç¾å®³é˜²å¾¡æ‰‹å†Œ",
                "crop": "é€šç”¨", 
                "stage": "é€šç”¨"
            },
            {
                "content": "ç—…è™«å®³ç»¼åˆé˜²æ²»åŸåˆ™ï¼šé¢„é˜²ä¸ºä¸»ï¼Œç»¼åˆé˜²æ²»ã€‚ä¼˜å…ˆä½¿ç”¨å†œä¸šé˜²æ²»ã€ç”Ÿç‰©é˜²æ²»ï¼ŒåŒ–å­¦é˜²æ²»ä½œä¸ºè¡¥å……ã€‚é€‰æ‹©é«˜æ•ˆä½æ¯’å†œè¯ï¼Œæ³¨æ„è½®æ¢ç”¨è¯é¿å…æŠ—æ€§ã€‚",
                "source": "ç—…è™«å®³é˜²æ²»æŒ‡å—",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            }
        ]
        
        # æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        for i, doc in enumerate(knowledge_docs):
            try:
                # ç”Ÿæˆæ–‡æ¡£å‘é‡
                embedding = self.model.encode(doc["content"]).tolist()
                
                # æ·»åŠ åˆ°æ•°æ®åº“
                self.collection.add(
                    embeddings=[embedding],
                    documents=[doc["content"]],
                    metadatas=[{
                        "source": doc["source"],
                        "crop": doc["crop"], 
                        "stage": doc["stage"],
                        "doc_id": i
                    }],
                    ids=[f"doc_{i}"]
                )
            except Exception as e:
                print(f"æ·»åŠ æ–‡æ¡£ {i} å¤±è´¥: {e}")
        
        print(f"æˆåŠŸåˆå§‹åŒ– {len(knowledge_docs)} æ¡å†œä¸šçŸ¥è¯†")
    
    def add_document(self, content: str, source: str, crop: str = "é€šç”¨", stage: str = "é€šç”¨"):
        """æ·»åŠ æ–°çš„çŸ¥è¯†æ–‡æ¡£"""
        if not self.available:
            return False
            
        try:
            # ç”Ÿæˆå”¯ä¸€ID
            doc_count = self.collection.count()
            doc_id = f"doc_{doc_count}"
            
            # ç”Ÿæˆå‘é‡
            embedding = self.model.encode(content).tolist()
            
            # æ·»åŠ åˆ°æ•°æ®åº“
            self.collection.add(
                embeddings=[embedding],
                documents=[content],
                metadatas=[{
                    "source": source,
                    "crop": crop,
                    "stage": stage,
                    "doc_id": doc_count
                }],
                ids=[doc_id]
            )
            return True
        except Exception as e:
            print(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def query(self, question: str, crop_type: str = "", growth_stage: str = "", n_results: int = 3) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æŸ¥è¯¢ç›¸å…³çŸ¥è¯†"""
        if not self.available:
            return []
            
        try:
            # æ„é€ æ›´å…·ä½“çš„æŸ¥è¯¢æ–‡æœ¬
            query_parts = [question]
            if crop_type:
                query_parts.append(crop_type)
            if growth_stage:
                query_parts.append(growth_stage)
            
            query_text = " ".join(query_parts)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.model.encode(query_text).tolist()
            
            # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            advice_snippets = []
            if results['documents'] and len(results['documents']) > 0:
                for i, (doc, meta, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    advice_snippets.append({
                        "content": doc,
                        "source": meta.get('source', 'æœªçŸ¥æ¥æº'),
                        "crop": meta.get('crop', 'é€šç”¨'),
                        "stage": meta.get('stage', 'é€šç”¨'),
                        "relevance_score": 1 - distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                        "rank": i + 1
                    })
            
            return advice_snippets
            
        except Exception as e:
            print(f"æ™ºèƒ½æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def format_advice(self, snippets: List[Dict[str, Any]], question: str) -> str:
        """å°†æ£€ç´¢ç»“æœæ ¼å¼åŒ–ä¸ºå‹å¥½çš„å»ºè®®æ–‡æœ¬"""
        if not snippets:
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³çš„å†œä¸šçŸ¥è¯†ã€‚å»ºè®®å’¨è¯¢å½“åœ°å†œæŠ€ä¸“å®¶ã€‚"
        
        formatted = f"æ ¹æ®å†œä¸šçŸ¥è¯†åº“ï¼Œå…³äºã€Œ{question}ã€çš„å»ºè®®å¦‚ä¸‹ï¼š\n\n"
        
        for snippet in snippets:
            relevance = snippet.get('relevance_score', 0)
            if relevance > 0.3:  # åªæ˜¾ç¤ºç›¸å…³åº¦è¾ƒé«˜çš„ç»“æœ
                formatted += f"ğŸ’¡ {snippet['content']}\n"
                formatted += f"   ğŸ“š æ¥æºï¼š{snippet['source']}\n\n"
        
        formatted += "---\n"
        formatted += "*ä»¥ä¸Šå»ºè®®åŸºäºæƒå¨å†œä¸šèµ„æ–™ï¼Œè¯·ç»“åˆå®åœ°æƒ…å†µçµæ´»åº”ç”¨ã€‚å¦‚æœ‰ç–‘é—®ï¼Œå»ºè®®å’¨è¯¢å½“åœ°å†œæŠ€ä¸“å®¶ã€‚*"
        
        return formatted

# å…¨å±€å®ä¾‹
smart_kb = None

def get_smart_knowledge_base():
    """è·å–æ™ºèƒ½çŸ¥è¯†åº“å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global smart_kb
    if smart_kb is None:
        smart_kb = SmartKnowledgeBase()
    return smart_kb

def smart_query(question: str, crop_type: str = "", growth_stage: str = "") -> str:
    """ä¾¿æ·çš„æ™ºèƒ½æŸ¥è¯¢å‡½æ•°"""
    kb = get_smart_knowledge_base()
    if not kb.available:
        return "æ™ºèƒ½çŸ¥è¯†åº“æš‚ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç›¸å…³ä¾èµ–æ˜¯å¦å·²å®‰è£…ã€‚"
    
    snippets = kb.query(question, crop_type, growth_stage)
    return kb.format_advice(snippets, question)

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½çŸ¥è¯†åº“
    print("æµ‹è¯•æ™ºèƒ½çŸ¥è¯†åº“...")
    
    kb = SmartKnowledgeBase()
    if kb.available:
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            ("å¶å­å‘é»„æ€ä¹ˆåŠ", "æ°´ç¨»", "åˆ†è˜–æœŸ"),
            ("å¦‚ä½•æ–½è‚¥", "ç‰ç±³", "æ‹”èŠ‚æœŸ"),
            ("ç—…è™«å®³é˜²æ²»", "", ""),
            ("é«˜æ¸©å¹²æ—±", "", "")
        ]
        
        for question, crop, stage in test_queries:
            print(f"\næŸ¥è¯¢: {question} (ä½œç‰©: {crop}, é˜¶æ®µ: {stage})")
            result = smart_query(question, crop, stage)
            print(result)
            print("-" * 50)
    else:
        print("æ™ºèƒ½çŸ¥è¯†åº“ä¸å¯ç”¨")