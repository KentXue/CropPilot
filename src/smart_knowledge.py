# æ™ºèƒ½çŸ¥è¯†åº“æ£€ç´¢æ¨¡å—
# åŸºäºChromaDBå’Œsentence-transformerså®ç°è¯­ä¹‰æœç´¢
# æ”¯æŒå¤šç§æ•°æ®æºï¼šJSONæ–‡ä»¶ã€æ•°æ®åº“ã€ç¡¬ç¼–ç å…œåº•

import os
import sys
from typing import List, Dict, Any
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    SMART_KNOWLEDGE_AVAILABLE = True
except ImportError as e:
    print(f"æ™ºèƒ½çŸ¥è¯†åº“ä¾èµ–æœªå®‰è£…: {e}")
    print("è¯·è¿è¡Œ: pip install chromadb sentence-transformers")
    SMART_KNOWLEDGE_AVAILABLE = False

# å¯¼å…¥çŸ¥è¯†åŠ è½½å™¨
try:
    from knowledge_loader import KnowledgeLoader
    KNOWLEDGE_LOADER_AVAILABLE = True
except ImportError:
    KNOWLEDGE_LOADER_AVAILABLE = False

class SmartKnowledgeBase:
    """æ™ºèƒ½çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ - æ”¯æŒå¤šæ•°æ®æº"""
    
    def __init__(self, data_source: str = "json", data_path: str = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½çŸ¥è¯†åº“
        
        Args:
            data_source: æ•°æ®æºç±»å‹ ("json", "database", "hardcoded")
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºJSON/CSVï¼‰
        """
        self.data_source = data_source
        self.data_path = data_path or "data/agriculture_knowledge.json"
        
        if not SMART_KNOWLEDGE_AVAILABLE:
            self.available = False
            return
            
        try:
            # å°è¯•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆè½»é‡çº§ï¼Œæ— éœ€GPUï¼‰
            print("æ­£åœ¨åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
            print("ğŸ“¥ é¦–æ¬¡ä½¿ç”¨éœ€è¦ä»HuggingFaceä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆçº¦90MBï¼‰...")
            
            # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´ç”¨äºæ¨¡å‹ä¸‹è½½
            import socket
            original_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(60)  # 60ç§’è¶…æ—¶ï¼Œç»™ä¸‹è½½æ›´å¤šæ—¶é—´
            
            try:
                # å°è¯•ä½¿ç”¨é•œåƒæºåŠ é€Ÿä¸‹è½½ï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
                import os
                os.environ.setdefault('HF_ENDPOINT', 'https://hf-mirror.com')
                
                print("ğŸ”„ æ­£åœ¨ä¸‹è½½sentence-transformersæ¨¡å‹...")
                print("   æ¨¡å‹: all-MiniLM-L6-v2 (è½»é‡çº§æ–‡æœ¬åµŒå…¥æ¨¡å‹)")
                print("   å¤§å°: ~90MB")
                print("   ç”¨é€”: å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå®ç°æ™ºèƒ½è¯­ä¹‰æœç´¢")
                
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… åµŒå…¥æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
                print("ğŸ’¾ æ¨¡å‹å·²ç¼“å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡å¯åŠ¨å°†ç›´æ¥ä½¿ç”¨")
            except Exception as model_error:
                print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {model_error}")
                print("ğŸ”„ åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼...")
                # æ¢å¤è¶…æ—¶è®¾ç½®
                socket.setdefaulttimeout(original_timeout)
                # ä½¿ç”¨ç¦»çº¿å…œåº•æ–¹æ¡ˆ
                self._init_offline_mode()
                return
            finally:
                # æ¢å¤åŸå§‹è¶…æ—¶è®¾ç½®
                socket.setdefaulttimeout(original_timeout)
            
            # åˆ›å»ºæˆ–è¿æ¥æœ¬åœ°å‘é‡æ•°æ®åº“
            db_path = os.path.join(os.path.dirname(__file__), '..', 'vector_db')
            os.makedirs(db_path, exist_ok=True)
            
            self.client = chromadb.PersistentClient(path=db_path)
            self.collection = self.client.get_or_create_collection(
                name="agriculture_knowledge",
                metadata={"description": "å†œä¸šçŸ¥è¯†åº“"}
            )
            
            # åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆå¦‚æœä¸ºç©ºæˆ–éœ€è¦æ›´æ–°ï¼‰
            if self.collection.count() == 0 or self._should_reload_knowledge():
                self._initialize_knowledge_base()
                
            self.available = True
            print("âœ… æ™ºèƒ½çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ æ™ºèƒ½çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼...")
            self._init_offline_mode()
    
    def _init_offline_mode(self):
        """åˆå§‹åŒ–ç¦»çº¿æ¨¡å¼ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        self.available = True
        self.offline_mode = True
        self.model = None
        self.client = None
        self.collection = None
        
        # åŠ è½½ç¦»çº¿çŸ¥è¯†åº“
        self.offline_knowledge = self._get_comprehensive_offline_knowledge()
        print("âœ… ç¦»çº¿æ¨¡å¼åˆå§‹åŒ–æˆåŠŸï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰")
    
    def _should_reload_knowledge(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½çŸ¥è¯†åº“"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é€»è¾‘ï¼Œæ¯”å¦‚æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        return False
    
    def _load_knowledge_documents(self) -> List[Dict[str, Any]]:
        """æ ¹æ®é…ç½®çš„æ•°æ®æºåŠ è½½çŸ¥è¯†æ–‡æ¡£"""
        if not KNOWLEDGE_LOADER_AVAILABLE:
            print("çŸ¥è¯†åŠ è½½å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¡¬ç¼–ç çŸ¥è¯†")
            return self._get_hardcoded_knowledge()
        
        loader = KnowledgeLoader()
        
        if self.data_source == "json":
            return loader.load_from_json(os.path.basename(self.data_path))
        elif self.data_source == "csv":
            return loader.load_from_csv(os.path.basename(self.data_path))
        elif self.data_source == "database":
            # è¿™é‡Œéœ€è¦ä¼ å…¥æ•°æ®åº“è¿æ¥
            # return loader.load_from_database(connection)
            print("æ•°æ®åº“åŠ è½½æš‚æœªå®ç°ï¼Œä½¿ç”¨JSONå…œåº•")
            return loader.load_from_json()
        else:
            return self._get_hardcoded_knowledge()

    def _initialize_knowledge_base(self):
        """åˆå§‹åŒ–å†œä¸šçŸ¥è¯†åº“ - æ”¯æŒå¤šæ•°æ®æº"""
        print(f"æ­£åœ¨ä» {self.data_source} åˆå§‹åŒ–å†œä¸šçŸ¥è¯†åº“...")
        
        # åŠ è½½çŸ¥è¯†æ–‡æ¡£
        knowledge_docs = self._load_knowledge_documents()
        
        if not knowledge_docs:
            print("æœªæ‰¾åˆ°çŸ¥è¯†æ–‡æ¡£ï¼Œä½¿ç”¨ç¡¬ç¼–ç å…œåº•")
            knowledge_docs = self._get_hardcoded_knowledge()
        
        # æ¸…ç©ºç°æœ‰é›†åˆï¼ˆå¦‚æœéœ€è¦é‡æ–°åŠ è½½ï¼‰
        try:
            self.client.delete_collection("agriculture_knowledge")
            self.collection = self.client.create_collection(
                name="agriculture_knowledge",
                metadata={"description": "å†œä¸šçŸ¥è¯†åº“"}
            )
        except:
            pass  # é›†åˆå¯èƒ½ä¸å­˜åœ¨
        
        # æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        for i, doc in enumerate(knowledge_docs):
            try:
                # ç”Ÿæˆæ–‡æ¡£å‘é‡
                embedding = self.model.encode(doc["content"]).tolist()
                
                # æ·»åŠ åˆ°æ•°æ®åº“
                self.collection.add(
                    embeddings=[embedding],
                    documents=[doc["content"]],
                    metadatas=[{
                        "source": doc["source"],
                        "crop": doc["crop"], 
                        "stage": doc["stage"],
                        "doc_id": doc.get("id", str(i)),
                        "priority": doc.get("priority", 1)
                    }],
                    ids=[doc.get("id", f"doc_{i}")]
                )
            except Exception as e:
                print(f"æ·»åŠ æ–‡æ¡£ {i} å¤±è´¥: {e}")
        
        print(f"æˆåŠŸåˆå§‹åŒ– {len(knowledge_docs)} æ¡å†œä¸šçŸ¥è¯†")
    
    def _get_hardcoded_knowledge(self) -> List[Dict[str, Any]]:
        """ç¡¬ç¼–ç çš„å…œåº•çŸ¥è¯†åº“ï¼ˆæœ€å°é›†åˆï¼‰"""
        return [
            {
                "id": "hardcoded_leaf_yellow",
                "content": "ä½œç‰©å¶ç‰‡å‘é»„å¯èƒ½çš„åŸå› ï¼š1.ç¼ºæ°®è‚¥å¯¼è‡´çš„ç”Ÿç†æ€§é»„åŒ–ï¼›2.æ ¹ç³»å—æŸå½±å“å…»åˆ†å¸æ”¶ï¼›3.ç—…å®³æ„ŸæŸ“å¦‚çº¹æ¯ç—…ã€å¶æ¯ç—…ï¼›4.è™«å®³å±å®³å¦‚èšœè™«ã€çº¢èœ˜è››ã€‚éœ€è¦æ ¹æ®å…·ä½“ç—‡çŠ¶åˆ¤æ–­åŸå› ã€‚",
                "source": "ç³»ç»Ÿå†…ç½®çŸ¥è¯†",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨",
                "priority": 1
            },
            {
                "id": "hardcoded_pest_control",
                "content": "ç—…è™«å®³ç»¼åˆé˜²æ²»åŸåˆ™ï¼šé¢„é˜²ä¸ºä¸»ï¼Œç»¼åˆé˜²æ²»ã€‚ä¼˜å…ˆä½¿ç”¨å†œä¸šé˜²æ²»ã€ç”Ÿç‰©é˜²æ²»ï¼ŒåŒ–å­¦é˜²æ²»ä½œä¸ºè¡¥å……ã€‚é€‰æ‹©é«˜æ•ˆä½æ¯’å†œè¯ï¼Œæ³¨æ„è½®æ¢ç”¨è¯é¿å…æŠ—æ€§ã€‚",
                "source": "ç³»ç»Ÿå†…ç½®çŸ¥è¯†",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨",
                "priority": 1
            },
            {
                "id": "hardcoded_drought",
                "content": "é«˜æ¸©å¹²æ—±æ¡ä»¶ä¸‹çš„åº”å¯¹æªæ–½ï¼š1.åŠæ—¶çŒæº‰ï¼Œä¿æŒåœŸå£¤æ¹¿æ¶¦ï¼›2.å¶é¢å–·æ°´é™æ¸©ï¼›3.è¦†ç›–é®é˜³ç½‘æˆ–ç§¸ç§†ï¼›4.å¶é¢å–·æ–½æŠ—æ—±å‰‚ï¼›5.é€‚å½“ä¿®å‰ªå‡å°‘è’¸è…¾ã€‚",
                "source": "ç³»ç»Ÿå†…ç½®çŸ¥è¯†",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨",
                "priority": 1
            }
        ]
    
    def _get_comprehensive_offline_knowledge(self) -> List[Dict[str, Any]]:
        """æ‰©å±•çš„ç¦»çº¿çŸ¥è¯†åº“"""
        return [
            # å¶ç‰‡é—®é¢˜
            {
                "keywords": ["å¶å­", "å¶ç‰‡", "å‘é»„", "é»„åŒ–", "å˜é»„"],
                "content": "å¶ç‰‡å‘é»„çš„å¸¸è§åŸå› åŠå¤„ç†æ–¹æ³•ï¼š\n1. ç¼ºæ°®ï¼šå¶ç‰‡ä»ä¸‹å¾€ä¸Šå‘é»„ï¼Œæ–½ç”¨æ°®è‚¥\n2. ç¼ºé“ï¼šæ–°å¶å‘é»„ï¼Œå¶è„‰ä»ç»¿ï¼Œå–·æ–½é“è‚¥\n3. ç—…å®³ï¼šå¶æ–‘ç—…ã€çº¹æ¯ç—…ç­‰ï¼Œä½¿ç”¨æ€èŒå‰‚\n4. è™«å®³ï¼šèšœè™«ã€çº¢èœ˜è››ç­‰ï¼Œä½¿ç”¨æ€è™«å‰‚\n5. æ°´åˆ†ï¼šè¿‡æ¹¿æˆ–è¿‡å¹²ï¼Œè°ƒèŠ‚çŒæº‰",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            },
            # ç—…è™«å®³é˜²æ²»
            {
                "keywords": ["ç—…è™«å®³", "é˜²æ²»", "ç—…å®³", "è™«å®³", "æ²»ç–—"],
                "content": "ç—…è™«å®³ç»¼åˆé˜²æ²»ç­–ç•¥ï¼š\n1. é¢„é˜²ä¸ºä¸»ï¼šé€‰ç”¨æŠ—ç—…å“ç§ï¼Œåˆç†è½®ä½œ\n2. å†œä¸šé˜²æ²»ï¼šæ¸…æ´ç”°å›­ï¼Œåˆç†æ–½è‚¥\n3. ç”Ÿç‰©é˜²æ²»ï¼šåˆ©ç”¨å¤©æ•Œï¼Œç”Ÿç‰©å†œè¯\n4. åŒ–å­¦é˜²æ²»ï¼šç§‘å­¦ç”¨è¯ï¼Œè½®æ¢ä½¿ç”¨\n5. ç›‘æµ‹é¢„è­¦ï¼šå®šæœŸæ£€æŸ¥ï¼ŒåŠæ—¶å‘ç°",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            },
            # é«˜æ¸©å¹²æ—±
            {
                "keywords": ["é«˜æ¸©", "å¹²æ—±", "ç¼ºæ°´", "çƒ­å®³", "æŠ—æ—±"],
                "content": "é«˜æ¸©å¹²æ—±åº”å¯¹æªæ–½ï¼š\n1. çŒæº‰ç®¡ç†ï¼šåŠæ—¶è¡¥æ°´ï¼Œæ»´çŒèŠ‚æ°´\n2. é®é˜³é™æ¸©ï¼šæ­å»ºé®é˜³ç½‘ï¼Œå‡å°‘è’¸è…¾\n3. å¶é¢å–·æ°´ï¼šæ—©æ™šå–·æ°´ï¼Œé™ä½å¶æ¸©\n4. è¦†ç›–ä¿å¢’ï¼šç§¸ç§†è¦†ç›–ï¼Œå‡å°‘è’¸å‘\n5. æŠ—æ—±å‰‚ï¼šå–·æ–½æŠ—æ—±å‰‚ï¼Œæé«˜æŠ—æ€§",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            },
            # æ–½è‚¥ç®¡ç†
            {
                "keywords": ["æ–½è‚¥", "è‚¥æ–™", "è¥å…»", "æ°®ç£·é’¾", "è¿½è‚¥"],
                "content": "ç§‘å­¦æ–½è‚¥æŒ‡å¯¼ï¼š\n1. åŸºè‚¥ï¼šæœ‰æœºè‚¥ä¸ºä¸»ï¼Œæ”¹è‰¯åœŸå£¤\n2. è¿½è‚¥ï¼šæ ¹æ®ç”Ÿé•¿æœŸéœ€æ±‚åˆ†æ¬¡æ–½ç”¨\n3. æ°®è‚¥ï¼šä¿ƒè¿›èŒå¶ç”Ÿé•¿ï¼Œæ³¨æ„ç”¨é‡\n4. ç£·è‚¥ï¼šä¿ƒè¿›æ ¹ç³»å’ŒèŠ±æœå‘è‚²\n5. é’¾è‚¥ï¼šæé«˜æŠ—æ€§ï¼Œæ”¹å–„å“è´¨",
                "crop": "é€šç”¨",
                "stage": "é€šç”¨"
            },
            # æ°´ç¨»ä¸“ç”¨
            {
                "keywords": ["æ°´ç¨»", "ç¨»ç”°", "åˆ†è˜–", "æŠ½ç©—", "çŒæµ†"],
                "content": "æ°´ç¨»ç®¡ç†è¦ç‚¹ï¼š\n1. åˆ†è˜–æœŸï¼šæµ…æ°´å‹¤çŒï¼Œä¿ƒè¿›åˆ†è˜–\n2. æ‹”èŠ‚æœŸï¼šæ·±æ°´æŠ¤è‹—ï¼Œé˜²æ­¢å€’ä¼\n3. æŠ½ç©—æœŸï¼šä¿æŒæ°´å±‚ï¼Œç¡®ä¿æŠ½ç©—\n4. çŒæµ†æœŸï¼šå¹²æ¹¿äº¤æ›¿ï¼Œæé«˜å“è´¨\n5. æˆç†ŸæœŸï¼šé€‚æ—¶æ–­æ°´ï¼Œä¾¿äºæ”¶è·",
                "crop": "æ°´ç¨»",
                "stage": "é€šç”¨"
            },
            # ç‰ç±³ä¸“ç”¨
            {
                "keywords": ["ç‰ç±³", "æ‹”èŠ‚", "æŠ½é›„", "çŒæµ†", "ç‰ç±³ç”°"],
                "content": "ç‰ç±³ç®¡ç†è¦ç‚¹ï¼š\n1. è‹—æœŸï¼šæ§æ°´è¹²è‹—ï¼Œä¿ƒè¿›æ ¹ç³»\n2. æ‹”èŠ‚æœŸï¼šé‡æ–½æ‹”èŠ‚è‚¥ï¼Œä¿ƒè¿›èŒç§†\n3. æŠ½é›„æœŸï¼šä¿è¯æ°´åˆ†ï¼Œç¡®ä¿æˆç²‰\n4. çŒæµ†æœŸï¼šå……è¶³æ°´è‚¥ï¼Œæé«˜äº§é‡\n5. æˆç†ŸæœŸï¼šé€‚æ—¶æ”¶è·ï¼Œç¡®ä¿å“è´¨",
                "crop": "ç‰ç±³",
                "stage": "é€šç”¨"
            }
        ]
    
    def add_document(self, content: str, source: str, crop: str = "é€šç”¨", stage: str = "é€šç”¨"):
        """æ·»åŠ æ–°çš„çŸ¥è¯†æ–‡æ¡£"""
        if not self.available:
            return False
            
        try:
            # ç”Ÿæˆå”¯ä¸€ID
            doc_count = self.collection.count()
            doc_id = f"doc_{doc_count}"
            
            # ç”Ÿæˆå‘é‡
            embedding = self.model.encode(content).tolist()
            
            # æ·»åŠ åˆ°æ•°æ®åº“
            self.collection.add(
                embeddings=[embedding],
                documents=[content],
                metadatas=[{
                    "source": source,
                    "crop": crop,
                    "stage": stage,
                    "doc_id": doc_count
                }],
                ids=[doc_id]
            )
            return True
        except Exception as e:
            print(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def query(self, question: str, crop_type: str = "", growth_stage: str = "", n_results: int = 3) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æŸ¥è¯¢ç›¸å…³çŸ¥è¯†"""
        if not self.available:
            return []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¦»çº¿æ¨¡å¼
        if hasattr(self, 'offline_mode') and self.offline_mode:
            return self._offline_query(question, crop_type, growth_stage, n_results)
            
        try:
            # æ„é€ æ›´å…·ä½“çš„æŸ¥è¯¢æ–‡æœ¬
            query_parts = [question]
            if crop_type:
                query_parts.append(crop_type)
            if growth_stage:
                query_parts.append(growth_stage)
            
            query_text = " ".join(query_parts)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.model.encode(query_text).tolist()
            
            # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            advice_snippets = []
            if results['documents'] and len(results['documents']) > 0:
                for i, (doc, meta, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    advice_snippets.append({
                        "content": doc,
                        "source": meta.get('source', 'æœªçŸ¥æ¥æº'),
                        "crop": meta.get('crop', 'é€šç”¨'),
                        "stage": meta.get('stage', 'é€šç”¨'),
                        "relevance_score": max(0, 1 - distance),  # ç¡®ä¿åˆ†æ•°ä¸ºæ­£æ•°
                        "distance": distance,  # ä¿ç•™åŸå§‹è·ç¦»ç”¨äºè°ƒè¯•
                        "rank": i + 1
                    })
            
            return advice_snippets
            
        except Exception as e:
            print(f"æ™ºèƒ½æŸ¥è¯¢å¤±è´¥: {e}")
            # å¦‚æœå‘é‡æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°ç¦»çº¿æ¨¡å¼
            return self._offline_query(question, crop_type, growth_stage, n_results)
    
    def _offline_query(self, question: str, crop_type: str = "", growth_stage: str = "", n_results: int = 3) -> List[Dict[str, Any]]:
        """ç¦»çº¿å…³é”®è¯åŒ¹é…æŸ¥è¯¢"""
        question_lower = question.lower()
        crop_lower = crop_type.lower()
        
        matched_results = []
        
        for knowledge in self.offline_knowledge:
            score = 0
            
            # å…³é”®è¯åŒ¹é…
            for keyword in knowledge["keywords"]:
                if keyword in question_lower:
                    score += 2
            
            # ä½œç‰©åŒ¹é…
            if crop_lower and knowledge["crop"].lower() in [crop_lower, "é€šç”¨"]:
                score += 1
            elif knowledge["crop"] == "é€šç”¨":
                score += 0.5
            
            if score > 0:
                matched_results.append({
                    "content": knowledge["content"],
                    "source": "ç¦»çº¿çŸ¥è¯†åº“",
                    "crop": knowledge["crop"],
                    "stage": knowledge["stage"],
                    "relevance_score": min(score / 3, 1.0),  # å½’ä¸€åŒ–åˆ°0-1
                    "distance": 1 - min(score / 3, 1.0),
                    "rank": 0
                })
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        matched_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        # é‡æ–°è®¾ç½®æ’å
        for i, result in enumerate(matched_results[:n_results]):
            result["rank"] = i + 1
        
        return matched_results[:n_results]

    
    def format_advice(self, snippets: List[Dict[str, Any]], question: str) -> str:
        """å°†æ£€ç´¢ç»“æœæ ¼å¼åŒ–ä¸ºå‹å¥½çš„å»ºè®®æ–‡æœ¬"""
        if not snippets:
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³çš„å†œä¸šçŸ¥è¯†ã€‚å»ºè®®å’¨è¯¢å½“åœ°å†œæŠ€ä¸“å®¶ã€‚"
        
        formatted = f"æ ¹æ®å†œä¸šçŸ¥è¯†åº“ï¼Œå…³äºã€Œ{question}ã€çš„å»ºè®®å¦‚ä¸‹ï¼š\n\n"
        
        for snippet in snippets:
            relevance = snippet.get('relevance_score', 0)
            distance = snippet.get('distance', 0)
            # å¯¹äºå‘é‡æœç´¢ï¼Œè·ç¦»å°äº5.0é€šå¸¸è¡¨ç¤ºæœ‰ä¸€å®šç›¸å…³æ€§
            if distance < 5.0:  
                formatted += f"ğŸ’¡ {snippet['content']}\n"
                formatted += f"   ğŸ“š æ¥æºï¼š{snippet['source']}\n\n"
        
        formatted += "---\n"
        formatted += "*ä»¥ä¸Šå»ºè®®åŸºäºæƒå¨å†œä¸šèµ„æ–™ï¼Œè¯·ç»“åˆå®åœ°æƒ…å†µçµæ´»åº”ç”¨ã€‚å¦‚æœ‰ç–‘é—®ï¼Œå»ºè®®å’¨è¯¢å½“åœ°å†œæŠ€ä¸“å®¶ã€‚*"
        
        return formatted

# å…¨å±€å®ä¾‹
smart_kb = None

def get_smart_knowledge_base(data_source: str = "json", data_path: str = None):
    """è·å–æ™ºèƒ½çŸ¥è¯†åº“å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global smart_kb
    if smart_kb is None:
        smart_kb = SmartKnowledgeBase(data_source=data_source, data_path=data_path)
    return smart_kb

def smart_query(question: str, crop_type: str = "", growth_stage: str = "") -> str:
    """ä¾¿æ·çš„æ™ºèƒ½æŸ¥è¯¢å‡½æ•°"""
    kb = get_smart_knowledge_base()  # é»˜è®¤ä½¿ç”¨JSONæ•°æ®æº
    if not kb.available:
        return "æ™ºèƒ½çŸ¥è¯†åº“æš‚ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç›¸å…³ä¾èµ–æ˜¯å¦å·²å®‰è£…ã€‚"
    
    snippets = kb.query(question, crop_type, growth_stage)
    return kb.format_advice(snippets, question)

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½çŸ¥è¯†åº“
    print("æµ‹è¯•æ™ºèƒ½çŸ¥è¯†åº“...")
    
    kb = SmartKnowledgeBase()
    if kb.available:
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            ("å¶å­å‘é»„æ€ä¹ˆåŠ", "æ°´ç¨»", "åˆ†è˜–æœŸ"),
            ("å¦‚ä½•æ–½è‚¥", "ç‰ç±³", "æ‹”èŠ‚æœŸ"),
            ("ç—…è™«å®³é˜²æ²»", "", ""),
            ("é«˜æ¸©å¹²æ—±", "", "")
        ]
        
        for question, crop, stage in test_queries:
            print(f"\næŸ¥è¯¢: {question} (ä½œç‰©: {crop}, é˜¶æ®µ: {stage})")
            result = smart_query(question, crop, stage)
            print(result)
            print("-" * 50)
    else:
        print("æ™ºèƒ½çŸ¥è¯†åº“ä¸å¯ç”¨")