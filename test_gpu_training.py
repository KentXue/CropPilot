#!/usr/bin/env python3
"""
GPUè®­ç»ƒæµ‹è¯•è„šæœ¬
æµ‹è¯•GPUåŠ é€Ÿçš„è®­ç»ƒæµç¨‹
"""

import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
    import numpy as np
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {e}")
    DEPENDENCIES_AVAILABLE = False

from src.model_architecture import create_plant_disease_model, ModelFactory
from src.model_trainer import ModelTrainer, create_default_config
from src.model_evaluator import create_evaluator

def test_gpu_training():
    """æµ‹è¯•GPUè®­ç»ƒ"""
    print("ğŸš€ GPUè®­ç»ƒæµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒæµ‹è¯•")
        return False
    
    try:
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
        print("åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
        num_samples = 160  # 20ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹8ä¸ªæ ·æœ¬
        images = torch.randn(num_samples, 3, 224, 224)
        labels = torch.randint(0, 38, (num_samples,))
        
        train_size = int(0.7 * num_samples)
        val_size = num_samples - train_size
        
        train_dataset = TensorDataset(images[:train_size], labels[:train_size])
        val_dataset = TensorDataset(images[train_size:], labels[train_size:])
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨GPUä¼˜åŒ–è®¾ç½®ï¼‰
        train_loader = DataLoader(
            train_dataset, 
            batch_size=8, 
            shuffle=True,
            num_workers=0,  # Windowsä¸Šè®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
            pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=8, 
            shuffle=False,
            num_workers=0,
            pin_memory=True
        )
        
        # åˆ›å»ºGPUè®­ç»ƒé…ç½®
        config = create_default_config(
            num_epochs=3,
            batch_size=8,
            learning_rate=0.001,
            model_name='efficientnet-b4',
            pretrained=False,  # é¿å…ä¸‹è½½
            save_dir='test_gpu_checkpoints',
            device='cuda',
            mixed_precision=True,
            early_stopping=False
        )
        
        print(f"é…ç½®ä¿¡æ¯:")
        print(f"  è®¾å¤‡: {config.device}")
        print(f"  æ‰¹å¤§å°: {config.batch_size}")
        print(f"  æ··åˆç²¾åº¦: {config.mixed_precision}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ModelTrainer(config)
        model = trainer.setup_model()
        
        print(f"æ¨¡å‹ä¿¡æ¯:")
        print(f"  è®¾å¤‡: {next(model.parameters()).device}")
        print(f"  å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æµ‹è¯•GPUå†…å­˜ä½¿ç”¨
        print(f"\nè®­ç»ƒå‰GPUå†…å­˜:")
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        cached = torch.cuda.memory_reserved(0) / 1024**3
        print(f"  å·²åˆ†é…: {allocated:.2f} GB")
        print(f"  å·²ç¼“å­˜: {cached:.2f} GB")
        
        # æ‰§è¡Œè®­ç»ƒ
        print(f"\nå¼€å§‹GPUè®­ç»ƒ...")
        start_time = time.time()
        
        training_results = trainer.train(train_loader, val_loader)
        
        training_time = time.time() - start_time
        
        # è®­ç»ƒåGPUå†…å­˜ä½¿ç”¨
        print(f"\nè®­ç»ƒåGPUå†…å­˜:")
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        cached = torch.cuda.memory_reserved(0) / 1024**3
        print(f"  å·²åˆ†é…: {allocated:.2f} GB")
        print(f"  å·²ç¼“å­˜: {cached:.2f} GB")
        
        print(f"\nâœ… GPUè®­ç»ƒå®Œæˆ:")
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_results['best_val_acc']:.2f}%")
        print(f"  ä½¿ç”¨è®¾å¤‡: {trainer.device}")
        
        # æ¸…ç†
        del model, trainer
        torch.cuda.empty_cache()
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        if os.path.exists('test_gpu_checkpoints'):
            shutil.rmtree('test_gpu_checkpoints')
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUè®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        return False

def compare_cpu_gpu_performance():
    """æ¯”è¾ƒCPUå’ŒGPUæ€§èƒ½"""
    print("\nâš¡ CPU vs GPUæ€§èƒ½å¯¹æ¯”")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”")
        return
    
    try:
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œæ•°æ®
        model_cpu = create_plant_disease_model('efficientnet', pretrained=False)
        model_gpu = create_plant_disease_model('efficientnet', pretrained=False).cuda()
        
        test_data_cpu = torch.randn(8, 3, 224, 224)
        test_data_gpu = test_data_cpu.cuda()
        
        # CPUæ€§èƒ½æµ‹è¯•
        model_cpu.eval()
        with torch.no_grad():
            # é¢„çƒ­
            for _ in range(5):
                _ = model_cpu(test_data_cpu)
            
            # æµ‹è¯•
            start_time = time.time()
            for _ in range(20):
                _ = model_cpu(test_data_cpu)
            cpu_time = (time.time() - start_time) / 20
        
        # GPUæ€§èƒ½æµ‹è¯•
        model_gpu.eval()
        with torch.no_grad():
            # é¢„çƒ­
            for _ in range(5):
                _ = model_gpu(test_data_gpu)
            torch.cuda.synchronize()
            
            # æµ‹è¯•
            start_time = time.time()
            for _ in range(20):
                _ = model_gpu(test_data_gpu)
            torch.cuda.synchronize()
            gpu_time = (time.time() - start_time) / 20
        
        speedup = cpu_time / gpu_time
        
        print(f"æ€§èƒ½å¯¹æ¯”ç»“æœ (æ‰¹å¤§å°=8):")
        print(f"  CPUæ—¶é—´: {cpu_time*1000:.1f}ms/batch")
        print(f"  GPUæ—¶é—´: {gpu_time*1000:.1f}ms/batch")
        print(f"  åŠ é€Ÿæ¯”: {speedup:.1f}x")
        
        if speedup > 1:
            print(f"ğŸš€ GPUæ¯”CPUå¿« {speedup:.1f} å€!")
        else:
            print(f"âš ï¸  GPUæ€§èƒ½æœªè¾¾åˆ°é¢„æœŸ")
        
        # æ¸…ç†
        del model_cpu, model_gpu, test_data_cpu, test_data_gpu
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½å¯¹æ¯”å¤±è´¥: {e}")
        torch.cuda.empty_cache()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª GPUè®­ç»ƒæµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        return
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œè¯·å…ˆè¿è¡Œ check_gpu.py æ£€æŸ¥GPUè®¾ç½®")
        return
    
    print(f"GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # æµ‹è¯•GPUè®­ç»ƒ
    success = test_gpu_training()
    
    if success:
        # æ€§èƒ½å¯¹æ¯”
        compare_cpu_gpu_performance()
        
        print("\nğŸ‰ GPUè®­ç»ƒæµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡ŒGPUè®­ç»ƒ:")
        print("python train_model.py --config gpu_training_config.json")
    else:
        print("\nâŒ GPUè®­ç»ƒæµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()