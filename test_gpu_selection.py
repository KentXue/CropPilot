#!/usr/bin/env python3
"""
GPUé€‰æ‹©æµ‹è¯•è„šæœ¬
"""

import torch
import time

def test_gpu_selection():
    """æµ‹è¯•GPUé€‰æ‹©é€»è¾‘"""
    print("ğŸ” GPUé€‰æ‹©æµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    gpu_count = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU:")
    
    # æ˜¾ç¤ºæ‰€æœ‰GPUä¿¡æ¯
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1024**3
        
        print(f"  GPU {i}: {gpu_name}")
        print(f"    æ˜¾å­˜: {memory_gb:.1f} GB")
        print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"    å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        print()
    
    # é€‰æ‹©æœ€ä½³GPU
    best_gpu = 0
    if gpu_count > 1:
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            # RTXã€GTXç­‰ç‹¬ç«‹æ˜¾å¡ä¼˜å…ˆ
            if any(keyword in gpu_name.upper() for keyword in ['RTX', 'GTX', 'TESLA', 'QUADRO']):
                best_gpu = i
                break
    
    print(f"ğŸ¯ é€‰æ‹©çš„GPU: GPU {best_gpu} - {torch.cuda.get_device_name(best_gpu)}")
    
    # æµ‹è¯•é€‰å®šGPUçš„æ€§èƒ½
    device = torch.device(f'cuda:{best_gpu}')
    
    print(f"\nğŸ§ª æµ‹è¯•GPU {best_gpu}æ€§èƒ½...")
    
    # çŸ©é˜µä¹˜æ³•æµ‹è¯•
    sizes = [1000, 2000, 3000]
    for size in sizes:
        x = torch.randn(size, size, device=device)
        y = torch.randn(size, size, device=device)
        
        start_time = time.time()
        z = torch.mm(x, y)
        torch.cuda.synchronize()
        end_time = time.time()
        
        print(f"  {size}x{size} çŸ©é˜µä¹˜æ³•: {(end_time-start_time)*1000:.1f}ms")
        
        del x, y, z
    
    # æ˜¾å­˜ä½¿ç”¨æµ‹è¯•
    print(f"\nğŸ’¾ æ˜¾å­˜ä½¿ç”¨æµ‹è¯•...")
    allocated = torch.cuda.memory_allocated(best_gpu) / 1024**3
    cached = torch.cuda.memory_reserved(best_gpu) / 1024**3
    total = torch.cuda.get_device_properties(best_gpu).total_memory / 1024**3
    
    print(f"  å·²åˆ†é…: {allocated:.2f} GB")
    print(f"  å·²ç¼“å­˜: {cached:.2f} GB")
    print(f"  æ€»æ˜¾å­˜: {total:.2f} GB")
    print(f"  ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")
    
    torch.cuda.empty_cache()
    print(f"\nâœ… GPU {best_gpu} å·¥ä½œæ­£å¸¸")

if __name__ == "__main__":
    test_gpu_selection()